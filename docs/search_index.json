[
["index.html", "Environmental Informatics Using Research Infrastructures and their Data Preface Acknowledgements", " Environmental Informatics Using Research Infrastructures and their Data Dr. Katharyn Duffy, Dr. Ben Ruddell Preface This textbook provides an introduction to environmental and ecological informatics in the context of “big science”- that is, in the context of research infrastructures and observatories that collect and publish reams of observations and derived data products about the earth and its environment. The reader will learn how to make use of environmental infrastructures’ data products. This textbook introduces a framework of learning outcomes that covers the broad context of these data products that is necessary to make proper use of the information: The infrastructure’s organizational structure and scientific scope; Instrumentation, quality control, metadata, data catalogs, API’s, and data products; Key references, key tutorials, and informatics best practices; Professional career tracks in informatics; This textbook is intended primarily for graduate students enrolled in computer science and informatics programs but engaged in studies and research on environmental and geoscience topics- with a special emphasis on ecological topics. Advanced undergraduates and other graduate students enrolled in STEM programs may also be well-served if they have a strong background in programming and computing. Additionally, professional scientists may find this textbook useful as a reference and as a training manual when they encounter the need to make use of the research infrastructures and data that are directly covered by the book’s content- or find the need to train a junior scientist on the use of these infrastructures’ data. The scope of the infrastructures and data products is mostly U.S. focused in this edition, but some of these infrastructures have a global reach, and the material is almost as useful for students in other countries as for U.S. students. The textbook can be tackled one unit at a time as a lab manual within a university course, or- in its intended application- a standard semester-long three-credit-hour graduate course should be offered to cover the entire textbook from start to finish. Digital supplements are provided with examples of successful projects. Efforts have been made to select activities using data products, software, and tools that are relatively mature and stable. Even so, because this textbook covers a rapidly moving field, portions will, unfortunately, become dated quickly. It is the authors’ intent to release frequent editions that update and expand the material to keep pace with the rapid development of our field. Informatics is arguably the key scientific discipline of the 21st century, and research infrastructures are the source of the raw natural resource fueling the informatics revolution: observational data. Most 21st century scientists and scientific staff will spend their careers immersed in the data revolution. We sincerely hope that this textbook provides the launchpad you need for your career or for your next project in environmental science. Acknowledgements This first version of the textbook was developed to offer INF550, a graduate course in the School of Informatics Computing and Cyber Systems at Northern Arizona University in Fall 2020, with funding and leadership from the National Science Foundation funded National Research Traineeship “T3” option in Ecological and Environmental Informatics within a PhD program in Informatics and Computing (NRT-HDR #1829075, PI’s Ogle, Barber, Richardson, Ruddell, and Sankey). Our research infrastructure partners were critical to the creation of the material. Our partners at NEON - Battelle deserve special gratitude for anchoring the project. The opinions expressed are those of the researchers, and not necessarily the funding agencies. Special thanks to Megan Jones and Donal O’Leary at NEON-Battelle for their support in pulling NEON materials. Key Contributors Alphabetized by organization, then last name: Alycia Crall, NEON - Battelle Chris Florian, NEON - Battelle Megan Jones, NEON - Battelle Hank Loescher, NEON - Battelle Paula Mabee, NEON - Battelle Donal O’Leary, NEON - Battelle Kate Thibault, NEON - Battelle Andrew Richardson, PhenoCam - NAU Bijan Seyednasrollah, PhenoCam - NAU Theresa Crimmons, USA-NPN Kathy Gerst, USA-NPN Lee Marsh, USA-NPN "],
["pre-course-setup-ecoinformatics-tools.html", "Pre-Course Setup: EcoInformatics Tools 0.1 Pre-Course Skills &amp; Setup 0.2 Linux R/RStudio Setup 0.3 Installing and Setting up Git &amp; Github on Your Machine 0.4 Installing Atom 0.5 Linking RStudio to Git 0.6 How we will be Conducting this Course 0.7 Exercises:", " Pre-Course Setup: EcoInformatics Tools The purpose of this course is to train you in key ecoinformatics practices. Therefore, as an Ecoinformatician you need to be able to: Pull data from Application Programming Interfaces (APIs) More on this in Chapter 2 Organize and document your code and data Version control your code to avoid disaster and make it reproducible For you, your collaborators, and/or the wider community Push your code up to public-facing repositories Pull others code from public repositories. More thoughts on the benefits and power of reproducibility can be found here To be successful, both in this course and in your careers you will need these skills. This is why they are a requirement for this course. If you are already using these skills on a daily basis, fantastic! If you don’t feel that you have mastery in the workflows listed above we have placed lesson links throughout this chapter so that you can build these skills and be successful in this course. 0.1 Pre-Course Skills &amp; Setup For the purpose of this course we will largely be using the following tools to access, pull, and explore data: R &amp; Rstudio Git, GitHub, &amp; Atom.io Markdown &amp; Rmarkdown As such we will need to install and/or update these tools on your personal computer before our first day of class. While we chose R for this course, nearly all of the packages and data are fully available and transferable to Python or other languages. If you’d like to brush up on your R skills I highly recommend Data Carpentry Boot camp’s free R for Reproducible Scientific Analysis course. 0.1.1 Installing or Updating R Please check your version of R. You will need R 3.6.0+ How to check your version in R or RStudio if you already have it: &gt; version _ platform x86_64-apple-darwin15.6.0 arch x86_64 os darwin15.6.0 system x86_64, darwin15.6.0 status major 3 minor 5.1 year 2018 month 07 day 02 svn rev 74947 language R version.string R version 3.5.1 (2018-07-02) nickname Feather Spray If you don’t already have R or need to update it do so here. 0.1.2 Windows R/RStudio Setup After you have downloaded R, run the .exe file that was just downloaded Go to the RStudio Download page Under Installers select RStudio X.XX.XXX - e.g. Windows Vista/7/8/10 Double click the file to install it Once R and RStudio are installed, click to open RStudio. If you don’t get any error messages you are set. If there is an error message, you will need to re-install the program. 0.1.3 Mac R/RStudio Setup After you have downloaded R, double click on the file that was downloaded and R will install Go to the RStudio Download page Under Installers select RStudio 1.2.1135 - Mac OS X XX.X (64-bit) to download it. Once it’s downloaded, double click the file to install it. Once R and RStudio are installed, click to open RStudio. If you don’t get any error messages you are set. If there is an error message, you will need to re-install the program. 0.2 Linux R/RStudio Setup R is available through most Linux package managers. You can download the binary files for your distribution from CRAN. Or you can use your package manager. e.g. for Debian/Ubuntu run sudo apt-get install r-base and for Fedora run sudo yum install R To install RStudio, go to the RStudio Download page Under Installers select the version for your distribution. Once it’s downloaded, double click the file to install it Once R and RStudio are installed, click to open RStudio. If you don’t get any error messages you are set. If there is an error message, you will need to re-install the program. 0.2.1 Install basic packages for this course You can run the following script to make sure all the required packages are properly installed on your computer. # list of required packages list.of.packages &lt;- c( &#39;data.table&#39;, &#39;tidyverse&#39;, &#39;jsonlite&#39;, &#39;jpeg&#39;, &#39;png&#39;, &#39;raster&#39;, &#39;rgdal&#39;, &#39;rmarkdown&#39;, &#39;knitr&#39; ) # identify new (not installed) packages new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # install new (not installed) packages if(length(new.packages)) install.packages(new.packages, repos=&#39;http://cran.rstudio.com/&#39;) # load all of the required libraries sapply(list.of.packages, library, character.only = T) Note: On some operating systems, you may need to install the Geospatial Data Abstraction Library (GDAL). More information about GDAL can be found from here. 0.3 Installing and Setting up Git &amp; Github on Your Machine For this course you will need: 1. Git installed on your local machine 2. Very basic bash scripting 3. A linked GitHub account 4. To link RStudio to git via RStudio or Atom.io As we will be using these skills constantly, they are a pre-requisite for this course. If you don’t yet have these skills it’s okay! You can learn everything that you need to know via the following freely available resources: The Unix Shell Version Control with Git Happy Git with R If you are learning these skills from scratch I estimate that you will need to devote ~4-6 hours to get set up and comfortable with the various workflows. Also remember that I have code office hours every week and that Stack Exchange is your friend. 0.4 Installing Atom Atom.io is a powerful and useful text editor for the following reasons: It is language agnostic It fully integrates with git and github + You can use it to push/pull/resolve conflicts and write code all in one space. 0.5 Linking RStudio to Git Happy Git with R has a fantastic tutorial to help you link Rstudio-Git-Github on your local machine and push/pull from or to public repositories. 0.6 How we will be Conducting this Course If you find a broken link or error in this course text submit an issue on the course github repository. At the end of each chapter you will find a set of Exercises. At the end of the assigned chapter you will be expected to submit via BBLearn two files: An RMarkdown file with the naming convention: LASTNAME_COURSECODE_Section#.Rmd, and A knitted .PDF with the same naming convention: LASTNAME_COURSECODE_Section#.pdf To generate these files you have two options: Click on the pencil and pad logo in the top of this text, copy the exercise section code, and drop it into your own .Rmd. Git clone our course Github Repository, navigate to the ’_Exercises’ folder, and use that .Rmd as a template. Note: Exercises submitted in any other format, or those missing questions will not be graded To generate your .PDF to upload, in your RMarkdown file simply push the ‘Knit’ button at the top of your document. 0.7 Exercises: Navigate to our course github git fork our repo onto your own personal github account. git clone the repo onto your own personal machine in a place that is functional and not temporary (e.g. not your downloads folder). #hints cd `Your/Path/Here&#39; git clone &#39;repo HTTPS&#39; Add 2-3 sentences introducing yourself in the _Course-participants folder. For example: *** Hi, I&#39;m Dr. Katharyn Duffy. I have a Ph.D in Earth Science from Northern Arizona University. Over the past two years I&#39;ve worked as an open-source software engineer in the PhenoCam lab, and now I&#39;m the coding and lab support for your course. I really look forward to working with all of you! *** Submit a pull request to add your introduction to our course participants folder. #hints git add ... git commit ... git status.... git push --set-upstream git remote -v git remote add upstream... Note: You may complete these either on the command line or via a program like Atom.io. If you haven’t yet made commits to a remote repository or submitted pull requests please reference the resources listed above. "],
["why-ecoinformatics.html", "Chapter 1 Why ‘EcoInformatics’? 1.1 The Framework of this Course 1.2 Final Course Project: Proposed Derived Data Product", " Chapter 1 Why ‘EcoInformatics’? Portions of the following introduction were adapted from Michener &amp; Jones 2012, Trends in Ecology &amp; Evolution ’Ecoinformatics: supporting ecology as a data-intensive science Ecology is increasingly becoming a data-intensive science, relying on massive amounts of data collected by both remote-sensing platforms and sensor networks embedded in the environment. New observatory networks, such as the US National Ecological Observatory Network (NEON), provide research platforms that enable scientists to examine phenomena across diverse ecosystem types through access to thousands of sensors collecting diverse environmental observations. These networks spatially and temporaly overlap with a number of other networks and infrastructures ranging from remote sensing, to citizen science, and so on. It has been argued that data-intensive science represents the fourth scientific paradigm following the empirical (i.e. description of natural phenomena), theoretical (e.g. modeling and generalization) and computational (e.g. simulation) scientific approaches, and comprises an approach for unifying theory, experimentation and simulation. Ecologists increasingly address questions at broader scales that have both scientific and societal relevance. For example, the 40 top priorities for science that can inform conservation and management policy in the USA rely principally on a sound foundation of ecological research, and the ability to scale knowledge and inter-connect data. Continental-scale patterns and dynamics result from climate and people as broad-scale drivers interacting with finer-scale vectors that redistribute materials within and among linked terrestrial and aquatic systems. Climate and land-use change interact with patterns and processes at multiple, finer scales (blue arrows). (a) These drivers can influence broad-scale patterns directly, and these constraints may act to overwhelm heterogeneity and processes at (b) mesoscales and at (c) the finer scale of local sites. Broad-scale drivers can also exert an indirect impact on broad-scale patterns through their interactions with disturbances, including (d) the spread of invasive species, (e) pattern–process relationships at meso-scales, or (f) at finer scales within a site. Connectivity imparted by the transfer of materials occurs both at (g) the meso-scale and at (h) finer scales within sites where terrestrial and aquatic systems are connected. These dynamics at fine scales can propagate to influence larger spatial extents (red arrows). Feedbacks occur throughout the system. The term “drivers” refers to both forcing functions that are part of the system and to external drivers. Peters et al., 2008 Ecology is also affected by changes that are occurring throughout science as a whole. In particular, scientists, professional societies and research sponsors are recognizing the value of data as a product of the scientific enterprise and placing increased emphasis on data stewardship, data sharing, openness and supporting study repeatability. Data on ecological and environmental systems are (A) acquired, checked for quality, documented using an acquisition workflow, and then both the raw and derived data products are versioned and deposited in the DataONE federated data archive (red dashed arrows). Researchers discover and access data from the federation and then (B) integrate and process the data in an analysis workflow, resulting in derived data products, visualizations, and scholarly papers that are in turn archived in the data federation (red dashed arrows). Other researchers directly cite any of the versioned data, workflows, and visualizations that are archived in the DataONE federation. Richman et al., 2011 The changes that are occurring in ecology create challenges with respect to acquiring, managing and analyzing the large volumes of data that are collected by scientists worldwide. One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological processes occurring at the scale of the continent and biosphere. The diversity in scales studied and the ways in which studies are carried out results in large numbers of small, idiosyncratic data sets that accumulate from the thousands of scientists that collect relevant biological, ecological and environmental data. A proposed high-level architecture for ecological and environmental data management is shown consisting of three primary levels. Data stored within distributed data repositories (a) is mediated by standard metadata and ontologies (b) to power software tools used by scientists and data managers (c). Software applications use community-endorsed ontologies and metadata standards from the middle level to provide tools that are more effective for publishing, querying, integrating and analyzing data. Ontologies are separated into framework ontologies and domain-specific extensions, enabling contributions from multiple research groups, disciplines and individuals. Cross-disciplinary data are maintained in local repositories, but made accessible to the broader research community through distributed systems based on shared, open protocols (such as Metacat). Example repositories include the LTER network, National Ecological Observatory Network, United States Geographical Survey and SEEK’s EarthGrid. Madin et al. 2008, Ecoinformatics is a framework that enables scientists to generate new knowledge through innovative tools and approaches for: discovering, managing, integrating, analyzing, visualizing, and preserving relevant biological, environmental, and socioeconomic data and information. Many ecoinformatics solutions have been developed over the past decade, increasing scientists’ efficiency and supporting faster and easier data discovery, integration and analysis; however, many challenges remain, especially in relation to installing ecoinformatics practices into mainstream research and education. And that, course participants, is why we are here. 1.1 The Framework of this Course Over the duration of this course we will survey a wide array of observation platforms and networks and build hands-on experience with the framework of Ecoinformatics. For coherance we will cover the following overarching themes: Each network’s mission and design Each network’s spatial design e.g. opprtunistic vs. planned, citizen science vs. orbital sensors The types of data that stream from each network e.g. sensors, derived products, metadata How to access that data e.g. APIs, landing pages, r packages etc. Opportunities to interact with or contribute to each network e.g. RFP’s coming down the pipeline, internships, and post-doctoral scholar programs. At the conclusion of each network’s section you will be asked to write a 1-page summary reviewing the above framework for each network, and highlight how it potentially aligns with your own research. These series of 1-page summaries will then culminate into a final presentation where you propose to derive your own data product for your own research touching upon multiple networks and accounting for differences in spatial footprints, frequency of observations, and important data cross-walks. 1.2 Final Course Project: Proposed Derived Data Product For your final project, you will present a 4-6 minute IGNITE-style derived data product pitch, followed by 2-3 minutes of questions from your audience (which will include members from the infrastrures we’ve covered). Think of this project as your ‘sales pitch’ to the research infrastructure whose data you are using, and/or the scientific community as a whole. In the IGNITE theme of ‘Enlighten us but make it quick’, you will construct a series of slides that auto-advance every 30 seconds. Specific instructions for the content of each slide are below. Ideally, this final presentation will feed upon a number of the ‘culmination write-ups’ you have conducted over the course of the semester. Ideally, this derived data product will utilize data from a number of sources, either covered within this course or external to it. Ideally, it will also convince your audience that your idea is novel, useful, and possible. In order to complete this presentation, you will need to have worked with the various data products you propose, have an in-depth understanding of them, and their challenges, along with original, clean, high-level summary graphics. Further, giving an IGNITE-style presentation takes practice. IGNITE-style presentations are powerful, as they keep you moving forward, and give your audience a high-level understanding of your topic. We fully recommend rehearsing your presentation many times before giving it live and recording yourself to learn how you can improve. In your derived data product pitch you will cover these themes: The need for the derived data that you are proposing to produce. What data you will use to derive this product, including the justification for this exact data. The processing pipeline for this product, along with estimates for a timeline. Potential hurdles you will have to overcome. How this product will serve the infrastructure and/or the scientific community. Specific slide criteria are as follows: Slide 1: Title, authors (including contacts at infrastructures covered if applicable) Slide 2: Justification for the derived data product; the gap or need that it fills Slides 3-x: 1 slide per data product used including: The exact data product (e.g. NEON data product id and full title) A 1 sentence summary of the data product and it’s justification for this purpose An original, clean, polished high-level plot of the data Slide x + 1: A high-level workflow diagram of the processing pipeline E.g.: Original data and how you pull it in (API, r-package etc) Filtering process using QA/QC or metadata Orthorectification in time or space Example: Slide x + 2: A clean plot of all of the data you mentioned together, and/or the derived data product itself with a 1 sentence summary Example: Slide x+3: Summary: Circle back on how this derived data product serves your research, the infrastructure, and the wider science community (no more than 10 words, suggestion: graphics or bullet points) Slide x + 4: Data citations for all data used in proposed derived data product The rubric for your final presentation grade is as follows: Presentation meets all requirements and criteria: 60% Aesthetics and craft of presentation: 10% Live presentation of materials: 30% "],
["introduction-to-neon-its-data.html", "Chapter 2 Introduction to NEON &amp; its Data 2.1 Learning Objectives 2.2 The NEON Project Mission &amp; Design 2.3 The Science and Design of NEON 2.4 NEON’s Spatial Design 2.5 How NEON Collects Data 2.6 Accessing NEON Data 2.7 Hands on: Accessing NEON Data &amp; User Tokens 2.7.1 Additional Resources 2.8 Hands on: NEON TOS Data 2.9 Intro to NEON Exercises Part 1 2.10 Part 2: Pulling NEON Data via the API 2.11 What is an API? 2.12 Stacking NEON data 2.13 Intro to NEON Exercises: Written Questions 2.14 Intro to NEON Exercises Part 2 2.15 NEON Coding Lab Part 2 2.16 Intro to NEON Culmination Activity", " Chapter 2 Introduction to NEON &amp; its Data Estimated Time: 2 hours Course participants: As you review this information, please consider the final course project that you will work on the over this semester. At the end of this section you will document an initial research question, or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 2.1 Learning Objectives At the end of this activity, you will be able to: Explain the mission of the National Ecological Observatory Network (NEON). Explain the how sites are located within the NEON project design. Determine how the different types of data that are collected and provided by NEON, and how they align with your own research. Pull NEON data from the API and neonUtilities package [@R-neonUtilites] 2.2 The NEON Project Mission &amp; Design To capture ecological heterogeneity across the United States, NEON’s design divides the continent into 20 statistically different eco-climatic domains. Each NEON field site is located within an eco-climatic domain. 2.3 The Science and Design of NEON To gain a better understanding of the broad scope of NEON watch this 4:08 minute long video. 2.4 NEON’s Spatial Design Watch this 4:22 minute video exploring the spatial design of NEON field sites. Please read the following page about NEON’s Spatial Design: Read this primer on NEON’s Sampling Design Read about the different types of field sites - core and relocatable 2.4.1 NEON Samples All 20 Eco-Regions Explore the NEON Field Site map taking note of the locations of: Aquatic &amp; terrestrial field sites. Core &amp; relocatable field sites. Click here to view the NEON Field Site Map Explore the NEON field site map. Do the following: Zoom in on a study area of interest to see if there are any NEON field sites that are nearby. Click the “More” button in the upper right hand corner of the map to filter sites by name, site host, domain or state. Select one field site of interest. Click on the marker in the map. Then click on the name of the field site to jump to the field site landing page. Data Tip: You can download maps, kmz, or shapefiles of the field sites here. 2.5 How NEON Collects Data Watch this 3:06 minute video exploring the data that NEON collects. Read the Data Collection Methods page to learn more about the different types of data that NEON collects and provides. Then, follow the links below to learn more about each collection method: Aquatic Observation System (AOS) Aquatic Instrument System (AIS) Terrestrial Instrument System (TIS) – Flux Tower Terrestrial Instrument System (TIS) – Soil Sensors and Measurements Terrestrial Organismal System (TOS) Airborne Observation Platform (AOP) All data collection protocols and processing documents are publicly available. Read more about the standardized protocols and how to access these documents. 2.5.1 Specimens &amp; Samples NEON also collects samples and specimens from which the other data products are based. These samples are also available for research and education purposes. Learn more: NEON Biorepository. 2.5.2 Airborne Remote Sensing Watch this 4:02 minute video to better understand the NEON Airborne Observation Platform (AOP). Data Tip: NEON also provides support to your own research including proposals to fly the AOP over other study sites, a mobile tower/instrumentation setup and others. Learn more here the Assignable Assets programs . 2.6 Accessing NEON Data NEON data are processed and go through quality assurance quality control checks at NEON headquarters in Boulder, CO. NEON carefully documents every aspect of sampling design, data collection, processing and delivery. This documentation is freely available through the NEON data portal. Visit the NEON Data Portal - data.neonscience.org Read more about the quality assurance and quality control processes for NEON data and how the data are processed from raw data to higher level data products. Explore NEON Data Products. On the page for each data product in the catalog you can find the basic information about the product, find the data collection and processing protocols, and link directly to downloading the data. Additionally, some types of NEON data are also available through the data portals of other organizations. For example, NEON Terrestrial Insect DNA Barcoding Data is available through the Barcode of Life Datasystem (BOLD). Or NEON phenocam images are available from the Phenocam network site. More details on where else the data are available from can be found in the Availability and Download section on the Product Details page for each data product (visit Explore Data Products to access individual Product Details pages). 2.6.1 Pathways to access NEON Data There are several ways to access data from NEON: Via the NEON data portal. Explore and download data. Note that much of the tabular data is available in zipped .csv files for each month and site of interest. To combine these files, use the neonUtilities package (R tutorial, Python tutorial). Use R or Python to programmatically access the data. NEON and community members have created code packages to directly access the data through an API. Learn more about the available resources by reading the Code Resources page or visiting the NEONScience GitHub repo. Using the NEON API. Access NEON data directly using a custom API call. Access NEON data through partner’s portals. Where NEON data directly overlap with other community resources, NEON data can be accessed through the portals. Examples include Phenocam, BOLD, Ameriflux, and others. You can learn more in the documentation for individual data products. 2.7 Hands on: Accessing NEON Data &amp; User Tokens 2.7.1 Via the NEON API, with your User Token NEON data can be downloaded from either the NEON Data Portal or the NEON API. When downloading from the Data Portal, you can create a user account. Read about the benefits of an account on the User Account page. You can also use your account to create a token for using the API. Your token is unique to your account, so don’t share it. While using a token is optional in general, it is required for this course. Using a token when downloading data via the API, including when using the neonUtilities package, links your downloads to your user account, as well as enabling faster download speeds. For more information about token usage and benefits, see the NEON API documentation page. For now, in addition to faster downloads, using a token helps NEON to track data downloads. Using anonymized user information, they can then calculate data access statistics, such as which data products are downloaded most frequently, which data products are downloaded in groups by the same users, and how many users in total are downloading data. This information helps NEON to evaluate the growth and reach of the observatory, and to advocate for training activities, workshops, and software development. Tokens can (and should) be used whenever you use the NEON API. In this tutorial, we’ll focus on using tokens with the neonUtilities R package. 2.7.1 Objectives After completing this section, you will be able to: Create a NEON API token Use your token when downloading data with neonUtilities 2.7.1 Things You’ll Need To Complete This Tutorial You will need a version of R (3.4.1 or higher) and RStudio loaded on your computer. 2.7.1 Install R Packages neonUtilities: install.packages(&quot;neonUtilities&quot;) 2.7.1 Additional Resources NEON Data Portal NEONScience GitHub Organization neonUtilities tutorial If you’ve never downloaded NEON data using the neonUtilities package before, we recommend starting with the Download and Explore tutorial before proceeding with this tutorial. In the next sections, we’ll get an API token from the NEON Data Portal, and then use it in neonUtilities when downloading data. 2.7.2 Get a NEON API Token The first step is create a NEON user account, if you don’t have one. Follow the instructions on the Data Portal User Accounts page. If you do already have an account, go to the NEON Data Portal, sign in, and go to your My Account profile page. Once you have an account, you can create an API token for yourself. At the bottom of the My Account page, you should see this bar: Click the ‘GET API TOKEN’ button. After a moment, you should see this: Click on the Copy button to copy your API token to the clipboard. 2.7.3 Use the API token in neonUtilities In the next section, we’ll walk through saving your token somewhere secure but accessible to your code. But first let’s try out using the token the easy way. First, we need to load the neonUtilities package and set the working directory: # install neonUtilities - can skip if already installed, but # API tokens are only enabled in neonUtilities v1.3.4 and higher # if your version number is lower, re-install install.packages(&quot;neonUtilities&quot;) # load neonUtilities library(neonUtilities) # set working directory wd &lt;- &quot;~/data&quot; # this will depend on your local machine setwd(wd) NEON API tokens are very long, so it would be annoying to keep pasting the entire text string into functions. Assign your token an object name: NEON_TOKEN &lt;- &quot;PASTE YOUR TOKEN HERE&quot; Now we’ll use the loadByProduct() function to download data. Your API token is entered as the optional token input parameter. For this example, we’ll download Plant foliar traits (DP1.10026.001). foliar &lt;- loadByProduct(dpID=&quot;DP1.10026.001&quot;, site=&quot;all&quot;, package=&quot;expanded&quot;, check.size=F, token=NEON_TOKEN) You should now have data saved in the foliar object; the API silently used your token. If you’ve downloaded data without a token before, you may notice this is faster! This format applies to all neonUtilities functions that involve downloading data or otherwise accessing the API; you can use the token input with all of them. For example, when downloading remote sensing data: chm &lt;- byTileAOP(dpID=&quot;DP3.30015.001&quot;, site=&quot;WREF&quot;, year=2017, check.size=F, easting=c(571000,578000), northing=c(5079000,5080000), savepath=wd, token=NEON_TOKEN) 2.7.4 Token management for open code Your API token is unique to your account, so don’t share it! If you’re writing code that will be shared with colleagues or available publicly, such as in a GitHub repository or supplemental materials of a published paper, you can’t include the line of code above where we assigned your token to NEON_TOKEN, since your token is fully visible in the code there. Instead, you’ll need to save your token locally on your computer, and pull it into your code without displaying it. There are a few ways to do this, we’ll show two options here. Option 1: Save the token in a local file, and source() that file at the start of every script. This is fairly simple but requires a line of code in every script. Option 2: Add the token to a .Renviron file to create an environment variable that gets loaded when you open R. This is a little harder to set up initially, but once it’s done, it’s done globally, and it will work in every script you run. 2.7.4.1 Option 1: Save token in a local file Open a new, empty R script (.R). Put a single line of code in the script: NEON_TOKEN &lt;- &quot;PASTE YOUR TOKEN HERE&quot; Save this file within your current R project and call the file neon_token_source.R. So that you don’t accidently push your token up to GitHub, move over to the command line or Atom.io and add it to your .gitignore file: knitr::include_graphics(&#39;./docs/images/git_ignore.png&#39;) Now, whenever you want to pull NEON data via the API, at the start of any analysis you would place this line of code: source(&#39;neon_token_source.R&#39;) Then you’ll be able to use token=NEON_TOKEN when you run neonUtilities functions, and you can share your code without accidentally sharing your token. 2.7.4.2 Option 2: Save your toekn to your R environment Instructions for finding and editing your .Renviron can be found in this tutorial in NEON’s Data Tutorials section. 2.8 Hands on: NEON TOS Data 2.8.1 Pull in Tree Data from NEON’s TOS and investigate relationships Adapted from Claire Lunch’s ‘Compare tree height measured from the ground to a Lidar-based Canopy Height Model’ tutorial Later in this course we will be working with NEON’s LiDAR-based Canopy Height Model (CHM) data from their extensive Airborne Observation Platform (AOP). In this section we will pull in DP1.10098.001, Woody plant vegetation structure from NEON’s Terrestrial Observation Sampling (TOS) data and explore the data, from requesting it to plotting it. Generalized TOS sampling schematic, showing the placement of Distributed, Tower, and Gradient Plots from the NEON GUIDE TO WOODY PLANT VEGETATION STRUCTURE, 2018 The vegetation structure data are collected by by field staff on the ground. This data product contains the quality-controlled, native sampling resolution data from in-situ measurements of live and standing dead woody individuals and shrub groups, from all terrestrial NEON sites with qualifying woody vegetation. The exact measurements collected per individual depend on growth form, and these measurements are focused on enabling biomass and productivity estimation, estimation of shrub volume and biomass, and calibration / validation of multiple NEON airborne remote-sensing data products. In general, comparatively large individuals that are visible to remote-sensing instruments are mapped, tagged and measured, and other smaller individuals are tagged and measured but not mapped. Smaller individuals may be subsampled according to a nested subplot approach in order to standardize the per plot sampling effort. Structure and mapping data are reported per individual per plot; sampling metadata, such as per growth form sampling area, are reported per plot. Illustration of a 20 m x 20 m Distributed/Gradient/Tower base plot (left), a 40 m x 40 m Tower base plot (right), and associated nested subplots used for measuring woody stem vegetation. Locations of subplots are denoted with plain text numbers, and locations of nested subplots are denoted with italic numbers from the NEON GUIDE TO WOODY PLANT VEGETATION STRUCTURE, 2018 For the purpose of this hands-on activity we will be using data from the Wind River Experimental Forest NEON field site located in Washington state. The predominant vegetation at that site is tall evergreen conifers. Note: this is also a core site for many other networks such as AmeriFlux and FLUXNET, which we will cover later. Image of the Wind River Crane Flux Tower from Ameriflux Let’s begin by: Installing the geoNEON package Making sure that the packages that we need are loaded, and Supressing ‘strings as factors’ in R, as factors make all sorts of functions in R ‘cranky’. options(stringsAsFactors=F) #install.packages(&quot;devtools&quot;) #uncomment if you don&#39;t yet have devtools #devtools::install_github(&quot;NEONScience/NEON-geolocation/geoNEON&quot;) library(neonUtilities) ## Warning: package &#39;neonUtilities&#39; was built under R version 3.6.2 library(geoNEON) library(sp) ## Warning: package &#39;sp&#39; was built under R version 3.6.2 Now lets begin by pulling in the vegetation structure data using the loadByProduct() function in the neonUtilities package. Inputs needed to the function are: dpID: data product ID; (woody vegetation structure = DP1.10098.001 site: 4-letter site code; Wind River = WREF package: basic or expanded; we’ll begin with a basic here veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, site=&quot;WREF&quot;, package=&quot;basic&quot;, check.size=FALSE, token = NEON_TOKEN) ## Finding available files ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## ## Downloading files totaling approximately 378.8 KiB ## Downloading 2 files ## | | | 0% | |======================================================================| 100% ## ## Unpacking zip files using 1 cores. ## Stacking operation across a single core. ## Stacking table vst_apparentindividual ## Stacking table vst_mappingandtagging ## Stacking table vst_perplotperyear ## Copied the most recent publication of validation file to /stackedFiles ## Copied the most recent publication of categoricalCodes file to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 3 data tables and 3 metadata tables! ## Stacking took 0.178236 secs ## All unzipped monthly data folders have been removed. Now, use the getLocTOS() function in the geoNEON package to get precise locations for the tagged plants. You can refer to the package documentation for more details. vegmap &lt;- getLocTOS(veglist$vst_mappingandtagging, &quot;vst_mappingandtagging&quot;) ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 7% | |====== | 8% | |====== | 9% | |======= | 10% | |======== | 11% | |======== | 12% | |========= | 13% | |========== | 14% | |========== | 15% | |=========== | 16% | |============ | 17% | |============ | 18% | |============= | 18% | |============= | 19% | |============== | 20% | |============== | 21% | |=============== | 21% | |=============== | 22% | |================ | 22% | |================ | 23% | |================ | 24% | |================= | 24% | |================= | 25% | |================== | 25% | |================== | 26% | |=================== | 26% | |=================== | 27% | |=================== | 28% | |==================== | 28% | |==================== | 29% | |===================== | 29% | |===================== | 30% | |====================== | 31% | |====================== | 32% | |======================= | 32% | |======================= | 33% | |======================== | 34% | |========================= | 35% | |========================= | 36% | |========================== | 37% | |=========================== | 38% | |=========================== | 39% | |============================ | 40% | |============================= | 41% | |============================= | 42% | |============================== | 43% | |=============================== | 44% | |=============================== | 45% | |================================ | 45% | |================================ | 46% | |================================= | 47% | |================================= | 48% | |================================== | 48% | |================================== | 49% | |=================================== | 49% | |=================================== | 50% | |=================================== | 51% | |==================================== | 51% | |==================================== | 52% | |===================================== | 52% | |===================================== | 53% | |====================================== | 54% | |====================================== | 55% | |======================================= | 55% | |======================================= | 56% | |======================================== | 57% | |========================================= | 58% | |========================================= | 59% | |========================================== | 60% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 63% | |============================================= | 64% | |============================================= | 65% | |============================================== | 66% | |=============================================== | 67% | |=============================================== | 68% | |================================================ | 68% | |================================================ | 69% | |================================================= | 70% | |================================================= | 71% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 72% | |=================================================== | 73% | |=================================================== | 74% | |==================================================== | 74% | |==================================================== | 75% | |===================================================== | 75% | |===================================================== | 76% | |====================================================== | 76% | |====================================================== | 77% | |====================================================== | 78% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 79% | |======================================================== | 80% | |========================================================= | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |=========================================================== | 84% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 88% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 91% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 94% | |================================================================== | 95% | |=================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 97% | |==================================================================== | 98% | |===================================================================== | 98% | |===================================================================== | 99% | |======================================================================| 99% | |======================================================================| 100% Now we need to merge the mapped locations of individuals (the vst_mappingandtagging table) with the annual measurements of height, diameter, etc (the vst_apparentindividual table). The two tables join based on individualID, the identifier for each tagged plant, but we’ll include namedLocation, domainID, siteID, and plotID in the list of variables to merge on, to avoid ending up with duplicates of each of those columns. Refer to the variables table and to the Data Product User Guide for Woody plant vegetation structure for more information about the contents of each data table. veg &lt;- merge(veglist$vst_apparentindividual, vegmap, by=c(&quot;individualID&quot;,&quot;namedLocation&quot;, &quot;domainID&quot;,&quot;siteID&quot;,&quot;plotID&quot;)) What did you just pull in? Are you sure you know what you’re working with? A best practice is to always do a quick visualization to make sure that you have the right data and that you understand its spread: symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;) A key component of any measurement, and therefore a reoccuring theme in this course, is an estimate of uncertainty. Let’s overlay estimates of uncertainty for the location of each stem in blue: symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;) symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$adjCoordinateUncertainty[which(veg$plotID==&quot;WREF_075&quot;)], inches=F, add=T, fg=&quot;lightblue&quot;) 2.9 Intro to NEON Exercises Part 1 2.9.1 Computational 2.9.1.1 Part 1: Sign up for and Use an NEON API Token: Submit via .Rmd and .pdf a simple script that uses a HIDDEN token to access NEON data. Example: source(&#39;neon_token_source.R&#39;) veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, site=&quot;WREF&quot;, package=&quot;basic&quot;, check.size=FALSE, token = NEON_TOKEN) ## Finding available files ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## ## Downloading files totaling approximately 378.8 KiB ## Downloading 2 files ## | | | 0% | |======================================================================| 100% ## ## Unpacking zip files using 1 cores. ## Stacking operation across a single core. ## Stacking table vst_apparentindividual ## Stacking table vst_mappingandtagging ## Stacking table vst_perplotperyear ## Copied the most recent publication of validation file to /stackedFiles ## Copied the most recent publication of categoricalCodes file to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 3 data tables and 3 metadata tables! ## Stacking took 0.09670997 secs ## All unzipped monthly data folders have been removed. summary(veglist) ## Length Class Mode ## categoricalCodes_10098 5 data.table list ## readme_10098 1 spec_tbl_df list ## validation_10098 8 data.table list ## variables_10098 9 data.table list ## vst_apparentindividual 40 data.frame list ## vst_mappingandtagging 29 data.frame list ## vst_perplotperyear 38 data.frame list 2.9.1.2 Part 2: Further Investigation of NEON TOS Vegetation Structure Data Suggested Timing: Complete this exercise before our next class session In the following section all demonstration code uses the iris dataset for R as examples. In this exercise the iris data is merely used for example code to get your started, you will complete all plots and models using the NEON TOS vegetation structure data Convert the above diameter plot into a ggplot: If you need some refreshers on ggplot Derek Sonderegger’s Introductory Data Science using R: Graphing Part II is a wonderful resource. I’ve pulled some of his plotting examples here. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2 print (&#39;your code here&#39;) ## [1] &quot;your code here&quot; Set the color your circles to be a function of each species: #hints: data(&quot;iris&quot;) ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() Generate a histogram of tree heights for each plot. Color your stacked bar as a function of each species: #hints for faceting: ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() + facet_grid( . ~ Species ) Use dplyr to remove dead trees: library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union #hints: #veg=veg%&gt;% #filter(..... !=....) Create a simple linear model that uses Diameter at Breast Height (DBH) and height to predict allometries. Print the summary information of your model: #hints mdl=lm(Some_diameter + Some_height, data=something) #Question: looking at the metadata which &#39;height&#39; and &#39;diameter&#39; variables should you use? print(mdl) Plot your linear model: # hints: mdl &lt;- lm( Petal.Length ~ Sepal.Length * Species, data = iris ) iris &lt;- iris %&gt;% select( -matches(&#39;fit&#39;), -matches(&#39;lwr&#39;), -matches(&#39;upr&#39;) ) %&gt;% cbind( predict(mdl, newdata=., interval=&#39;confidence&#39;) ) head(iris, n=3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species fit lwr ## 1 5.1 3.5 1.4 0.2 setosa 1.474373 1.398783 ## 2 4.9 3.0 1.4 0.2 setosa 1.448047 1.371765 ## 3 4.7 3.2 1.3 0.2 setosa 1.421721 1.324643 ## upr ## 1 1.549964 ## 2 1.524329 ## 3 1.518798 ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() + geom_line( aes(y=fit) ) + geom_ribbon( aes( ymin=lwr, ymax=upr, fill=Species), alpha=.3 ) # alpha is the ribbon transparency Answer the following questions: What do you think about your simile linear model? What are its limitations? How many unique species are present at WREF? What are the top_5 trees based on height? Diameter? What proportion of sampled trees are dead? 2.10 Part 2: Pulling NEON Data via the API This section covers pulling data from the NEON API or Application Programming Interface using R and the R package httr, but the core information about the API is applicable to other languages and approaches. As a reminder, there are 3 basic categories of NEON data: Observational - Data collected by a human in the field, or in an analytical laboratory, e.g. beetle identification, foliar isotopes Instrumentation - Data collected by an automated, streaming sensor, e.g. net radiation, soil carbon dioxide Remote sensing - Data collected by the airborne observation platform, e.g. LIDAR, surface reflectance This lab covers all three types of data, it is required to complete these sections in order and not skip ahead, since the query principles are explained in the first section, on observational data. 2.10 Objectives After completing this activity, you will be able to: Pull observational, instrumentation, and geolocation data from the NEON API. Transform API-accessed data from JSON to tabular format for analyses. 2.10 Things You’ll Need To Complete This Section To complete this tutorial you will need the most current version of R and, preferably, RStudio loaded on your computer. 2.10 Install R Packages httr: install.packages(&quot;httr&quot;) jsonlite: install.packages(&quot;jsonlite&quot;) dplyr: install.packages(&quot;dplyr&quot;) devtools: install.packages(&quot;devtools&quot;) downloader: install.packages(&quot;downloader&quot;) geoNEON: devtools::install_github(&quot;NEONScience/NEON-geolocation/geoNEON&quot;) neonUtilities: devtools::install_github(&quot;NEONScience/NEON-utilities/neonUtilities&quot;) Note, you must have devtools installed &amp; loaded, prior to loading geoNEON or neonUtilities. 2.10 Additional Resources Webpage for the NEON API GitHub repository for the NEON API ROpenSci wrapper for the NEON API (not covered in this tutorial) 2.11 What is an API? The following material was adapted from: “Using the NEON API in R” description: “Tutorial for getting data from the NEON API, using R and the R package httr” dateCreated: 2017-07-07 authors: [Claire K. Lunch] contributors: [Christine Laney, Megan A. Jones] If you are unfamiliar with the concept of an API, think of an API as a ‘middle person’ that provides a communication path for a software application to obtain information from a digital data source. APIs are becoming a very common means of sharing digital information. Many of the apps that you use on your computer or mobile device to produce maps, charts, reports, and other useful forms of information pull data from multiple sources using APIs. In the ecological and environmental sciences, many researchers use APIs to programmatically pull data into their analyses. (Quoted from the NEON Observatory Blog story: API and data availability viewer now live on the NEON data portal.) There are actually many types or constructions of APIs. If you’re interested you can read a little more about them here 2.11.1 Anatomy of an API call An example API call: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 This includes the base URL, endpoint, and target. 2.11.1.1 Base URL: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 Specifics are appended to this in order to get the data or metadata you’re looking for, but all calls to this API will include the base URL. For the NEON API, this is http://data.neonscience.org/api/v0 – not clickable, because the base URL by itself will take you nowhere! 2.11.1.2 Endpoints: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 What type of data or metadata are you looking for? ~/products Information about one or all of NEON’s data products ~/sites Information about data availability at the site specified in the call ~/locations Spatial data for the NEON locations specified in the call ~/data Data! By product, site, and date (in monthly chunks). 2.11.2 Targets: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 The specific data product, site, or location you want to get data for. 2.11.3 Observational data (OS) Which product do you want to get data for? Consult the Explore Data Products page. We’ll pick Breeding landbird point counts, DP1.10003.001 First query the products endpoint of the API to find out which sites and dates have data available. In the products endpoint, the target is the numbered identifier for the data product: # Load the necessary libraries library(httr) ## Warning: package &#39;httr&#39; was built under R version 3.6.2 library(jsonlite) ## Warning: package &#39;jsonlite&#39; was built under R version 3.6.2 library(dplyr, quietly=T) library(downloader) # Request data using the GET function &amp; the API call req &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.10003.001&quot;) req ## Response [https://data.neonscience.org/api/v0/products/DP1.10003.001] ## Date: 2020-09-08 17:45 ## Status: 200 ## Content-Type: application/json;charset=UTF-8 ## Size: 24.2 kB The object returned from GET() has many layers of information. Entering the name of the object gives you some basic information about what you downloaded. The content() function returns the contents in the form of a highly nested list. This is typical of JSON-formatted data returned by APIs. We can use the names() function to view the different types of information within this list. # View requested data req.content &lt;- content(req, as=&quot;parsed&quot;) names(req.content$data) ## [1] &quot;productCodeLong&quot; &quot;productCode&quot; ## [3] &quot;productCodePresentation&quot; &quot;productName&quot; ## [5] &quot;productDescription&quot; &quot;productStatus&quot; ## [7] &quot;productCategory&quot; &quot;productHasExpanded&quot; ## [9] &quot;productScienceTeamAbbr&quot; &quot;productScienceTeam&quot; ## [11] &quot;productPublicationFormatType&quot; &quot;productAbstract&quot; ## [13] &quot;productDesignDescription&quot; &quot;productStudyDescription&quot; ## [15] &quot;productBasicDescription&quot; &quot;productExpandedDescription&quot; ## [17] &quot;productSensor&quot; &quot;productRemarks&quot; ## [19] &quot;themes&quot; &quot;changeLogs&quot; ## [21] &quot;specs&quot; &quot;keywords&quot; ## [23] &quot;siteCodes&quot; You can see all of the infoamtion by running the line print(req.content), but this will result in a very long printout in your console. Instead, you can view list items individually. Here, we highlight a couple of interesting examples: # View Abstract req.content$data$productAbstract ## [1] &quot;This data product contains the quality-controlled, native sampling resolution data from NEON&#39;s breeding landbird sampling. Breeding landbirds are defined as “smaller birds (usually exclusive of raptors and upland game birds) not usually associated with aquatic habitats” (Ralph et al. 1993). The breeding landbird point counts product provides records of species identification of all individuals observed during the 6-minute count period, as well as metadata which can be used to model detectability, e.g., weather, distances from observers to birds, and detection methods. The NEON point count method is adapted from the Integrated Monitoring in Bird Conservation Regions (IMBCR): Field protocol for spatially-balanced sampling of landbird populations (Hanni et al. 2017; http://bit.ly/2u2ChUB). For additional details, see the user guide, protocols, and science design listed in the Documentation section in [this data product&#39;s details webpage](https://data.neonscience.org/data-products/DP1.10003.001). \\n\\nLatency:\\nThe expected time from data and/or sample collection in the field to data publication is as follows, for each of the data tables (in days) in the downloaded data package. See the Data Product User Guide for more information.\\n \\nbrd_countdata: 120\\n\\nbrd_perpoint: 120\\n\\nbrd_personnel: 120\\n\\nbrd_references: 120&quot; # View Available months and associated URLs for Onaqui, Utah - ONAQ req.content$data$siteCodes[[27]] ## $siteCode ## [1] &quot;ONAQ&quot; ## ## $availableMonths ## $availableMonths[[1]] ## [1] &quot;2017-05&quot; ## ## $availableMonths[[2]] ## [1] &quot;2018-05&quot; ## ## $availableMonths[[3]] ## [1] &quot;2018-06&quot; ## ## $availableMonths[[4]] ## [1] &quot;2019-05&quot; ## ## ## $availableDataUrls ## $availableDataUrls[[1]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2017-05&quot; ## ## $availableDataUrls[[2]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-05&quot; ## ## $availableDataUrls[[3]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-06&quot; ## ## $availableDataUrls[[4]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2019-05&quot; To get a more accessible view of which sites have data for which months, you’ll need to extract data from the nested list. There are a variety of ways to do this, in this tutorial we’ll explore a couple of them. Here we’ll use fromJSON(), in the jsonlite package, which doesn’t fully flatten the nested list, but gets us the part we need. To use it, we need a text version of the content. The text version is not as human readable but is readable by the fromJSON() function. # make this JSON readable -&gt; &quot;text&quot; req.text &lt;- content(req, as=&quot;text&quot;) # Flatten data frame to see available data. avail &lt;- jsonlite::fromJSON(req.text, simplifyDataFrame=T, flatten=T) avail ## $data ## $data$productCodeLong ## [1] &quot;NEON.DOM.SITE.DP1.10003.001&quot; ## ## $data$productCode ## [1] &quot;DP1.10003.001&quot; ## ## $data$productCodePresentation ## [1] &quot;NEON.DP1.10003&quot; ## ## $data$productName ## [1] &quot;Breeding landbird point counts&quot; ## ## $data$productDescription ## [1] &quot;Count, distance from observer, and taxonomic identification of breeding landbirds observed during point counts&quot; ## ## $data$productStatus ## [1] &quot;ACTIVE&quot; ## ## $data$productCategory ## [1] &quot;Level 1 Data Product&quot; ## ## $data$productHasExpanded ## [1] TRUE ## ## $data$productScienceTeamAbbr ## [1] &quot;TOS&quot; ## ## $data$productScienceTeam ## [1] &quot;Terrestrial Observation System (TOS)&quot; ## ## $data$productPublicationFormatType ## [1] &quot;TOS Data Product Type&quot; ## ## $data$productAbstract ## [1] &quot;This data product contains the quality-controlled, native sampling resolution data from NEON&#39;s breeding landbird sampling. Breeding landbirds are defined as “smaller birds (usually exclusive of raptors and upland game birds) not usually associated with aquatic habitats” (Ralph et al. 1993). The breeding landbird point counts product provides records of species identification of all individuals observed during the 6-minute count period, as well as metadata which can be used to model detectability, e.g., weather, distances from observers to birds, and detection methods. The NEON point count method is adapted from the Integrated Monitoring in Bird Conservation Regions (IMBCR): Field protocol for spatially-balanced sampling of landbird populations (Hanni et al. 2017; http://bit.ly/2u2ChUB). For additional details, see the user guide, protocols, and science design listed in the Documentation section in [this data product&#39;s details webpage](https://data.neonscience.org/data-products/DP1.10003.001). \\n\\nLatency:\\nThe expected time from data and/or sample collection in the field to data publication is as follows, for each of the data tables (in days) in the downloaded data package. See the Data Product User Guide for more information.\\n \\nbrd_countdata: 120\\n\\nbrd_perpoint: 120\\n\\nbrd_personnel: 120\\n\\nbrd_references: 120&quot; ## ## $data$productDesignDescription ## [1] &quot;Depending on the size of the site, sampling for this product occurs either at either randomly distributed individual points or grids of nine points each. At larger sites, point count sampling occurs at five to fifteen 9-point grids, with grid centers collocated with distributed base plot centers (where plant, beetle, and/or soil sampling may also occur), if possible. At smaller sites (i.e., sites that cannot accommodate a minimum of 5 grids) point counts occur at the southwest corner (point 21) of 5-25 distributed base plots. Point counts are conducted once per breeding season at large sites and twice per breeding season at smaller sites. Point counts are six minutes long, with each minute tracked by the observer, following a two-minute settling-in period. All birds are recorded to species and sex, whenever possible, and the distance to each individual or flock is measured with a laser rangefinder, except in the case of flyovers.&quot; ## ## $data$productStudyDescription ## [1] &quot;This sampling occurs at all NEON terrestrial sites.&quot; ## ## $data$productBasicDescription ## [1] &quot;The basic package contains the per point metadata table that includes data pertaining to the observer and the weather conditions and the count data table that includes all of the observational data.&quot; ## ## $data$productExpandedDescription ## [1] &quot;The expanded package includes two additional tables and two additional fields within the count data table. The personnel table provides institutional information about each observer, as well as their performance on identification quizzes, where available. The references tables provides the list of resources used by an observer to identify birds. The additional fields in the countdata table are family and nativeStatusCode, which are derived from the NEON master list of birds.&quot; ## ## $data$productSensor ## NULL ## ## $data$productRemarks ## [1] &quot;Queries for this data product will return data collected during the date range specified for `brd_perpoint` and `brd_countdata`, but will return data from all dates for `brd_personnel` (quiz scores may occur over time periods which are distinct from when sampling occurs) and `brd_references` (which apply to a broad range of sampling dates). A record from `brd_perPoint` should have 6+ child records in `brd_countdata`, at least one per pointCountMinute. Duplicates or missing data may exist where protocol and/or data entry aberrations have occurred; users should check data carefully for anomalies before joining tables. Taxonomic IDs of species of concern have been &#39;fuzzed&#39;; see data package readme files for more information.&quot; ## ## $data$themes ## [1] &quot;Organisms, Populations, and Communities&quot; ## ## $data$changeLogs ## NULL ## ## $data$specs ## specId specNumber ## 1 3656 NEON.DOC.000916vC ## 2 2565 NEON_bird_userGuide_vA ## 3 3729 NEON.DOC.014041vJ ## ## $data$keywords ## [1] &quot;vertebrates&quot; &quot;birds&quot; &quot;diversity&quot; ## [4] &quot;taxonomy&quot; &quot;community composition&quot; &quot;distance sampling&quot; ## [7] &quot;avian&quot; &quot;species composition&quot; &quot;population&quot; ## [10] &quot;Aves&quot; &quot;Chordata&quot; &quot;point counts&quot; ## [13] &quot;landbirds&quot; &quot;invasive&quot; &quot;introduced&quot; ## [16] &quot;native&quot; &quot;animals&quot; &quot;Animalia&quot; ## ## $data$siteCodes ## siteCode ## 1 ABBY ## 2 BARR ## 3 BART ## 4 BLAN ## 5 BONA ## 6 CLBJ ## 7 CPER ## 8 DCFS ## 9 DEJU ## 10 DELA ## 11 DSNY ## 12 GRSM ## 13 GUAN ## 14 HARV ## 15 HEAL ## 16 JERC ## 17 JORN ## 18 KONA ## 19 KONZ ## 20 LAJA ## 21 LENO ## 22 MLBS ## 23 MOAB ## 24 NIWO ## 25 NOGP ## 26 OAES ## 27 ONAQ ## 28 ORNL ## 29 OSBS ## 30 PUUM ## 31 RMNP ## 32 SCBI ## 33 SERC ## 34 SJER ## 35 SOAP ## 36 SRER ## 37 STEI ## 38 STER ## 39 TALL ## 40 TEAK ## 41 TOOL ## 42 TREE ## 43 UKFS ## 44 UNDE ## 45 WOOD ## 46 WREF ## 47 YELL ## availableMonths ## 1 2017-05, 2017-06, 2018-06, 2018-07, 2019-05 ## 2 2017-07, 2018-07, 2019-06 ## 3 2015-06, 2016-06, 2017-06, 2018-06, 2019-06 ## 4 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06 ## 5 2017-06, 2018-06, 2018-07, 2019-06 ## 6 2017-05, 2018-04, 2019-04, 2019-05 ## 7 2013-06, 2015-05, 2016-05, 2017-05, 2017-06, 2018-05, 2019-06 ## 8 2017-06, 2017-07, 2018-07, 2019-06, 2019-07 ## 9 2017-06, 2018-06, 2019-06 ## 10 2015-06, 2017-06, 2018-05, 2019-06 ## 11 2015-06, 2016-05, 2017-05, 2018-05, 2019-05 ## 12 2016-06, 2017-05, 2017-06, 2018-05, 2019-05 ## 13 2015-05, 2017-05, 2018-05, 2019-05, 2019-06 ## 14 2015-05, 2015-06, 2016-06, 2017-06, 2018-06, 2019-06 ## 15 2017-06, 2018-06, 2018-07, 2019-06, 2019-07 ## 16 2016-06, 2017-05, 2018-06, 2019-06 ## 17 2017-04, 2017-05, 2018-04, 2018-05, 2019-04 ## 18 2018-05, 2018-06, 2019-06 ## 19 2017-06, 2018-05, 2018-06, 2019-06 ## 20 2017-05, 2018-05, 2019-05, 2019-06 ## 21 2017-06, 2018-05, 2019-06 ## 22 2018-06, 2019-05 ## 23 2015-06, 2017-05, 2018-05, 2019-05 ## 24 2015-07, 2017-07, 2018-07, 2019-07 ## 25 2017-07, 2018-07, 2019-07 ## 26 2017-05, 2017-06, 2018-04, 2018-05, 2019-05 ## 27 2017-05, 2018-05, 2018-06, 2019-05 ## 28 2016-05, 2016-06, 2017-05, 2018-06, 2019-05 ## 29 2016-05, 2017-05, 2018-05, 2019-05 ## 30 2018-04 ## 31 2017-06, 2017-07, 2018-06, 2018-07, 2019-06, 2019-07 ## 32 2015-06, 2016-05, 2016-06, 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06 ## 33 2017-05, 2017-06, 2018-05, 2019-05 ## 34 2017-04, 2018-04, 2019-04 ## 35 2017-05, 2018-05, 2019-05 ## 36 2017-05, 2018-04, 2018-05, 2019-04 ## 37 2016-05, 2016-06, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06 ## 38 2013-06, 2015-05, 2016-05, 2017-05, 2018-05, 2019-05, 2019-06 ## 39 2015-06, 2016-07, 2017-06, 2018-06, 2019-05 ## 40 2017-06, 2018-06, 2019-06, 2019-07 ## 41 2017-06, 2018-07, 2019-06 ## 42 2016-06, 2017-06, 2018-06, 2019-06 ## 43 2017-06, 2018-06, 2019-06 ## 44 2016-06, 2016-07, 2017-06, 2018-06, 2019-06 ## 45 2015-07, 2017-07, 2018-07, 2019-06, 2019-07 ## 46 2018-06, 2019-05, 2019-06 ## 47 2018-06, 2019-06 ## availableDataUrls ## 1 https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2019-05 ## 2 https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2019-06 ## 3 https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2019-06 ## 4 https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2019-06 ## 5 https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2019-06 ## 6 https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2019-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2019-05 ## 7 https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2013-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2019-06 ## 8 https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2019-07 ## 9 https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2019-06 ## 10 https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2019-06 ## 11 https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2019-05 ## 12 https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2019-05 ## 13 https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2019-06 ## 14 https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2019-06 ## 15 https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2019-07 ## 16 https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2019-06 ## 17 https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2017-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2019-04 ## 18 https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2019-06 ## 19 https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2019-06 ## 20 https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2019-06 ## 21 https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2019-06 ## 22 https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2019-05 ## 23 https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2019-05 ## 24 https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2015-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2019-07 ## 25 https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2019-07 ## 26 https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2019-05 ## 27 https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2019-05 ## 28 https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2019-05 ## 29 https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2019-05 ## 30 https://data.neonscience.org/api/v0/data/DP1.10003.001/PUUM/2018-04 ## 31 https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2019-07 ## 32 https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2019-06 ## 33 https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2019-05 ## 34 https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2017-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2019-04 ## 35 https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2019-05 ## 36 https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2019-04 ## 37 https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2019-06 ## 38 https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2013-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2019-06 ## 39 https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2016-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2019-05 ## 40 https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2019-07 ## 41 https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2019-06 ## 42 https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2019-06 ## 43 https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2019-06 ## 44 https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2016-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2019-06 ## 45 https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2019-07 ## 46 https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2019-06 ## 47 https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2019-06 The object contains a lot of information about the data product, including: keywords under $data$keywords, references for documentation under $data$specs, data availability by site and month under $data$siteCodes, and specific URLs for the API calls for each site and month under $data$siteCodes$availableDataUrls. We need $data$siteCodes to tell us what we can download. $data$siteCodes$availableDataUrls allows us to avoid writing the API calls ourselves in the next steps. # get data availability list for the product bird.urls &lt;- unlist(avail$data$siteCodes$availableDataUrls) length(bird.urls) #total number of URLs ## [1] 204 bird.urls[1:10] #show first 10 URLs available ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-05&quot; ## [2] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-06&quot; ## [3] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-06&quot; ## [4] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-07&quot; ## [5] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2019-05&quot; ## [6] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2017-07&quot; ## [7] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2018-07&quot; ## [8] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2019-06&quot; ## [9] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2015-06&quot; ## [10] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2016-06&quot; These are the URLs showing us what files are available for each month where there are data. Let’s look at the bird data from Woodworth (WOOD) site from July 2015. We can do this by using the above code but now specifying which site/date we want using the grep() function. Note that if there were only one month of data from a site, you could leave off the date in the function. If you want data from more than one site/month you need to iterate this code, GET fails if you give it more than one URL. # get data availability for WOOD July 2015 brd &lt;- GET(bird.urls[grep(&quot;WOOD/2015-07&quot;, bird.urls)]) brd.files &lt;- jsonlite::fromJSON(content(brd, as=&quot;text&quot;)) # view just the available data files brd.files$data$files ## name ## 1 NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt ## 2 NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.basic.20191107T152331Z.csv ## 3 NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv ## 4 NEON.D09.WOOD.DP1.10003.001.2015-07.basic.20191107T152331Z.zip ## 5 NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.basic.20191107T152331Z.csv ## 6 NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv ## 7 NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml ## 8 NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv ## 9 NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt ## 10 NEON.Bird_Conservancy_of_the_Rockies.brd_personnel.csv ## 11 NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv ## 12 NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.expanded.20191107T152331Z.csv ## 13 NEON.D09.WOOD.DP1.10003.001.2015-07.expanded.20191107T152331Z.zip ## 14 NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.expanded.20191107T152331Z.csv ## 15 NEON.D09.WOOD.DP1.10003.001.brd_references.expanded.20191107T152331Z.csv ## 16 NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml ## size md5 crc32 ## 1 12784 d84b496cf950b5b96e762473beda563a NA ## 2 23521 f37931d46213246dccf2a161211c9afe NA ## 3 10084 6d15da01c03793da8fc6d871e6659ea8 NA ## 4 67816 4438e5e050fc7be5949457f42089a397 NA ## 5 346679 e0adb3146b5cce59eea09864145efcb1 NA ## 6 7337 e67f1ae72760a63c616ec18108453aaa NA ## 7 70539 df102cb4cfdce092cda3c0942c9d9b67 NA ## 8 10084 6d15da01c03793da8fc6d871e6659ea8 NA ## 9 13063 680a2f53c0a9d1b0ab4f8814bda5b399 NA ## 10 46349 a2c47410a6a0f49d0b1cf95be6238604 NA ## 11 7337 e67f1ae72760a63c616ec18108453aaa NA ## 12 23521 f37931d46213246dccf2a161211c9afe NA ## 13 79998 22e3353dabb8b154768dc2eee9873718 NA ## 14 367402 2ad379ae44f4e87996bdc3dee70a0794 NA ## 15 1012 d76cfc5443ac27a058fab1d319d31d34 NA ## 16 78750 6ba91b6e109ff14d1911dcaad9febeb9 NA ## url ## 1 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=6d5f917b6d688cb4caf459eefe3ed7c8687df86ca66e9edec874d701a58b5612 ## 2 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.basic.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=f9dc1adcc3bf5dc0da9076c672bb922cc689ae1786bfdb4288669fae96bcdc19 ## 3 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=027b0ca1ead94f2fa22464ddc91de0caed90f7d02ef1cf0919b2d1376efe9883 ## 4 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.2015-07.basic.20191107T152331Z.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=c8f4673d10d1584d1c54f7939186ffa1612e95f22217eb6f9ef746ccd7e74b4a ## 5 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.basic.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=34852b450b4a5cc8d8a8afb014d3baad07d38be67a3afbafe17c9697893b29cd ## 6 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=e079787a4bb7899d867ce7c5be46923f999d32b9b11c0bd4fed2172f0b4d5727 ## 7 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=32bbffc2faa6c43a8d2a6eabb228317a09019fa1dd702a37a765888987a994db ## 8 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=54357ef13ec47c7877866032c46de3828591af0cc5880b6a2b88514acf98ec53 ## 9 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=8f86fc4284524271761ea4385add1f9f6b8fa4c5655a2133d67dd79cc31e7887 ## 10 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.Bird_Conservancy_of_the_Rockies.brd_personnel.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=9d052b9b2d83325abe8b19912f5b45ac89fbea836e2315f9c0f072101a2ecc7f ## 11 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=43ec1f82bea2499aaf772d207f03ab514d9d95728a2516d4f74491e45098e7e1 ## 12 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.expanded.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=b835c2ca2be1c51e1a13ab6112fb112823ae94b1921aef8e0931ad4955ed16ff ## 13 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.2015-07.expanded.20191107T152331Z.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=9370bdcdabb83dc7eee75456c4e189ddeecc05de61c5fb07a4dc678f26e27596 ## 14 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.expanded.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=f8e5e4dc1621e74945bde411933687317db93a66d2b2538e1580881b37205ffe ## 15 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_references.expanded.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=d42e2afc2f1780718d6f741e4ce73efb0ff38f91501e635dd4f9b17742f6142e ## 16 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200908T174503Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=11dd7e225f20ef9d99b02160a8b8273e8198d91725e859d58cc98e4364927bd3 In this output, name and url are key fields. It provides us with the names of the files available for this site and month, and URLs where we can get the files. We’ll use the file names to pick which ones we want. The available files include both data and metadata, and both the basic and expanded data packages. Typically the expanded package includes additional quality or uncertainty data, either in additional files or additional fields than in the basic files. Basic and expanded data packages are available for most NEON data products (some only have basic). Metadata are described by file name below. The format for most of the file names is: NEON.[domain number].[site code].[data product ID].[file-specific name]. [date of file creation] Some files omit the domain and site, since they’re not specific to a location, like the data product readme. The date of file creation uses the ISO6801 format, in this case 20170720T182547Z, and can be used to determine whether data have been updated since the last time you downloaded. Available files in our query for July 2015 at Woodworth are all of the following (leaving off the initial NEON.D09.WOOD.10003.001): ~.2015-07.expanded.20170720T182547Z.zip: zip of all files in the expanded package ~.brd_countdata.2015-07.expanded.20170720T182547Z.csv: count data table, expanded package version: counts of birds at each point ~.brd_perpoint.2015-07.expanded.20170720T182547Z.csv: point data table, expanded package version: metadata at each observation point NEON.Bird Conservancy of the Rockies.brd_personnel.csv: personnel data table, accuracy scores for bird observers ~.2015-07.basic.20170720T182547Z.zip: zip of all files in the basic package ~.brd_countdata.2015-07.basic.20170720T182547Z.csv: count data table, basic package version: counts of birds at each point ~.brd_perpoint.2015-07.basic.20170720T182547Z.csv: point data table, basic package version: metadata at each observation point NEON.DP1.10003.001_readme.txt: readme for the data product (not specific to dates or location). Appears twice in the list, since it’s in both the basic and expanded package ~.20150101-20160613.xml: Ecological Metadata Language (EML) file. Appears twice in the list, since it’s in both the basic and expanded package ~.validation.20170720T182547Z.csv: validation file for the data product, lists input data and data entry rules. Appears twice in the list, since it’s in both the basic and expanded package ~.variables.20170720T182547Z.csv: variables file for the data product, lists data fields in downloaded tables. Appears twice in the list, since it’s in both the basic and expanded package We’ll get the data tables for the point data and count data in the basic package. The list of files doesn’t return in the same order every time, so we won’t use position in the list to select. Plus, we want code we can re-use when getting data from other sites and other months. So we select files based on the data table name and the package name. # Get both files brd.count &lt;- read.delim(brd.files$data$files$url [intersect(grep(&quot;countdata&quot;, brd.files$data$files$name), grep(&quot;basic&quot;, brd.files$data$files$name))], sep=&quot;,&quot;) brd.point &lt;- read.delim(brd.files$data$files$url [intersect(grep(&quot;perpoint&quot;, brd.files$data$files$name), grep(&quot;basic&quot;, brd.files$data$files$name))], sep=&quot;,&quot;) Now we have the data and can access it in R. Just to show that the files we pulled have actual data in them, let’s make a quick graphic: # Cluster by species clusterBySp &lt;- brd.count %&gt;% dplyr::group_by(scientificName) %&gt;% dplyr::summarise(total=sum(clusterSize, na.rm=T)) ## `summarise()` ungrouping output (override with `.groups` argument) # Reorder so list is ordered most to least abundance clusterBySp &lt;- clusterBySp[order(clusterBySp$total, decreasing=T),] # Plot barplot(clusterBySp$total, names.arg=clusterBySp$scientificName, ylab=&quot;Total&quot;, cex.names=0.5, las=2) Wow! There are lots of Agelaius phoeniceus (Red-winged Blackbirds) at WOOD in July. 2.11.4 Instrumentation data (IS) The process is essentially the same for sensor data. We’ll do the same series of queries for Soil Temperature, DP1.00041.001. Let’s use data from Moab in June 2017 this time. # Request soil temperature data availability info req.soil &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.00041.001&quot;) # make this JSON readable # Note how we&#39;ve change this from two commands into one here avail.soil &lt;- jsonlite::fromJSON(content(req.soil, as=&quot;text&quot;), simplifyDataFrame=T, flatten=T) # get data availability list for the product temp.urls &lt;- unlist(avail.soil$data$siteCodes$availableDataUrls) # get data availability from location/date of interest tmp &lt;- GET(temp.urls[grep(&quot;MOAB/2017-06&quot;, temp.urls)]) tmp.files &lt;- jsonlite::fromJSON(content(tmp, as=&quot;text&quot;)) length(tmp.files$data$files$name) # There are a lot of available files ## [1] 190 tmp.files$data$files$name[1:10] # Let&#39;s print the first 10 ## [1] &quot;NEON.D13.MOAB.DP1.00041.001.readme.20200620T070859Z.txt&quot; ## [2] &quot;NEON.D13.MOAB.DP1.00041.001.004.503.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [3] &quot;NEON.D13.MOAB.DP1.00041.001.003.504.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [4] &quot;NEON.D13.MOAB.DP1.00041.001.005.501.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [5] &quot;NEON.D13.MOAB.DP1.00041.001.001.509.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [6] &quot;NEON.D13.MOAB.DP1.00041.001.003.509.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [7] &quot;NEON.D13.MOAB.DP1.00041.001.004.505.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [8] &quot;NEON.D13.MOAB.DP1.00041.001.005.502.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [9] &quot;NEON.D13.MOAB.DP1.00041.001.004.509.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [10] &quot;NEON.D13.MOAB.DP1.00041.001.001.504.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; These file names start and end the same way as the observational files, but the middle is a little more cryptic. The structure from beginning to end is: NEON.[domain number].[site code].[data product ID].00000. [soil plot number].[depth].[averaging interval].[data table name]. [year]-[month].[data package].[date of file creation] So “NEON.D13.MOAB.DP1.00041.001.003.507.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv” is the: NEON (NEON.) Domain 13 (.D13.) Moab field site (.MOAB.) soil temperature data (.DP1.00041.001.) collected in Soil Plot 2, (.002.) at the 7th depth below the surface (.507.) and reported as a 30-minute mean of (.030. and .ST_30_minute.) only for the period of June 2017 (.2017-06.) and provided in a expanded data package (.basic.) published on June 20th, 2020 (.0200620T070859Z.). More information about interpreting file names can be found in the readme that accompanies each download. Let’s get data (and the URL) for only the 2nd depth described above by selecting 002.502.030 and the word basic in the file name. Go get it: soil.temp &lt;- read.delim(tmp.files$data$files$url [intersect(grep(&quot;002.502.030&quot;, tmp.files$data$files$name), grep(&quot;basic&quot;, tmp.files$data$files$name))], sep=&quot;,&quot;) Now we have the data and can use it to conduct our analyses. To take a quick look at it, let’s plot the mean soil temperature by date. # plot temp ~ date plot(soil.temp$soilTempMean~as.POSIXct(soil.temp$startDateTime, format=&quot;%Y-%m-%d T %H:%M:%S Z&quot;), pch=&quot;.&quot;, xlab=&quot;Date&quot;, ylab=&quot;T&quot;) As we’d expect we see daily fluctuation in soil temperature. 2.11.5 Remote sensing data (AOP) Again, the process of determining which sites and time periods have data, and finding the URLs for those data, is the same as for the other data types. We’ll go looking for High resolution orthorectified camera imagery, DP1.30010, and we’ll look at the flight over San Joaquin Experimental Range (SJER) in March 2017. # Request camera data availability info req.aop &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.30010.001&quot;) # make this JSON readable # Note how we&#39;ve changed this from two commands into one here avail.aop &lt;- jsonlite::fromJSON(content(req.aop, as=&quot;text&quot;), simplifyDataFrame=T, flatten=T) # get data availability list for the product cam.urls &lt;- unlist(avail.aop$data$siteCodes$availableDataUrls) # get data availability from location/date of interest cam &lt;- GET(cam.urls[intersect(grep(&quot;SJER&quot;, cam.urls), grep(&quot;2017&quot;, cam.urls))]) cam.files &lt;- jsonlite::fromJSON(content(cam, as=&quot;text&quot;)) # this list of files is very long, so we&#39;ll just look at the first ten head(cam.files$data$files$name, 10) ## [1] &quot;17032816_EH021656(20170328193201)-0898_ort.tif&quot; ## [2] &quot;17032816_EH021656(20170328191341)-0745_ort.tif&quot; ## [3] &quot;17032816_EH021656(20170328175804)-0128_ort.tif&quot; ## [4] &quot;17032816_EH021656(20170328185313)-0567_ort.tif&quot; ## [5] &quot;17032816_EH021656(20170328194634)-1017_ort.tif&quot; ## [6] &quot;17032816_EH021656(20170328174658)-0026_ort.tif&quot; ## [7] &quot;17032816_EH021656(20170328191932)-0794_ort.tif&quot; ## [8] &quot;17032816_EH021656(20170328195229)-1073_ort.tif&quot; ## [9] &quot;17032816_EH021656(20170328184706)-0517_ort.tif&quot; ## [10] &quot;17032816_EH021656(20170328190156)-0650_ort.tif&quot; File names for AOP data are more variable than for IS or OS data; different AOP data products use different naming conventions. File formats differ by product as well. This particular product, camera imagery, is stored in TIFF files. Instead of reading a TIFF into R, we’ll download it to the working directory. This is one option for getting AOP files from the API. To download the TIFF file, we use the downloader package, and we’ll select a file based on the time stamp in the file name: 20170328192931 download(cam.files$data$files$url[grep(&quot;20170328192931&quot;, cam.files$data$files$name)], paste(getwd(), &quot;/SJER_image.tif&quot;, sep=&quot;&quot;), mode=&quot;wb&quot;) The image, below, of the San Joaquin Experimental Range should now be in your working directory. An example of camera data (DP1.30010.001) from the San Joaquin Experimental Range. Source: National Ecological Observatory Network (NEON) 2.11.6 Geolocation data You may have noticed some of the spatial data referenced above are a bit vague, e.g. “soil plot 2, 4th depth below the surface.” This section describes how to get spatial data and what to do with it depends on which type of data you’re working with. 2.11.6.1 Instrumentation data (both aquatic and terrestrial) Downloads of instrument system (IS) data include a file called sensor_positions.csv. The sensor positions file contains information about the coordinates of each sensor, relative to a reference location. While the specifics vary, techniques are generalizable for working with sensor data and the sensor_positions.csv file. Let’s look at the sensor locations for photosynthetically active radiation (PAR; DP1.00024.001) at the NEON Treehaven site (TREE) in July 2018. To reduce our file size, we’ll use the 30 minute averaging interval. Our final product from this section is to create a spatially explicit picture of light attenuation through the canopy. # load PAR data of interest par &lt;- loadByProduct(dpID=&quot;DP1.00024.001&quot;, site=&quot;TREE&quot;, startdate=&quot;2018-07&quot;, enddate=&quot;2018-07&quot;, avg=30, check.size=F, token=NEON_TOKEN) ## Finding available files ## | | | 0% | |======================================================================| 100% ## ## Downloading files totaling approximately 934.7 KiB ## Downloading 9 files ## | | | 0% | |========= | 12% | |================== | 25% | |========================== | 38% | |=================================== | 50% | |============================================ | 62% | |==================================================== | 75% | |============================================================= | 88% | |======================================================================| 100% ## ## Stacking operation across a single core. ## Stacking table PARPAR_30min ## Merged the most recent publication of sensor position files for each site and saved to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 1 data tables and 2 metadata tables! ## Stacking took 0.09035087 secs ## All unzipped monthly data folders have been removed. Now we can specifically look at the sensor positions file: # create object for sens. pos. file pos &lt;- par$sensor_positions_00024 # view names names(pos) ## [1] &quot;siteID&quot; &quot;HOR.VER&quot; &quot;name&quot; ## [4] &quot;description&quot; &quot;start&quot; &quot;end&quot; ## [7] &quot;referenceName&quot; &quot;referenceDescription&quot; &quot;referenceStart&quot; ## [10] &quot;referenceEnd&quot; &quot;xOffset&quot; &quot;yOffset&quot; ## [13] &quot;zOffset&quot; &quot;pitch&quot; &quot;roll&quot; ## [16] &quot;azimuth&quot; &quot;referenceLatitude&quot; &quot;referenceLongitude&quot; ## [19] &quot;referenceElevation&quot; &quot;publicationDate&quot; The sensor locations are indexed by the HOR.VER variable - see the file naming conventions page for more details. Using unique() we can view all the locations indexes in this file. # view names unique(pos$HOR.VER) ## [1] &quot;000.010&quot; &quot;000.020&quot; &quot;000.030&quot; &quot;000.040&quot; &quot;000.050&quot; &quot;000.060&quot; PAR data are collected at multiple levels of the NEON tower but along a single vertical plane. We see this reflected in the data where HOR=000 (all data collected) at the tower location. The VER index varies (VER = 010 to 060) showing that the vertical position is changing and that PAR is measured at six different levels. The x, y, and z offsets in the sensor positions file are the relative distance, in meters, to the reference latitude, longitude, and elevation in the file. The HOR and VER indices in the sensor positions file correspond to the verticalPosition and horizontalPosition fields in par$PARPAR_30min. Say we wanted to plot a profile of the PAR through the canopy, we would need to start by using the aggregate() function to calculate mean PAR at each vertical position on the tower over the month: # calc mean PAR at each level parMean &lt;- aggregate(par$PARPAR_30min$PARMean, by=list(par$PARPAR_30min$verticalPosition), FUN=mean, na.rm=T) Now we can plot mean PAR relative to height on the tower (or the zOffset): # plot PAR plot(parMean$x, parMean$Group.1, type=&quot;b&quot;, pch=20, xlab=&quot;Photosynthetically active radiation&quot;, ylab=&quot;Height above tower base (m)&quot;) 2.11.6.2 Observational data - Terrestrial Latitude, longitude, elevation, and associated uncertainties are included in data downloads (Remember NEON COding Lab part 1?). These are the coordinates and uncertainty of the sampling plot; for many protocols it is possible to calculate a more precise location. Instructions for doing this are in the respective data product user guides, and code is in the geoNEON package on GitHub. 2.11.7 Querying a single named location Let’s look at the named locations in the bird data we downloaded above. To do this, look for the field called namedLocation, which is present in all observational data products, both aquatic and terrestrial. # view named location head(brd.point$namedLocation) ## [1] &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; ## [4] &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; Here we see the first six entries in the namedLocation column which tells us the names of the Terrestrial Observation plots where the bird surveys were conducted. We can query the locations endpoint of the API for the first named location, WOOD_013.birdGrid.brd. # location data req.loc &lt;- GET(&quot;http://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd&quot;) # make this JSON readable brd.WOOD_013 &lt;- jsonlite::fromJSON(content(req.loc, as=&quot;text&quot;)) brd.WOOD_013 ## $data ## $data$locationName ## [1] &quot;WOOD_013.birdGrid.brd&quot; ## ## $data$locationDescription ## [1] &quot;Plot \\&quot;WOOD_013\\&quot; at site \\&quot;WOOD\\&quot;&quot; ## ## $data$locationType ## [1] &quot;OS Plot - brd&quot; ## ## $data$domainCode ## [1] &quot;D09&quot; ## ## $data$siteCode ## [1] &quot;WOOD&quot; ## ## $data$locationDecimalLatitude ## [1] 47.13912 ## ## $data$locationDecimalLongitude ## [1] -99.23243 ## ## $data$locationElevation ## [1] 579.31 ## ## $data$locationUtmEasting ## [1] 482375.7 ## ## $data$locationUtmNorthing ## [1] 5220650 ## ## $data$locationUtmHemisphere ## [1] &quot;N&quot; ## ## $data$locationUtmZone ## [1] 14 ## ## $data$alphaOrientation ## [1] 0 ## ## $data$betaOrientation ## [1] 0 ## ## $data$gammaOrientation ## [1] 0 ## ## $data$xOffset ## [1] 0 ## ## $data$yOffset ## [1] 0 ## ## $data$zOffset ## [1] 0 ## ## $data$offsetLocation ## NULL ## ## $data$locationProperties ## locationPropertyName locationPropertyValue ## 1 Value for Coordinate source GeoXH 6000 ## 2 Value for Coordinate uncertainty 0.28 ## 3 Value for Country unitedStates ## 4 Value for County Stutsman ## 5 Value for Elevation uncertainty 0.48 ## 6 Value for Filtered positions 121 ## 7 Value for Geodetic datum WGS84 ## 8 Value for Horizontal dilution of precision 1 ## 9 Value for Maximum elevation 579.31 ## 10 Value for Minimum elevation 569.79 ## 11 Value for National Land Cover Database (2001) grasslandHerbaceous ## 12 Value for Plot dimensions 500m x 500m ## 13 Value for Plot ID WOOD_013 ## 14 Value for Plot size 250000 ## 15 Value for Plot subtype birdGrid ## 16 Value for Plot type distributed ## 17 Value for Positional dilution of precision 2.4 ## 18 Value for Reference Point Position B2 ## 19 Value for Slope aspect 238.91 ## 20 Value for Slope gradient 2.83 ## 21 Value for Soil type order Mollisols ## 22 Value for State province ND ## 23 Value for Subtype Specification ninePoints ## 24 Value for UTM Zone 14N ## ## $data$locationParent ## [1] &quot;WOOD&quot; ## ## $data$locationParentUrl ## [1] &quot;https://data.neonscience.org/api/v0/locations/WOOD&quot; ## ## $data$locationChildren ## [1] &quot;WOOD_013.birdGrid.brd.B2&quot; &quot;WOOD_013.birdGrid.brd.A2&quot; ## [3] &quot;WOOD_013.birdGrid.brd.C3&quot; &quot;WOOD_013.birdGrid.brd.A3&quot; ## [5] &quot;WOOD_013.birdGrid.brd.B3&quot; &quot;WOOD_013.birdGrid.brd.C1&quot; ## [7] &quot;WOOD_013.birdGrid.brd.A1&quot; &quot;WOOD_013.birdGrid.brd.B1&quot; ## [9] &quot;WOOD_013.birdGrid.brd.C2&quot; ## ## $data$locationChildrenUrls ## [1] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B2&quot; ## [2] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A2&quot; ## [3] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C3&quot; ## [4] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A3&quot; ## [5] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B3&quot; ## [6] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C1&quot; ## [7] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A1&quot; ## [8] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B1&quot; ## [9] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C2&quot; Note spatial information under $data$[nameOfCoordinate] and under $data$locationProperties. Also note $data$locationChildren: these are the finer scale locations that can be used to calculate precise spatial data for bird observations. For convenience, we’ll use the geoNEON package to make the calculations. First we’ll use getLocByName() to get the additional spatial information available through the API, and look at the spatial resolution available in the initial download: # load the geoNEON package library(geoNEON) # extract the spatial data brd.point.loc &lt;- getLocByName(brd.point) ## | | | 0% | |========== | 14% | |==================== | 29% | |============================== | 43% | |======================================== | 57% | |================================================== | 71% | |============================================================ | 86% | |======================================================================| 100% # plot bird point locations # note that decimal degrees is also an option in the data symbols(brd.point.loc$easting, brd.point.loc$northing, circles=brd.point.loc$coordinateUncertainty, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;, tck=0.01, inches=F) And use getLocTOS() to calculate the point locations of observations. brd.point.pt &lt;- getLocTOS(brd.point, &quot;brd_perpoint&quot;) ## | | | 0% | |= | 2% | |== | 3% | |=== | 5% | |==== | 6% | |====== | 8% | |======= | 10% | |======== | 11% | |========= | 13% | |========== | 14% | |=========== | 16% | |============ | 17% | |============= | 19% | |============== | 21% | |================ | 22% | |================= | 24% | |================== | 25% | |=================== | 27% | |==================== | 29% | |===================== | 30% | |====================== | 32% | |======================= | 33% | |======================== | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 40% | |============================= | 41% | |============================== | 43% | |=============================== | 44% | |================================ | 46% | |================================= | 48% | |================================== | 49% | |==================================== | 51% | |===================================== | 52% | |====================================== | 54% | |======================================= | 56% | |======================================== | 57% | |========================================= | 59% | |========================================== | 60% | |=========================================== | 62% | |============================================ | 63% | |============================================== | 65% | |=============================================== | 67% | |================================================ | 68% | |================================================= | 70% | |================================================== | 71% | |=================================================== | 73% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 78% | |======================================================== | 79% | |========================================================= | 81% | |========================================================== | 83% | |=========================================================== | 84% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================== | 94% | |=================================================================== | 95% | |==================================================================== | 97% | |===================================================================== | 98% | |======================================================================| 100% # plot bird point locations # note that decimal degrees is also an option in the data symbols(brd.point.pt$adjEasting, brd.point.pt$adjNorthing, circles=brd.point.pt$adjCoordinateUncertainty, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;, tck=0.01, inches=F) Now you can see the individual points where the respective point counts were located. 2.11.8 Taxonomy NEON maintains accepted taxonomies for many of the taxonomic identification data we collect. NEON taxonomies are available for query via the API; they are also provided via an interactive user interface, the Taxon Viewer. NEON taxonomy data provides the reference information for how NEON validates taxa; an identification must appear in the taxonomy lists in order to be accepted into the NEON database. Additions to the lists are reviewed regularly. The taxonomy lists also provide the author of the scientific name, and the reference text used. The taxonomy endpoint of the API works a little bit differently from the other endpoints. In the “Anatomy of an API Call” section above, each endpoint has a single type of target - a data product number, a named location name, etc. For taxonomic data, there are multiple query options, and some of them can be used in combination. For example, a query for taxa in the Pinaceae family: http://data.neonscience.org/api/v0/taxonomy/?family=Pinaceae The available types of queries are listed in the taxonomy section of the API web page. Briefly, they are: taxonTypeCode: Which of the taxonomies maintained by NEON are you looking for? BIRD, FISH, PLANT, etc. Cannot be used in combination with the taxonomic rank queries. each of the major taxonomic ranks from genus through kingdom scientificname: Genus + specific epithet (+ authority). Search is by exact match only, see final example below. verbose: Do you want the short (false) or long (true) response offset: Skip this number of items in the list. Defaults to 50. limit: Result set will be truncated at this length. Defaults to 50. Staff on the NEON project have plans to modify the settings for offset and limit, such that offset will default to 0 and limit will default to ∞, but in the meantime users will want to set these manually. They are set to non-default values in the examples below. For the first example, let’s query for the loon family, Gaviidae, in the bird taxonomy. Note that query parameters are case-sensitive. loon.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?family=Gaviidae&amp;offset=0&amp;limit=500&quot;) Parse the results into a list using fromJSON(): loon.list &lt;- jsonlite::fromJSON(content(loon.req, as=&quot;text&quot;)) And look at the $data element of the results, which contains: The full taxonomy of each taxon The short taxon code used by NEON (taxonID/acceptedTaxonID) The author of the scientific name (scientificNameAuthorship) The vernacular name, if applicable The reference text used (nameAccordingToID) The terms used for each field are matched to Darwin Core (dwc) and the Global Biodiversity Information Facility (gbif) terms, where possible, and the matches are indicated in the column headers. loon.list$data ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 BIRD ARLO ARLO Gavia arctica ## 2 BIRD COLO COLO Gavia immer ## 3 BIRD PALO PALO Gavia pacifica ## 4 BIRD RTLO RTLO Gavia stellata ## 5 BIRD YBLO YBLO Gavia adamsii ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 (Linnaeus) species Arctic Loon ## 2 (Brunnich) species Common Loon ## 3 (Lawrence) species Pacific Loon ## 4 (Pontoppidan) species Red-throated Loon ## 5 (G. R. Gray) species Yellow-billed Loon ## dwc:nameAccordingToID dwc:kingdom dwc:phylum dwc:class dwc:order ## 1 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 2 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 3 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 4 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 5 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## dwc:family dwc:genus gbif:subspecies gbif:variety ## 1 Gaviidae Gavia NA NA ## 2 Gaviidae Gavia NA NA ## 3 Gaviidae Gavia NA NA ## 4 Gaviidae Gavia NA NA ## 5 Gaviidae Gavia NA NA To get the entire list for a particular taxonomic type, use the taxonTypeCode query. Be cautious with this query, the PLANT taxonomic list has several hundred thousand entries. For an example, let’s look up the small mammal taxonomic list, which is one of the shorter ones, and use the verbose=true option to see a more extensive list of taxon data, including many taxon ranks that aren’t populated for these taxa. For space here, we display only the first 10 taxa: mam.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?taxonTypeCode=SMALL_MAMMAL&amp;offset=0&amp;limit=500&amp;verbose=true&quot;) mam.list &lt;- jsonlite::fromJSON(content(mam.req, as=&quot;text&quot;)) mam.list$data[1:10,] ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 SMALL_MAMMAL AMHA AMHA Ammospermophilus harrisii ## 2 SMALL_MAMMAL AMIN AMIN Ammospermophilus interpres ## 3 SMALL_MAMMAL AMLE AMLE Ammospermophilus leucurus ## 4 SMALL_MAMMAL AMLT AMLT Ammospermophilus leucurus tersus ## 5 SMALL_MAMMAL AMNE AMNE Ammospermophilus nelsoni ## 6 SMALL_MAMMAL AMSP AMSP Ammospermophilus sp. ## 7 SMALL_MAMMAL APRN APRN Aplodontia rufa nigra ## 8 SMALL_MAMMAL APRU APRU Aplodontia rufa ## 9 SMALL_MAMMAL ARAL ARAL Arborimus albipes ## 10 SMALL_MAMMAL ARLO ARLO Arborimus longicaudus ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 Audubon and Bachman species Harriss Antelope Squirrel ## 2 Merriam species Texas Antelope Squirrel ## 3 Merriam species Whitetailed Antelope Squirrel ## 4 Goldman subspecies &lt;NA&gt; ## 5 Merriam species Nelsons Antelope Squirrel ## 6 &lt;NA&gt; genus &lt;NA&gt; ## 7 Taylor subspecies &lt;NA&gt; ## 8 Rafinesque species Sewellel ## 9 Merriam species Whitefooted Vole ## 10 True species Red Tree Vole ## taxonProtocolCategory dwc:nameAccordingToID ## 1 opportunistic isbn: 978 0801882210 ## 2 opportunistic isbn: 978 0801882210 ## 3 opportunistic isbn: 978 0801882210 ## 4 opportunistic isbn: 978 0801882210 ## 5 opportunistic isbn: 978 0801882210 ## 6 opportunistic isbn: 978 0801882210 ## 7 non-target isbn: 978 0801882210 ## 8 non-target isbn: 978 0801882210 ## 9 target isbn: 978 0801882210 ## 10 target isbn: 978 0801882210 ## dwc:nameAccordingToTitle ## 1 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 2 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 3 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 4 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 5 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 6 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 7 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 8 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 9 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 10 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## dwc:kingdom gbif:subkingdom gbif:infrakingdom gbif:superdivision ## 1 Animalia NA NA NA ## 2 Animalia NA NA NA ## 3 Animalia NA NA NA ## 4 Animalia NA NA NA ## 5 Animalia NA NA NA ## 6 Animalia NA NA NA ## 7 Animalia NA NA NA ## 8 Animalia NA NA NA ## 9 Animalia NA NA NA ## 10 Animalia NA NA NA ## gbif:division gbif:subdivision gbif:infradivision gbif:parvdivision ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## 7 NA NA NA NA ## 8 NA NA NA NA ## 9 NA NA NA NA ## 10 NA NA NA NA ## gbif:superphylum dwc:phylum gbif:subphylum gbif:infraphylum gbif:superclass ## 1 NA Chordata NA NA NA ## 2 NA Chordata NA NA NA ## 3 NA Chordata NA NA NA ## 4 NA Chordata NA NA NA ## 5 NA Chordata NA NA NA ## 6 NA Chordata NA NA NA ## 7 NA Chordata NA NA NA ## 8 NA Chordata NA NA NA ## 9 NA Chordata NA NA NA ## 10 NA Chordata NA NA NA ## dwc:class gbif:subclass gbif:infraclass gbif:superorder dwc:order ## 1 Mammalia NA NA NA Rodentia ## 2 Mammalia NA NA NA Rodentia ## 3 Mammalia NA NA NA Rodentia ## 4 Mammalia NA NA NA Rodentia ## 5 Mammalia NA NA NA Rodentia ## 6 Mammalia NA NA NA Rodentia ## 7 Mammalia NA NA NA Rodentia ## 8 Mammalia NA NA NA Rodentia ## 9 Mammalia NA NA NA Rodentia ## 10 Mammalia NA NA NA Rodentia ## gbif:suborder gbif:infraorder gbif:section gbif:subsection gbif:superfamily ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## 7 NA NA NA NA NA ## 8 NA NA NA NA NA ## 9 NA NA NA NA NA ## 10 NA NA NA NA NA ## dwc:family gbif:subfamily gbif:tribe gbif:subtribe dwc:genus ## 1 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 2 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 3 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 4 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 5 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 6 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 7 Aplodontiidae &lt;NA&gt; &lt;NA&gt; NA Aplodontia ## 8 Aplodontiidae &lt;NA&gt; &lt;NA&gt; NA Aplodontia ## 9 Cricetidae Arvicolinae &lt;NA&gt; NA Arborimus ## 10 Cricetidae Arvicolinae &lt;NA&gt; NA Arborimus ## dwc:subgenus gbif:subspecies gbif:variety gbif:subvariety gbif:form ## 1 &lt;NA&gt; NA NA NA NA ## 2 &lt;NA&gt; NA NA NA NA ## 3 &lt;NA&gt; NA NA NA NA ## 4 &lt;NA&gt; NA NA NA NA ## 5 &lt;NA&gt; NA NA NA NA ## 6 &lt;NA&gt; NA NA NA NA ## 7 &lt;NA&gt; NA NA NA NA ## 8 &lt;NA&gt; NA NA NA NA ## 9 &lt;NA&gt; NA NA NA NA ## 10 &lt;NA&gt; NA NA NA NA ## gbif:subform speciesGroup dwc:specificEpithet dwc:infraspecificEpithet ## 1 NA &lt;NA&gt; harrisii &lt;NA&gt; ## 2 NA &lt;NA&gt; interpres &lt;NA&gt; ## 3 NA &lt;NA&gt; leucurus &lt;NA&gt; ## 4 NA &lt;NA&gt; leucurus tersus ## 5 NA &lt;NA&gt; nelsoni &lt;NA&gt; ## 6 NA &lt;NA&gt; sp. &lt;NA&gt; ## 7 NA &lt;NA&gt; rufa nigra ## 8 NA &lt;NA&gt; rufa &lt;NA&gt; ## 9 NA &lt;NA&gt; albipes &lt;NA&gt; ## 10 NA &lt;NA&gt; longicaudus &lt;NA&gt; To get information about a single taxon, use the scientificname query. This query will not do a ‘fuzzy match’, so you need to query the exact name of the taxon in the NEON taxonomy. Because of this, the query will be most useful when you already have NEON data in hand and are looking for more information about a specific taxon. Querying on scientificname is unlikely to be an efficient way to figure out if NEON recognizes a particular taxon. In addition, scientific names contain spaces, which are not allowed in a URL. The spaces need to be replaced with the URL encoding replacement, %20. For an example, let’s look up the little sand verbena, Abronia minor Standl. Searching for Abronia minor will fail, because the NEON taxonomy for this species includes the authority. The search will also fail with spaces. Search for Abronia%20minor%20Standl., and in this case we can omit offset and limit because we know there can only be a single result: am.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?scientificname=Abronia%20minor%20Standl.&quot;) am.list &lt;- jsonlite::fromJSON(content(am.req, as=&quot;text&quot;)) am.list$data ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 PLANT ABMI2 ABMI2 Abronia minor Standl. ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 Standl. species little sand verbena ## dwc:nameAccordingToID dwc:kingdom dwc:phylum ## 1 http://plants.usda.gov (accessed 8/25/2014) Plantae Magnoliophyta ## dwc:class dwc:order dwc:family dwc:genus gbif:subspecies ## 1 Magnoliopsida Caryophyllales Nyctaginaceae Abronia NA ## gbif:variety ## 1 NA 2.12 Stacking NEON data At the top of this tutorial, we installed the neonUtilities package. This is a custom R package that stacks the monthly files provided by the NEON data portal into a single continuous file for each type of data table in the download. It currently handles files downloaded from the data portal, but not files pulled from the API. For a guide to using neonUtilities on data downloaded from the portal, look here. 2.13 Intro to NEON Exercises: Written Questions 2.14 Intro to NEON Exercises Part 2 2.14.1 Written Suggested Timing: Complete this exercise before our next class meeting &gt;&gt;&gt;&gt;&gt;&gt;&gt; upstream/master Question 1: How does NEON address ‘dark data’ (Chapter 1)? Question 2: How might or does the NEON project intersect with your current research or future career goals? (1 paragraph) Question 3: Use the map in Chapter 2:Intro to NEON to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: Are there NEON field sites that are in study regions of interest to you? What domains are the sites located in? What NEON field sites do your current research or Capstone Project ideas coincide with? Is the site or sites core or relocatable? Are they terrestrial or aquatic? Are there data available for the NEON field site(s) that you are most interested in? What kind of data are available? Question 4: Consider either your current or future research, or a question you’d like to address durring this course and answer each of the following questions: Which types of NEON data may be more useful to address these questions? What non-NEON data resources could be combined with NEON data to help address your question? What challenges, if any, could you foresee when beginning to work with these data? Question 5: Use the Data Portal tools to investigate the data availability for the field sites you’ve already identified in the previous sections and answer each of the following questions: What types of aquatic or terrestrial data are currently available? Remote sensing data? Of these, what type of data are you most interested in working with for your project during this course? For what time period does the data cover? What format is the downloadable file available in? Where is the metadata to support this data? 2.14.2 NEON Coding Lab Part 2 2.15 NEON Coding Lab Part 2 Suggested Timing: Complete this exercise a few days before your NEON clumination write up Use the answers that you’ve provided above to select a single NEON site. e.g. ONAQ Use the answers that you’ve provided above to select 3 NEON data products from either the TOS, TIS or ARS (AOP) collection methods. Sumarize each product with its NEON identifier, along with a sumarry. e.g.: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD **DP1.10055.001**: Plant phenology observations: phenophase status and insensity of tagged plants. This data product contains the quality-controlled, native sampling resolution data from in-situ observations of plant leaf development and reproductive phenophases, at **D15.ONAQ**. Using the NEON Ulitites package or the API pull in those data along with metadata. Organize your data into data.frames and produce summaries for each of your data: Filter your data based on metadata and quality flags: DP1.10055.001: Plant phenology observations: phenophase status and intensity of tagged plants. This data product contains the quality-controlled, native sampling resolution data from in-situ observations of plant leaf development and reproductive phenophases, at D15.ONAQ. Here I will focus on the phenophase intensity data, which is a measure of how prevalent that particular phenophase is in the sampled plants. Using the NEON Ulitites package or the API pull in those data along with metadata. sitesOfInterest &lt;- c(&quot;ONAQ&quot;) dpid &lt;- as.character(&#39;DP1.10055.001&#39;) #phe data pheDat &lt;- loadByProduct(dpID=&quot;DP1.10055.001&quot;, site = sitesOfInterest, package = &quot;basic&quot;, check.size = FALSE, token=NEON_TOKEN) Organize your data into data.frames and produce summaries for each of your data: #NEON sends the data as a nested list, so I need to undo that # unlist all data frames list2env(pheDat ,.GlobalEnv) summary(phe_perindividualperyear) summary(phe_statusintensity) Filter and format your data based on metadata and quality flags: #remove duplicate records phe_statusintensity &lt;- select(phe_statusintensity, -uid) phe_statusintensity &lt;- distinct(phe_statusintensity) #Format dates (native format is &#39;factor&#39; silly R) phe_statusintensity$date &lt;- as.Date(phe_statusintensity$date, &quot;%Y-%m-%d&quot;) phe_statusintensity$editedDate &lt;- as.Date(phe_statusintensity$editedDate, &quot;%Y-%m-%d&quot;) phe_statusintensity$year &lt;- substr(phe_statusintensity$date, 1, 4) phe_statusintensity$monthDay &lt;- format(phe_statusintensity$date, format=&quot;%m-%d&quot;) Now I want to remove NA values so I can see what’s really going on: phe_statusintensity=phe_statusintensity%&gt;% filter(!is.na(phe_statusintensity$phenophaseIntensity)) Create minimum of 1 plot per data type (minimum of 3 plots total). These will vary based on that data that you’ve chosen. A non-exhastive list of ideas: 1. Your data as a function of height on the tower (FPAR example) 2. A map of the locations where your data is sampled (TOS tree example, bird example) 3. A model based on the data you’re interested in working work (Coding lab 1 example) 4. A timeseries of your data (example below) What is the frequency of the data you decided was of interest? How do the data align to answer a central question? What challenges did you run into when investigating these data? How will you address these challenges and document your code? One to two paragraphs Intro to NEON Culmination Activity What is the temporal frequency of observations in the data you decided was of interest? How do the data align to answer a central question? What challenges did you run into when investigating these data? How will you address these challenges and document your code? One to two paragraphs 2.16 Intro to NEON Culmination Activity Due before we start Chapter 3: USA-NPN &gt;&gt;&gt;&gt;&gt;&gt;&gt; upstream/master Write up a 1-page summary of a project that you might want to explore using NEON data over the duration of this course. Include: the types of NEON (and other data) that you will need to implement this project, including data product id numbers. If in your NEON coding lab part 2 you highlighted challenges to using these data, discuss methods to address those challenges. *e.g. If your site doesn’t yet have a long data recocrd, is it located close to a longer lived site from another network? (LTER, Ameriflux, LTAR etc) One high-level summary graphic including all of your data from the NEON Coding Lab Part 2 Save this summary as you will be refining and adding to your ideas over the course of the semester. "],
["introduction-to-usa-npn-its-data.html", "Chapter 3 Introduction to USA-NPN &amp; its Data 3 USA-NPN Learning Objectives 3 USA-NPN Project Mission &amp; Design: 3 Vision &amp; Mission 3 USA-NPN’s Spatial design: 3 Types of USA-NPN Data: 3 How to Access USA-NPN Data: 3 USA-NPN Written Questions 3.1 Hands on: Accessing USA-NPN Data via rNPN 3.2 Accumulated Growing Degree Day Products 3.3 Extended Spring Indices 3.4 Putting it all together: 3.5 Combine Point and Raster Data 3.6 Live Demo Code with Lee Marsh of USA-NPN 3.7 USA-NPN Coding Lab 3.8 NEON TOS Phenology Data Lecture 3.9 Intro to USA-NPN Culmination Activity", " Chapter 3 Introduction to USA-NPN &amp; its Data Estimated Time: 2 hours Course participants: As you review this information, please consider the final course project that you will build upon over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 3 USA-NPN Learning Objectives At the end of this activity, you will be able to: Understand the mission and purpose of the USA-National Phenology Network (USA-NPN) and the nature of the citizen science program from which the data is derived Access all of the various tools &amp; resources that are available to pull USA-NPN geospatial and observational data Effectively use the rNPN package to integrate and analyze NPN data with other similar datasets 3 USA-NPN Project Mission &amp; Design: The USA National Phenology Network (USA-NPN) collects, organizes, and shares phenological data and information to aid decision-making, scientific discovery, and a broader understanding of phenology from a diversity of perspectives. The USA National Phenology Network consists of a National Coordinating Office (NCO), thousands of volunteer observers and many partners, including research scientists, resource managers, educators, and policy-makers. Anyone who participates in Nature’s Notebook or collaborates with NCO staff to advance the science of phenology or to inform decisions is part of the USA-NPN. 3 Vision &amp; Mission USA-NPN’s vision is to provide data and information on the timing of seasonal events in plants and animals to ensure the well-being of humans, ecosystems, and natural resources. To support this and its mission the USA-NPN collects, organizes, and shares phenological data and information to aid decision-making, scientific discovery, and a broader understanding of phenology from a diversity of perspectives. 3 Relevant documents &amp; background information: USA-NPN Strategic Plan USA-NPN Information Sheet: Tracking seasonal changes to support science, natural resource management, and society 2019 USA-NPN Annual Report 3 USA-NPN’s Spatial design: Phenology datasets that are best suited for supporting scientific discovery and decision making are those that consist of observations of multiple life-cycle stages collected at regular intervals at the same locations over multiple years. The USA-NPN collects, stores, and shares high-quality observations of plant and animal phenology at a national scale by engaging observers in Nature’s Notebook, a national-scale, multi-taxon phenology observing program appropriate for both professional and volunteer participants. Because observations are entirely voluntary, the sampling design for observations is opportunistic. The Nature’s Notebook program has been adopted widely; data are collected at over 100 academic institutions, 78 National Ecological Observatory Network (NEON) sites, and by hundreds of researchers to contribute observations to support scientific discovery. The program is also used by tens of thousands of individual observers and members of federal, state, NGO, and private sector organizations as well as K-12 and higher-ed institutions. A unique aspect of Nature’s Notebook is that monitoring can be undertaken by individuals as well as by community or regionally-organized groups referred to as Local Phenology Programs (LPP). Organizations such as nature centers, arboreta, land conservancies, and National Wildlife Refuges use Nature’s Notebook to meet a diversity of outcomes, including asking and answering scientific questions about the impact of environmental change, informing natural resource management and decision-making, and educating and engaging the public. 3 Types of USA-NPN Data: 3 Observational Observational phenology data, consisting of observations made of phenological status on individual organisms, are collected and submitted by professional and citizen scientists, primarily through the USA-NPN plant and animal phenology observing program, Nature’s Notebook. These data are submitted to the USA-NPN and serve as the backbone of all USA-NPN observational data products. Observation protocols consist of status monitoring, in which observers visit a site at regular intervals to evaluate the phenological status of marked individual plants (or patches of plants) and animal species The protocols are described fully in Denny et al. (2014). In this system, phenological status is reported by yes or no answers to a series of questions, for example, “Do you see leaves?” or “Do you see active individuals?”. In addition to “Yes” or “No,” observers may also report “?”, indicating that they are uncertain of the phenophase status. Observers are also invited to document the degree to which the phenophase is expressed on an individual plant, or for animals, at a site. This intensity or abundance question takes the form of a count or percentage - for example, “95–100 percent” of a beech tree’s canopy is full with “Leaves,” or 12 “Active individual” robins are seen. USA-NPN observational data and derivative products are described in USA National Phenology Network Observational Data Documentation (Rosemartin et al. 2018). The three formats in which the USA-NPN observational data are made available include: status and intensity data, individual phenometrics, and site-level phenometrics. Visual comparison of data collected by monitoring phenological events, phenophase status, and phenophase status plus intensity. Event monitoring captures onset of a given phenophase, whereas status monitoring captures onset and duration. Status monitoring with intensity (or abundance) captures onset, duration, and magnitude of a phenophase. Examples are derived from 2012 data submitted in Nature’s Notebook for (a) sugar maple (Acer saccharum) leafing for one individual plant in Maine, and (b) forsythia flowering (Forsythia sp.) for one individual plant in Massachusetts. Each point represents one observation; black points indicate presence of the phenophase while white points indicate absence. (a) illustrates the date on which the first leaf appears (event), the period during which leaves are present (status), and the period and rate at which the canopy fills from 0 to 100 % capacity and then, empties back to 0 with leaf fall (status + intensity, circles and solid line) using estimates of canopy fullness. Also illustrated is the period and rate at which the canopy fills and empties of autumn colored leaves (status + intensity, triangles and dashed line). (b) illustrates the date on which the first open flower appears (event), the periods during which open flowers are present on the plant (status), and an estimate of the number of open flowers on the plant over the periods in which they are present (status + intensity). In both examples, the event point is calculated as the first date of the year where the phenophase was reported as present. Note that in (b) there are two distinct periods of flowering, the second of which would not have been captured using event monitoring alone. (Denny et al., 2014) 3 Status &amp; Intensity Data Status and intensity data consist of presence/absence records for individual phenophases on individual plants or species of animals at a site on a single visit. These records also include intensity and abundance measures. Individual Phenometrics and Site Phenometrics, which are synthesized sequentially from Status and Intensity data, provide estimated phenophase onset and end dates. Individual Phenometrics are derived estimates of phenophase onset and end dates for organisms within a given period of interest. Site Phenometrics are summary metrics of the onset and end date of phenophase activity across multiple individuals of the same species at a site within a given period of interest. Magnitude Phenometrics provide measures of the extent to which a phenophase is expressed across multiple individuals or sites, for a given time interval. These metrics include several approaches for capturing the shape of seasonal activity curves. In Nature’s Notebook, plants are marked and tracked through time, while animals are not, resulting in several key differences between the phenometric data types for plants and for animals. Individual Phenometrics and Site Phenometrics are nearly identical for animals, while for plants the former provide data for individual plants and the latter aggregate data across plants of the same species at a site. Magnitude Phenometrics provide additional information on animals, including correcting abundance values by search time and search area, which is not relevant for plants. As additional observational phenology data types are created by the USA-NPN, they are described at www.usanpn.org/data/new_data_products. USA-NPN Animal Phenological Data by Type from Rosemartin et al.,2018 3 Gridded Raster Data The USA-NPN offers a growing suite of gridded (raster) maps of phenological events, patterns, and trends. These products include historical, real-time, and short-term forecasts and anomalies in the timing of events such as the start of the spring season, and growing degree days. These products are described in the USA National Phenology Network gridded products documentation (Crimmins et al. 2017) Accumulated Growing Degree Days anomaly in 2018 3 Pheno-Forecasts USA-NPN Pheno-Forecasts include real-time maps and short-term forecasts of insect pest activity at management-relevant spatial and temporal resolutions and are based on accumulated temperature thresholds associated with critical life-cycle stages of econmically important pests. Pheno Forecasts indicate, for a specified day, the status of the insect’s target life-cycle stage in real time across the contiguous United States. The maps are available for 12 insect pest species including the invasive emerald ash borer, hemlock woolly adelgid, and gypsy moth. These products are described in “Short-term forecasts of insect phenology inform pest management” (Crimmins et al. 2020) &gt; Example of USA-NPN’s Hemlock Wolly Adelgid Pheno-Forecast for August, 2020. Pheno-Forecasts are also available for an invasive grasses, such as buffelgrass. The buffelgrass Pheno-Forecast is based on known precipitation thresholds for triggering green-up to a level where management actions are most effective. These maps are updated daily and predict green-up one to two weeks in the future. Land Surface Phenology products The USA-NPN offers maps derived from MODIS 6 land surface phenology data. Satellite observations can be linked to in-situ observations to help understand vegetation dynamics across large spatial scales. The MODIS Land Cover Dynamics Product (MLCD) provides global land surface phenology (LSP) data from 2001-present. MLCD serves a wide variety of applications and is currently the only source of operationally produced global LSP data. MLCD data have enabled important discoveries about the role of climate in driving seasonal vegetation changes, helped to create improved maps of land cover, and support ecosystem modeling efforts, among many other important applications. The LSP Climate Indicators (LSP-CI) dataset is a curated collection of the most relevant phenological indicators: a measure of spring and autumn timing and a measure of seasonal productivity. Statistically robust estimates of long-term normals (median and median absolute deviation, MAD), significance-screened trends (Theil-Sen slope magnitude where p&lt;=0.05), and interannual anomalies (in days as well as multiples of MAD) have been computed for these three phenological indicators. The data have been mosaiced across CONUS, reprojected and resampled to a more familiar spatial reference system that matches complementary datasets and delivered in the universally accessible GeoTIFF format. 3 How to Access USA-NPN Data: The USA-NPN makes the data they produce available through a number of different channels and tools. This is partly driven by the format of the data; GIS data, in many ways, can and should be managed differently than observational records, which can more easily be managed in a relational database. However, the need for these different venues is also driven by end-user need. The different tiers of tools makes the data accessible to anyone regardless of their level of technical experience. This is true from the casual observer that would like to use the visualization tool to see how their contributions to citizen science relate to the broader world, all the way to the data scientist that needs simple and standard APIs to integrate USA-NPN data into larger applications and analyses. 3 The USA-NPN Landing page A concise list of all available NPN data sets, tools, products. 3 APIs This is a set of standard web service calls that allows for programmatic access to NPN data independent of any particular programming language. *USA-NPN Web Service API Documentation *USA-NPN Geoserver Documentation *USA-NPN GeoServer API 3 Rnpn package This suite of R functions allows for programmatic access to both gridded and in-situ NPN data sets in an R environment. Full documentation available here: https://usa-npn.github.io/rnpn/ 3 Phenology Observation Portal (for observational data) This tool allows users to download customized datasets of observational data from the National Phenology Database, which includes phenology data collected via the Nature’s Notebook phenology program (2009-present for the United States), and additional integrated datasets, such as historical lilac and honeysuckle data (1955-present). Filters are available to specify dates, regions, species and phenophases of interest. This provides access to all phenometrics, which represents varying degrees of data aggregation. 3 Geospatial Request Builder (for raster data and image files) This tool simplifies the process of accessing NPN gridded data through standard WMS and WCS services. WMS services provide the data as basic graphic images, such as PNGs or TIFFs, whereas WCS services provide the same data in formats accessible to GIS applications. 3 Visualization Tool The Visualization Tool provides an easier way to explore phenology data and maps. The user-friendly interface is intended to allow for searching for comparing general trends and quick-and-easy access to map data/products. 3 USA-NPN Written Questions Suggested timing: Complete before lecture 2 of USA-NPN Hands on Coding Exercises Question 1: How might or does USA-NPN intersect with your current research or future career goals? (1 paragraph) Question 2: Use the USA-NPN visualization tool (www.usanpn.org/data/visualizations) to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: · Are there species, regions, or phenophases of interest to you? · Is there geospatial phenology data that is useful for your work (e.g. Spring Indices or Growing Degree Days)? · What is the timeframe of data you will need to address your research interests? · What is the spatial extent of data you will need? Question 3: Consider either your current or future research, or a question you’d like to address during this course: · What climate data or additional phenological datasets would be valuable to address your research interests? · What challenges, if any, could you foresee when beginning to work with these data? 3.1 Hands on: Accessing USA-NPN Data via rNPN 3.1.1 Introduction The USA National Phenology Network (USA-NPN) is a USGS funded organization that collects phenological observation records from volunteer and professional scientists to better understand the impact of changes in the environment on the timing of species’ life cycles. The USA-NPN also provides a number of raster-based climatological data sets and phenological models. These in-situ observation and geospatial, modeled datasets are available through a number of tools and data services. The USA-NPN R library, “rnpn”, is primarily a data access service for USA-NPN data products, serving as a wrapper to the USA-NPN REST based web services. This guide details how to use the library to access and work with all USA-NPN data types. 3.1.2 Accessing USA-NPN Observational Data USA-NPN Observational data are collected on the ground by citizen and professional observers following standardized protocols, using the Nature’s Notebook platform. The data are available 2009 to present, and come in four formats or data types: Status &amp; Intensity, Individual Phenometrics, Site Phenometrics and Magnitude Phenometrics. An overview of the differences is provided in the figure below, and each type is detailed in the following sections. For a complete description of the USA-NPN approach and notes for working with each data type see the Open File Report on USA-NPN Observational Data. In Nature’s Notebook, observers register a location, and then at each location they register any number of individual plants or animal species. The expectation is that the user then takes regular observations on each individual/species at a regular interval. Phenological status is reported by yes or no answers to a series of questions, for example, “Do you see leaves?” or “Do you see active individuals?”. In contrast to traditional monitoring of annual “first” events (for example, date of first leaf or first robin), this approach captures absence data when the phenophase is not occurring and repeat events. Each observation is comprised of a series of 1, 0 and -1 values, representing yes/no/uncertain for each possible phenophase for the plant on that date. To explore data in this native “Status and Intensity” format, see the vignette by the same name. A few considerations and functions apply across all USA-NPN Observational data types. 3.1.2.1 Basic format for for Observational data calls The basic format for an observational data call in the rnpn library is: npn_download_[NAME OF DATA TYPE] ( request_source = [NULL] year = [NULL] species_ID = [NULL] ) ‘Request source’ should usually be populated with your full name or the name of the organization you represent. Species_ID is the unique identifier for all the available plants and animals in the USA-NPN database. You can create a table of all available species and their ID numbers: species &lt;- npn_species() Search for a species by common name from the full list: species[species$common_name==&quot;red maple&quot;,] There are many parameters which can be set beyond these basic ones, depending on the data type, and further detailed in the other vignettes featured in this package. 3.1.2.2 Required Parameters Note that specifying the year(s) of interest is a required parameter. There’s also another required field, “request_source”, which is a user-provided, self-identifying string. This allows the client to provide some information about who is accessing the data. Knowing who is using the data is very helpful for our staff to report the impact of the USA-NPN to the scientific community. The input provided here is entirely honor-based. 3.1.2.3 Find stations at which a species has been observed You can also now look up which stations have a registered plant for a particular species. In the example below, we use the species ID for red maple, which we were able to find through the npn_species() function, to find all stations with that species. npn_stations_with_spp (3) 3.1.3 Status and Intensity Data The Status and Intensity data type is the most direct presentation of the phenology data stored in the NPDb. Each row is comprised of a single record of the status (1/present/“Yes”, 0/absent/“No” or -1/uncertain/“?”) of a single phenophase on an individual plant or species of animal at a site on a single site visit, as well as the estimated intensity or abundance e.g., percent canopy fullness or number of individual robins observed respectively. Retrieving this kind of data using this package is easy, and heavily parameterized. It’s possible to filter data using a number of including year, geographic extent and species. In this example we get all records of bird observations in the New England states from 2018. npn_download_status_data( request_source = &#39;Your Name Here&#39;, years = c(&#39;2018&#39;), states = c(&quot;NY&quot;,&quot;PA&quot;,&quot;VT&quot;,&quot;MA&quot;), functional_types = &#39;Bird&#39; ) ‘states’ is an example of an optional parameter that allows you to filter data based on geographic location. Another example is ‘functional_types’ which allows you to get all available data for a group of similar species (e.g., all birds, shrubs or invasive species). The best place to review all available optional filters is the autogenerated package description. Another important optional parameter is called ‘download_path’. By default requests for data from the services are returned as a data frame that gets stored in memory as a variable. In some cases, it makes more sense to save the data to file for easy and fast retrieval later. The download_path parameter allows you to specify a file path to redirect the output from the service, without having to fuss with pesky I/O operations. Additionally, requests made this way streams the data returned, so if the dataset you’re working with is particularly large, it’s possible to redirect the stream of data to file instead of loading it all into memory which can be useful if your environment doesn’t have enough RAM to store the entire data set at once. npn_download_status_data( request_source = &#39;Your Name Here&#39;, years = c(&#39;2018&#39;), functional_types = &#39;Bird&#39;, additional_fields = &#39;Site_Name&#39;, download_path =&#39;Bird_data_2018_SiteName.csv&#39; ) Using this function to get observational records is the most basic presentation of the data, and is the most robust for doing analysis, but there are a number of other products offered through the data service which provide additional value to data end users, outlined in the next vignettes. 3.1.4 Individual Phenometrics While Status and Intensity data provide a direct and complete look at the observational data, some analyses rely on more synthesized output. Individual Phenometrics are derived from phenophase status data and provide estimates of phenophase onset and end dates based on the first and last “Yes” status values for organisms within a specified season of interest. Each row in this data type is comprised of values that are derived from a string of consecutive “Yes” status reports without an intervening “No” status report for a single phenophase for an individual plant or animal species at a site, called a “series”. For plants, this data type provides information on the onset and end of a phenophase on an individual plant. For animals, it provides information on the onset and end of the presence of an animal species at a site. As animal presence at a site is much more likely to be interrupted by absence than the presence of a phenophase on a plant, Status and Intensity data or Site Phenometrics may be more appropriate for investigating animal phenology. However, we provide animal phenology in the same format as individual plants in this data type to allow users to readily compare individual plant phenology with animal activity. Note that more than one series may exist for a given phenophase in an individual plant or animal species within a single growing season or year, this might occur in the case of leaf bud break followed by a killing frost and second round of breaking leaf buds. It could also occur at group sites where two or more observers are reporting on the same plant on sequential days but are not in agreement on phenophase status. Any call for individual phenometrics requires chronological bounds, usually a calendar year, as determining onset and end depend on knowing what the time frame of interest is. If you query the services directly (without the benefit of this library) it’s possible to specify arbitrary dates, in contrast this library allows you to specify a series of calendar years as input. Here’s an example of how to query the services for individual phenometrics data. Note that the overall structure and parameters are very similar to the call for status data. The biggest difference in this case is that start and end date parameters are now replaced with a ‘years’ array, which predictably takes a set of year values with which to query the service. npn_download_individual_phenometrics( request_source=&#39;Your Name Here&#39;, years=c(2013,2014,2015,2016), species_id=c(210), download_path=&quot;saguaro_data_2013_2016.csv&quot; ) In this example, we’re able to see individual saguaro phenology for 2013 through 2016. The results returned from the service is a tabular set of records, giving start and end date by individual saguaro plant. By default, each record contains information about the location, species, phenophase, and start and end dates. Climate data from DayMet can also be acquired with Status &amp; Intensity, Individual Phenometrics and Site Phenometric data types, by setting the climate_data parameter to true. In this example, we are getting colored leaves (phenophase ID is 498) data for birches, using the four birch species IDs, for 2015: npn_download_individual_phenometrics( request_source = &#39;Your Name Here&#39;, years = c(&#39;2015&#39;), species_ids = c(97, 98, 99, 430), phenophase_ids = c(498), climate_data = TRUE, download_path = &#39;Betula_data_2015.csv&#39; ) To show what this looks like, we can plot the day of year of the first observation of colored leaves in birches (genus Betula) against summer Tmax. BetulaLeaf &lt;-read.csv( &#39;Betula_data_2015.csv&#39;, header = TRUE, na=-9999, stringsAsFactors = FALSE ) plot( first_yes_doy~tmax_summer, data=BetulaLeaf, ylab=c(&quot;Day of Year&quot;), xlab=c(&quot;Tmax Summer&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21 ) 3.1.5 Site Phenometrics Site Phenometrics, derived from Individual Phenometrics, provide summary metrics of the onset and end date of phenophase activity for a species at a site. Observers are directed to create sites that represent uniform habitat and are no larger than 15 acres. For plants, this metric is calculated as an average for all individuals of a species at the site. For animals, where individuals are not tracked, this metric represents the first and last recorded appearance of the species during the season of interest. For instance, if you asked for red maple leafing data, and there was a site with three red maple trees being observed, then the data would be the average onset date for all three of those red maple trees at that site. Here’s an example of how to query the services for site phenometrics data, for cloned lilacs, breaking leaf buds, 2013. The call is very similar to the call for individual phenometrics data, however, in addition you can supply the quality control filter for the number of days between a yes record and preceding no record (also applies to the last yes and following no), for the observation to be included in the calculations. Typically this is set to 7, 14 or 30, as when downloading data using the USA-NPN Phenology Observation Portal. If you do not set this parameter, it defaults to 30 days. Note that in this example the results are stored in memory, rather than output as a file. LilacLeafPoints2013&lt;-npn_download_site_phenometrics( request_source = &#39;Your Name Here&#39;, years = c(&#39;2013&#39;), num_days_quality_filter = &#39;30&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39; ) In this example we’re able to see the date of the first observation of breaking leaf buds for cloned lilacs, averaged across individuals within sites. If any observation did not have a preceding no record within 30 days it was excluded from the calculations. We can now plot our cloned lilac site phenometric onset data by latitude. plot( mean_first_yes_doy~latitude, data=LilacLeafPoints2013, ylab=c(&quot;Day of Year&quot;), xlab=c(&quot;Latitude&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21, xlim=c(30,55), ylim=c(0,200) ) 3.1.6 Magnitude Phenometrics Magnitude Phenometrics are a suite of eight metrics derived from Status and Intensity data. This data type provides information on the extent to which a phenophase is expressed across multiple individuals or sites, for a given set of sequential time intervals. The data user may select a weekly, bi-weekly, monthly, or custom time interval to summarize the metrics. Two metrics are available for both plants and animals, one metric is available for plants alone and five metrics are available for animals alone (table 1). Three of the five animal metrics correct animal abundance values for observer effort in time and space. Here’s an example of how to query for Magnitude Phenometrics, for the active individuals phenophase for black-capped chickadee data, in 2018. Requirements are similar to other data types. You must additionally specify the time interval by which the data should be summarized. Typically this is weekly, biweekly or monthly, as in the POP and Visualization Tool. The interval chosen in this example is 7 days. npn_download_magnitude_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2018&#39;, period_frequency = &quot;7&quot;, species_ids = &#39;245&#39;, phenophase_ids = &#39;292&#39;, download_path = &#39;MPM_BCC_ActInd_2018.csv&#39; ) In this example we’re able to see all of the magnitude phenometric fields, including proportion_yes_records, and mean_num_animals_in-phase. See the https://pubs.usgs.gov/of/2018/1060/ofr20181060.pdf for full field descriptions. From this dataset we can view the Proportion_Yes_Records (of all the records submitted on this species, what proportion are positive/yes records) by weekly interval: BCC_AI&lt;-read.csv( &#39;MPM_BCC_ActInd_2018.csv&#39;, header = TRUE, na=-9999, stringsAsFactors = FALSE ) plot( BCC_AI$proportion_yes_record~as.Date(BCC_AI$start_date,&quot;%Y-%m-%d&quot;), ylab=c(&quot;Proportion Yes Records&quot;), xlab=c(&quot;Date&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21, xlim=as.Date(c(&quot;2018-01-01&quot;, &quot;2018-08-01&quot;)), ylim=c(0,1) ) 3.1.7 USA-NPN Geospatial Data USA-NPN provides phenology-relevant climate data in raster format. There are two main suites of products in this category: Accumulated Growing Degree Days and Extended Spring Indices. Accumulated Growing Degree Days and the Extended Spring Indices are both representations of accumulated temperature. As accumulated winter and spring heat drives many spring season phenological events in much of the country, these products can be used to better understand patterns in the current and historical timing of these events across the landscape. For a complete description of the USA-NPN approach and notes for working with each data type see the Open File Report on USA-NPN Gridded Data. Both suites are available as: Current year value, with a 6-day forecast Current year anomaly, with a 6-day forecast Long-term (30 year) average Historical years AGDD - 2016-Prior Year Extended Spring Index - 1880-Prior Year All of these products can be downloaded using the npn_download_geospatial call. There is a number of other products and permutations of the above listed AGDD and Spring Index products, so you can get a complete list of available layers and additional details about them including resolution, extent and the abstract/layer description. layers &lt;- npn_get_layer_details() The following sections describe how to parameterize calls for both AGDD and Spring Index layers. These calls result in raster data sets for the contiguous United States. If you are interested in how many GDDs had accumulated when the red maple in your backyard leafed out, or what day the Spring Index requirements for leaf out were met for your location, you may wish to query the layers for these values, based on location and date. There are two ways to accomplish this, using the npn_get_point_data function which works for all layers and the npn_get_AGDD_point_data function, which only works for AGDD layers and provides a more precise result. npn_get_agdd_point_data( &#39;gdd:agdd_50f&#39;, &#39;38&#39;, &#39;-90&#39;, &#39;2019-02-25&#39; ) This returns a value of 7.64098 GDD, base 50F, for the coordinates 38 north, -90 west on February 25th, 2019. npn_get_point_data( &#39;si-x:lilac_bloom_ncep&#39;, &#39;30&#39;, &#39;-90&#39;, &#39;2019-02-25&#39; ) This returns a value for lilac bloom of day 48, for the coordinates 30 north, -90 west, as of February 25th, 2019. The above mentioned AGDD products use base temperatures of 32F or 50F and are managed through WCS services. There is also a function to get dynamic AGDD calculations based on a user defined base temperature and a number of other parameters. custom_agdd_raster &lt;- npn_get_custom_agdd_raster( method = &#39;double-sine&#39;, climate_data_source = &#39;NCEP&#39;, temp_unit = &#39;fahrenheit&#39;, start_date = &#39;2019-01-01&#39;, end_date = &#39;2019-05-10&#39;, base_temp = 20, upper_threshold = 90 ) 3.2 Accumulated Growing Degree Day Products Heat accumulation is commonly used as a way of predicting the timing of phenological transitions in plants and animals, including when plants exhibit leaf out, flowering, or fruit ripening, or when insects emerge from dormancy. This is typically expressed as accumulated heat units, either Growing Degree Hours or Growing Degree Days. Growing degree day thresholds have been established for many species, and are commonly used in agriculture, horticulture, and pest management to schedule activities such as harvesting, pesticide treatment, and flower collection. The USA-NPN is currently generating Accumulated Growing Degree Days (AGDD) rasters using a January 1 start date, calculated using simple averaging. These are available calculated using two base temperatures, 32 degrees Fahrenheit (F) and 50 F. When querying certain layers, the underlying data is agnostic about the specific year, and in these cases it makes sense to use the day of year to request data, since that will provide a standardized result, (i.e., April 1st is day 91 in some years and day 92 in others). npn_download_geospatial( &#39;gdd:30yr_avg_agdd_50f&#39;, 95 ) But if you’re looking at a specific year, such as a current year layer, it makes sense to use a specific calendar date (formatted YYYY-MM-DD). It’s also possible to save the raster directly to file instead of loading it into memory. npn_download_geospatial( &#39;gdd:agdd&#39;, &#39;2018-05-05&#39;, output_path=&#39;20180505-agdd-value.tiff&#39; ) In the case of the historic Spring Index layers, however, the product represents the overall outcome for the entire year, so while the year component of the date matters, the month and day do not. In this case, specify January 1 as the month and date. npn_download_geospatial( &quot;si-x:average_bloom_prism&quot;, &quot;1995-01-01&quot; ) The dimension.range value, returned in the npn_get_layer_details() function, clarifies the full set of applicable dates for each layer. Of course, it’s also easy to grab raster data and load it into a visual plot as in this example, showing a map of AGDD base 50 on 2019-06-25: AGDDJun2019&lt;-npn_download_geospatial( &#39;gdd:agdd_50f&#39;, &#39;2019-06-25&#39; ) plot( AGDDJun2019, main = &quot;AGDD base 50 on June 25th, 2019&quot; ) An important layer to know of is the 30 year average for AGDD products. This is useful for many comparative analyses. This layer takes DOY as the date input, since it’s the average AGDD value for each day of year for 1981 - 2010. average_30yr &lt;- npn_download_geospatial( &quot;gdd:30yr_avg_agdd&quot;, 45 ) 3.3 Extended Spring Indices The Extended Spring Indices are mathematical models that predict the “start of spring” (timing of first leaf or first bloom) at a particular location. These models were constructed using historical observations of the timing of first leaf and first bloom in a cloned lilac cultivar (Syringa X chinensis ‘Red Rothomagensis’) and two cloned honeysuckle cultivars (Lonicera tatarica L. ‘Arnold Red’ and Lonicera korolkowii Stapf, also known as ‘Zabelii’), which were selected based on the availability of historical observations from across a wide geographic area. Primary inputs to the model are temperature and weather events, beginning January 1 of each year. The model outputs are first leaf and first bloom date for a given location. Data for the Spring Index is available through an enumeration of layers that represents each of the three sub-models as well as an ‘average’ model which represents the aggregation of the three sub-models. These layers are further enumerated by both of the represented phenophases, leaf and bloom. In the example below, first the layer representing only the Arnold Red model for 1987 is retrieved, while the second function call gets the model averaging all three of the models for the same year. npn_download_geospatial( &quot;si-x:arnoldred_bloom_prism&quot;, &quot;1987-01-01&quot; ) average_model &lt;- npn_download_geospatial( &quot;si-x:average_bloom_prism&quot;, &quot;1987-01-01&quot; ) The Spring Indices are also unique in that the algorithm has been run against the BEST climate data set, so historic data going back to 1880 is available. BESTSIxData1905 &lt;- npn_download_geospatial( &#39;si-x:average_bloom_best&#39;, &#39;1905-01-01&#39; ) NAvalue(BESTSIxData1905) &lt;- -9999 plot( BESTSIxData1905, main = &quot;Spring Index, 1905&quot; ) 3.3.1 Other Layers Besides the AGDD and Spring Index layers there are a number of other useful layers available through these services, including daily temperature minimum and maximums and aggregated MODISv6 phenometrics. The daily temperature minimum and maximum values are the underlying climate data used to generate current year AGDD and Spring Index maps. These data are generated by NOAA’s National Centers for Environmental Prediction (NCEP) and are reserved through NPN’s geospatial services. daily_max_20190505 &lt;- npn_download_geospatial( &#39;climate:tmax&#39;, &#39;2019-05-05&#39; ) plot( daily_max_20190505, main = &quot;Daily Temperature Max (C), May 5th, 2019&quot; ) The MODISv6 layers are aggregate values for remote sensing values from the MODISv6 data set, representing a subset of the following phenometrics, aggregated across 2001 - 2017: EVI Area, Mid-Greenup, Mid-Greendown. The available aggregate values for each layer are: median, TSslope, and mean absolute deviation. This example shows the median green up value, as DOY. Note that because this layer has a fixed date, the date parameter is input as a blank string. median_greenup &lt;- npn_download_geospatial( &#39;inca:midgup_median_nad83_02deg&#39;, &#39;&#39; ) plot( median_greenup, main = &quot;MODIS Median Mid-Greenup, 2001 - 2017&quot; ) 3.4 Putting it all together: 3.5 Combine Point and Raster Data Observational and gridded data can be visualized or analyzed together for a variety of purposes. Users may want to identify spatial patterns in the alignment of dogwood bloom and the Spring Index bloom model. The current year’s lilac leaf out observations may be compared to the 30 year average lilac sub-model of the spring index to see how well the model predicts the observations. This example shows several data access calls to assemble observational and gridded data. Option 1: You can add a parameter to an observational data call to additionally get a gridded layer value for each observation location/date. Note that if you don’t specify which sub model of the Spring Index you want, you will get the SI-x Average layers. npn_download_site_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2013&#39;, num_days_quality_filter = &#39;30&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39;, download_path = &#39;cl_lilac_data_2013_SIxLeaf.csv&#39;, six_leaf_layer = TRUE, six_sub_model = &#39;lilac&#39; ) If you want to append raster data other than Spring Index, Leaf values, there’s alternative boolean flags that can be set, including six_bloom_layer for Spring Index, Bloom data, and agdd_layer. Instead of TRUE or FALSE agdd_layer takes 32 or 50 and will correlate each data point with the corresponding AGDD value for the given date using either 32 or 50 base temperature. Option 2: You can create a combined plot of observational data with modeled/raster data. Building on the approach for accessing point data from earlier vignettes describing Individual Phenometrics and getting raster data, we can access and plot these products together. In this example, we will look at how well cloned lilac leaf out observations in 2018 are predicted by the lilac leaf sub model of the Spring Index. 3.5.1 Step 1: Get the data LilacLeaf2018&lt;-npn_download_geospatial( &#39;si-x:lilac_leaf_ncep&#39;, &#39;2018-12-31&#39;, ) LilacLeaf2018Obs &lt;-npn_download_individual_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2018&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39; ) 3.5.2 Step 2: Preparing the data coords &lt;- LilacLeaf2018Obs[ , c(&quot;longitude&quot;, &quot;latitude&quot;)] data &lt;- as.data.frame(LilacLeaf2018Obs$first_yes_doy) crs &lt;- CRS(&quot;+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) LL_spdf &lt;- SpatialPointsDataFrame( coords = coords, data = data, proj4string = crs ) 3.5.3 Step 3: Define style options and create graph my.palette &lt;- brewer.pal(n=9,name=&quot;OrRd&quot;) plot( LilacLeaf2018, col = my.palette, main=&quot;2018 Observed and Predicted Lilac Leaf Out&quot; ) plot( LL_spdf, main=&quot;Lilac Obs&quot;, pch = 21, bg = my.palette, col = &#39;black&#39;, xlim=c(-125.0208,-66.47917), ylim=c(24.0625 ,49.9375), add = TRUE ) legend( &quot;bottomright&quot;, legend=c(&quot;Cloned Lilac Leaf Out Observations&quot;), pch = 21, bg = &#39;white&#39;, col = &#39;black&#39;, bty=&quot;n&quot;, cex=.8 ) 3.6 Live Demo Code with Lee Marsh of USA-NPN 3.6.1 Basic Utility Functions species &lt;- npn_species() phenophases &lt;- npn_phenophases() layer_details &lt;- npn_get_layer_details() quick_get_phenophase &lt;- function(species_id, date,phenophase_name){ phenophases&lt;-npn_phenophases_by_species(c(species_id),date=date) phenophases_species &lt;- phenophases[phenophases$species_id==species_id]$phenophases[[1]] phenophases_species[phenophases_species$phenophase_name==phenophase_name,]$phenophase_id } quick_get_species &lt;- function(species_name){ species[species$common_name==species_name,]$species_id } 3.6.2 Download Observational Data white_oak_id &lt;- quick_get_species(&quot;white oak&quot;) fruits_id&lt;- quick_get_phenophase(white_oak_id,&quot;2017-05-15&quot;,&quot;Fruits&quot;) # Raw data download s2017_white_oak_raw &lt;- npn_download_status_data( request_source = &quot;R Demo&quot;, years = c(2017), species_ids = c(white_oak_id), phenophase_ids = c(fruits_id) ) 3.6.3 Magnitude Data m2017_white_oak_magnitude &lt;- npn_download_magnitude_phenometrics( request_source = &quot;R Demo&quot;, years = c(2017), species_ids = c(white_oak_id), phenophase_ids = c(fruits_id), period_frequency = &quot;14&quot; ) datasets &lt;- npn_datasets() # NEON data, file download, additional fields npn_download_status_data( request_source = &quot;R Demo&quot;, years = c(2018:2020), states = c(&quot;CO&quot;), dataset_ids = c(16), additional_fields = c(&quot;Site_Name&quot;), download_path = &quot;NEON_CO_Data_2018-2010.csv&quot; ) 3.6.4 Downloading Geospatial Data SIXBloom2018 &lt;- npn_download_geospatial( &#39;si-x:average_bloom_ncep&#39;, &#39;2018-12-31&#39; ) npn_download_geospatial( &#39;gdd:agdd&#39;, &#39;2018-04-15&#39;, output_path = &quot;20180415-32-agdd.tiff&quot; ) my_point &lt;- npn_get_point_data(&quot;gdd:agdd_50f&quot;, 33.649, -111.861, &quot;2017-05-15&quot;) 3.6.5 Putting it together dogwood_id &lt;- quick_get_species(&quot;flowering dogwood&quot;) dogwood_flowering_id &lt;- quick_get_phenophase(dogwood_id,&quot;2018-05-05&quot;,&quot;Flowers or flower buds&quot;) dogwood_data &lt;- npn_download_site_phenometrics( request_source = &#39;Demo&#39;, years = &#39;2018&#39;, species_ids = dogwood_id, phenophase_ids = dogwood_flowering_id, six_leaf_layer = TRUE, agdd=32 ) 3.6.6 Other Data Sources, e.g. Daymet, MODIS add_fields &lt;- npn_download_status_data( request_source = &quot;Demo&quot;, years = c(2014), species_id = c(4), additional_fields = c(&quot;tmaxf&quot;,&quot;Greenup_0&quot;,&quot;MidGreenup_0&quot;) ) 3.7 USA-NPN Coding Lab library(rnpn) library(ggplot2) library(neonUtilities) library(dplyr) source(&#39;/Users/kdw223/Research/katharynduffy.github.io/neon_token_source.R&#39;) For the purposes of this exercise we will be focusing on two NEON sites: HARV and CPER. Save these two sites into your workplace so that you can feed them into functions and packages. Define AGGD and write the equation using LaTeX. What is an appropriate time interval over which we should calculate AGGD? This will be relevant for following questions Use the neonUtilities package to pull plant phenology observations (DP1.10055.001). We will work with the statusintensity data: Hints: #TOS Phenology Data sitesOfInterest &lt;- c(&quot;HARV&quot;) dpid &lt;- as.character(&#39;DP1.10055.001&#39;) #phe data pheDat &lt;- loadByProduct(dpID=&quot;DP1.10055.001&quot;, site = sitesOfInterest, package = &quot;basic&quot;, check.size = FALSE, token=NEON_TOKEN) #NEON sends the data as a nested list, so I need to undo that # unlist all data frames list2env(pheDat ,.GlobalEnv) summary(phe_perindividualperyear) summary(phe_statusintensity) #remove duplicate records phe_statusintensity &lt;- select(phe_statusintensity, -uid) phe_statusintensity &lt;- distinct(phe_statusintensity) #Format dates (native format is &#39;factor&#39; silly R) phe_statusintensity$date &lt;- as.Date(phe_statusintensity$date, &quot;%Y-%m-%d&quot;) phe_statusintensity$editedDate &lt;- as.Date(phe_statusintensity$editedDate, &quot;%Y-%m-%d&quot;) phe_statusintensity$year &lt;- substr(phe_statusintensity$date, 1, 4) phe_statusintensity$monthDay &lt;- format(phe_statusintensity$date, format=&quot;%m-%d&quot;) In your phe_statusintensity data.frame pick a phenophase name of interest: unique(phe_statusintensity$phenophaseName) And select a single taxon: unique(phe_perindividual$taxonID) Now create a new, filtered dataframe only including those observations and print a summary. You’ll also want to filter for typical things like NA values, and think about how you’ll work with data that comes in factors or strings. Are there ways you could extract numerical values for plotting? Could you count data? Summarize your strategy. Using dpid DP1.00002.001 Single Aspirated Air Temperature calculate AGGD based on NEON tower data over the time period you decidided upon in question 1. To save you time and frustration I’ve placed some mostly complete example code for one height on the tower just for Harvard. You will need to determine which height you think it best and conmplete these calculations for both sites. You will also need to consder things like filtering your temperature data for quality flags, and converting from GMT (Greenwich Mean Time) to your location’s time: ##load libraries library(tidyverse) library(neonUtilities) install.packages(&#39;mgcv&#39;) library(mgcv) dpid &lt;- as.character(&#39;DP1.00002.001&#39;) ##single aspirated air temperature tempDat &lt;- loadByProduct(dpID=dpid, site = &quot;HARV&quot;, startdate = &quot;2017-01&quot;, enddate=&quot;2017-12&quot;, avg=30, package = &quot;basic&quot;, check.size = FALSE) df &lt;- tempDat$SAAT_30min # GDD typically reported in F # define function to convert temp c to f c_to_f &lt;- function(x) (x * 1.8 + 32) # convert df temps df$meanTempF &lt;- c_to_f(df$tempSingleMean) #pull date value from dateTime df$date &lt;- substr(df$endDateTime, 1, 10) # group data and summarize values # Here, we will group the 30-minute temperature averages by data (to get daily values) # You will want to consider which vertical position is most appropriate to use for your analysis. # You can view the sensor position data in the sensor_positions table downloaded above, # where HOR.VER are the horizontal and vertical position indices (separated by a period), # and zOffset is in meters above the ground select(tempDat$sensor_positions_00002, c(HOR.VER, zOffset)) # (you can also view all of the sensor position info with the following line:) # View(tempDat$sensor_positions_00002) # For example, the lowest position sensor (verticalPosition == 010) may be most appropriate for # comparison with the phenological state of very short plants, while the highest verticalPosition # may be better for comparison with canopy trees. Selected level 1 for demonstration day_temp &lt;- df%&gt;% filter(verticalPosition==&quot;010&quot;)%&gt;% group_by(siteID, date)%&gt;% mutate(dayMaxTemp=max(meanTempF), dayMinTemp=min(meanTempF), dayMeanTemp=mean(meanTempF))%&gt;% select(siteID, date, dayMaxTemp, dayMinTemp, dayMeanTemp)%&gt;% distinct() ##alternative, simplified mean, consistent with many GDD calculations ### does accumulation differ for true mean vs. simplified mean? day_temp$mean2 &lt;- (day_temp$dayMinTemp + day_temp$dayMaxTemp)/2 #caluculate daily GDD for true mean ## 50 degrees F is a common base temperature used to calculate plant specific GDD. When might you select a differnt base temp? day_temp$GDD &lt;- ifelse(day_temp$dayMeanTemp-50 &lt; 0, 0, round(day_temp$mean2-50, 0)) # define year day_temp$year &lt;- substr(day_temp$date, 1, 4) #function to add daily GDD values sumr.2 &lt;- function(x) { sapply(1:length(x), function(i) sum(x[1:i])) } #calculate Accumlated GDD day_temp$AGDD &lt;- sumr.2(x=day_temp$GDD) day_temp &lt;- ungroup(day_temp) Plot your calculated AGGD and comment on your calculations. Do you need to revise your time horizon or sensor height? Hint: ggplot(data=data, aes(x=date, y=AGDD, group=1)) + geom_point()+ geom_smooth() Your plot should resemble something like this (intentionally a little vague): Now we’re going to build a model to see how AGGD impacts phenological status. But Wait. Is phenology all driven by temperature? Should you consider any other variables? What about AGGD and just plain temperature? Also, we have one very temperate site, and another that is a semi-arid grassland. Should water availability of any sort be considered? Create a GAM (Generalized Additive Model) for your phenological data including any variables you think might be relevant. One of the bonuses of a GAM is that it will tell you which variables are relevant and which aren’t so you can iterate a bit on your model and revise it. You might want to test a few positions on your asipirated air temperature, or a few other additional variables. Your selection is up to you, but you must document and justify your decision. Hints: library(mgcv) model &lt;- mgcv::gam(phenological_status_you_picked ~ AGGD + s(temp or maybe precip) + s(doy), data=your_data) mgcv::summary.gam(model) and plot your models for each site: Hint: mgcv::plot.gam(model, pages=1 ) Now that we have a model for NEON data, let’s use the rnpn package to see how adding additional data could improve our fit. Use the taxonID that you selected at each NEON tower, and feed that to the rnpn package to grab observational data and increase your number of observations. Hints: Feeding a state or other region will make the data more congruent You’ll likely need to either request the phenophase that you selected from NEON, or filter again. It might make your life easier to request the NEON data from rnpn as they host it as well. Pull AGGD from USA-NPN based on the observations you just pulled. Combine your NEON and USA-NPN data into the same data.frame and re-fit your GAM. Summarize your new model Plot your new model Comment on your new model: was it improved? If so how? 3.8 NEON TOS Phenology Data Lecture Please watch the recorded lecture with Dr. Katie Jones, lead plant ecologist with NEON Battelle 3.9 Intro to USA-NPN Culmination Activity Note: I fully realize that phenology data may not be relevant to all of you. Two suggestions: 1. Be creative, example: Say you work with ground water hydrology, how could leaves on trees perhaps be relevant to groundwater recharge rate? Might there be a lag? Etc etc. 2. I will fully accept alternate citizen-science-based datasets and project proposals based on those. The challenge of taking this option is that we have not covered that data. Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using USA-NPN data. Include the types of USA-NPN (and other data) that you will need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. Sugestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragaph summarizing how this data or analysis is useful to you and/or the infrastructure. "],
["digital-repeat-photography-networks-methods.html", "Chapter 4 Digital Repeat Photography Networks &amp; Methods 4.1 Digital Repeat Photography Networks Learning Objectives 4.2 Introduction to Digital Repeat Photography Methods 4.3 Detecting Foggy Images using the ‘hazer’ R Package 4.4 Extracting Timeseries from Images using the xROI R Package 4.5 Documentation and Citation 4.6 Challenge: Use xROI 4.7 The PhenoCam Network Mission &amp; Design 4.8 Pulling Data via the phenocamapi Package 4.9 Exploring PhenoCam metadata 4.10 PhenoCam time series 4.11 Download midday images 4.12 Digital Repeat Photography Exercises", " Chapter 4 Digital Repeat Photography Networks &amp; Methods Estimated Time: 4 hours Course participants: As you review this information, please consider the final course project that you will work on at the over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 4.1 Digital Repeat Photography Networks Learning Objectives At the end of this activity, you will be able to: perform basic image processing. estimate image haziness as an indication of fog, cloud or other natural or artificial factors using the hazerR package. Define and use a Region of Interest, or ROI, for digitial repeat photography methods. Handle Field-of-View (FOV) shifts in digital repeat photography. Extract timeseries data from a stack of images using color-based metrics. 4.2 Introduction to Digital Repeat Photography Methods The concept of repeat photography for studying environmental has been introduced to scientists long time ago (See Stephens et al., 1987). But in the past decade the idea has gained much popularity for monitoring environmental change (e.g., Sonnentag et al., 2012). One of the main applications of digital repeat photography is studying vegetation phenology for a diverse range of ecosystems and biomes (Richardson et al., 2019). The methods has also shown great applicability in other fields such as: assessing the seasonality of gross primary production, salt marsh restoration, monitoring tidal wetlands, investigating growth in croplands, and evaluating phenological data products derived from satellite remote sensing. Obtaining quantitative data from digital repeat photography images is usually performed by defining appropriate region of interest, also know as ROI’s, and for the red (R), green (G) and blue (B) color channels, calculating pixel value (intensity) statistics across the pixels within each ROI. ROI boundaries are delineated by mask files which define which pixels are included and which are excluded from these calculations. The masks are then used to extract color-based time series from a stack of images. Following the time-series, statistical metrics are used to obtain 1-day and 3-day summary time series. From the summary product time series, phenological transition dates corresponding to the start and the end of green-up and green-down phenological phases are calculated. In this chapter we explain this process by starting from general image processing tools and then to phenocam-based software applications. For more details about digital repeat photogrpahy you can check out the following publications: - Seyednarollah, et al. 2019, “Tracking vegetation phenology across diverse biomes using Version 2.0 of the PhenoCam Dataset”. - Seyednarollah, et al. 2019, “Data extraction from digital repeat photography using xROI: An interactive framework to facilitate the process”. 1929 - 2016: Alpine Mountains near East Fork Toklat River, Alaska. Credit: Denali National Park (NPS) 4.3 Detecting Foggy Images using the ‘hazer’ R Package 4.3.0.1 Read &amp; Plot an Image We will use several packages in this tutorial. All are available from CRAN. # load packages library(hazer) library(jpeg) library(data.table) library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## between, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Before we start the image processing steps, let’s read in and plot an image. This image is an example image that comes with the hazer package. # read the path to the example image jpeg_file &lt;- system.file(package = &#39;hazer&#39;, &#39;pointreyes.jpg&#39;) # read the image as an array rgb_array &lt;- jpeg::readJPEG(jpeg_file) # plot the RGB array on the active device panel # first set the margin in this order:(bottom, left, top, right) par(mar=c(0,0,3,0)) plotRGBArray(rgb_array, bty = &#39;n&#39;, main = &#39;Point Reyes National Seashore&#39;) When we work with images, all data we work with is generally on the scale of each individual pixel in the image. Therefore, for large images we will be working with large matrices that hold the value for each pixel. Keep this in mind before opening some of the matrices we’ll be creating this tutorial as it can take a while for them to load. 4.3.0.2 Histogram of RGB channels A histogram of the colors can be useful to understanding what our image is made up of. Using the density() function from the base stats package, we can extract density distribution of each color channel. # color channels can be extracted from the matrix red_vector &lt;- rgb_array[,,1] green_vector &lt;- rgb_array[,,2] blue_vector &lt;- rgb_array[,,3] # plotting par(mar=c(5,4,4,2)) plot(density(red_vector), col = &#39;red&#39;, lwd = 2, main = &#39;Density function of the RGB channels&#39;, ylim = c(0,5)) lines(density(green_vector), col = &#39;green&#39;, lwd = 2) lines(density(blue_vector), col = &#39;blue&#39;, lwd = 2) In hazer we can also extract three basic elements of an RGB image : Brightness Darkness Contrast 4.3.0.3 Brightness The brightness matrix comes from the maximum value of the R, G, or B channel. We can extract and show the brightness matrix using the getBrightness() function. # extracting the brightness matrix brightness_mat &lt;- getBrightness(rgb_array) # unlike the RGB array which has 3 dimensions, the brightness matrix has only two # dimensions and can be shown as a grayscale image, # we can do this using the same plotRGBArray function par(mar=c(0,0,3,0)) plotRGBArray(brightness_mat, bty = &#39;n&#39;, main = &#39;Brightness matrix&#39;) Here the grayscale is used to show the value of each pixel’s maximum brightness of the R, G or B color channel. To extract a single brightness value for the image, depending on our needs we can perform some statistics or we can just use the mean of this matrix. # the main quantiles quantile(brightness_mat) ## 0% 25% 50% 75% 100% ## 0.09019608 0.43529412 0.62745098 0.80000000 0.91764706 # create histogram par(mar=c(5,4,4,2)) hist(brightness_mat) Question for the class: Why are we getting so many images up in the high range of the brightness? Where does this correlate to on the RGB image? 4.3.0.4 Darkness Darkness is determined by the minimum of the R, G or B color channel. In the Similarly, we can extract and show the darkness matrix using the getDarkness() function. # extracting the darkness matrix darkness_mat &lt;- getDarkness(rgb_array) # the darkness matrix has also two dimensions and can be shown as a grayscale image par(mar=c(0,0,3,0)) plotRGBArray(darkness_mat, bty = &#39;n&#39;, main = &#39;Darkness matrix&#39;) # main quantiles quantile(darkness_mat) ## 0% 25% 50% 75% 100% ## 0.03529412 0.23137255 0.36470588 0.47843137 0.83529412 # histogram par(mar=c(5,4,4,2)) hist(darkness_mat) 4.3.0.5 Contrast The contrast of an image is the difference between the darkness and brightness of the image. The contrast matrix is calculated by difference between the darkness and brightness matrices. The contrast of the image can quickly be extracted using the getContrast() function. # extracting the contrast matrix contrast_mat &lt;- getContrast(rgb_array) # the contrast matrix has also 2D and can be shown as a grayscale image par(mar=c(0,0,3,0)) plotRGBArray(contrast_mat, bty = &#39;n&#39;, main = &#39;Contrast matrix&#39;) # main quantiles quantile(contrast_mat) ## 0% 25% 50% 75% 100% ## 0.0000000 0.1450980 0.2470588 0.3333333 0.4509804 # histogram par(mar=c(5,4,4,2)) hist(contrast_mat) 4.3.0.6 Image fogginess &amp; haziness Haziness of an image can be estimated using the getHazeFactor() function. This function is based on the method described in Mao et al. (2014). The technique was originally developed to for “detecting foggy images and estimating the haze degree factor” for a wide range of outdoor conditions. The function returns a vector of two numeric values: haze as the haze degree and A0 as the global atmospheric light, as it is explained in the original paper. The PhenoCam standards classify any image with the haze degree greater than 0.4 as a significantly foggy image. # extracting the haze matrix haze_degree &lt;- getHazeFactor(rgb_array) print(haze_degree) ## $haze ## [1] 0.2251633 ## ## $A0 ## [1] 0.7105258 Here we have the haze values for our image. Note that the values might be slightly different due to rounding errors on different platforms. 4.3.0.7 Process sets of images We can use for loops or the lapply functions to extract the haze values for a stack of images. You can download the related datasets from here (direct download). Download and extract the zip file to be used as input data for the following step. #pointreyes_url &lt;- &#39;http://bit.ly/2F8w2Ia&#39; # set up the input image directory data_dir &lt;- &#39;data/&#39; #dir.create(data_dir, showWarnings = F) #pointreyes_zip &lt;- paste0(data_dir, &#39;pointreyes.zip&#39;) pointreyes_dir &lt;- paste0(data_dir, &#39;pointreyes&#39;) #download zip file #download.file(pointreyes_url, destfile = pointreyes_zip) #unzip(pointreyes_zip, exdir = data_dir) # get a list of all .jpg files in the directory pointreyes_images &lt;- dir(path = &#39;data/pointreyes&#39;, pattern = &#39;*.jpg&#39;, ignore.case = TRUE, full.names = TRUE) Now we can use a for loop to process all of the images to get the haze and A0 values. # number of images n &lt;- length(pointreyes_images) # create an empty matrix to fill with haze and A0 values haze_mat &lt;- data.frame() # the process takes a bit, a progress bar lets us know it is working. pb &lt;- txtProgressBar(0, n, style = 3) ## | | | 0% for(i in 1:n) { image_path &lt;- pointreyes_images[i] img &lt;- jpeg::readJPEG(image_path) hz &lt;- getHazeFactor(img) haze_mat &lt;- rbind(haze_mat, data.frame(file = as.character(image_path), haze = hz[1], A0 = hz[2])) setTxtProgressBar(pb, i) } ## | |= | 1% | |== | 3% | |=== | 4% | |==== | 6% | |===== | 7% | |====== | 8% | |======= | 10% | |======== | 11% | |========= | 13% | |========== | 14% | |=========== | 15% | |============ | 17% | |============= | 18% | |============== | 20% | |=============== | 21% | |================ | 23% | |================= | 24% | |================== | 25% | |=================== | 27% | |==================== | 28% | |===================== | 30% | |====================== | 31% | |======================= | 32% | |======================== | 34% | |========================= | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 39% | |============================= | 41% | |============================== | 42% | |=============================== | 44% | |================================ | 45% | |================================= | 46% | |================================== | 48% | |=================================== | 49% | |=================================== | 51% | |==================================== | 52% | |===================================== | 54% | |====================================== | 55% | |======================================= | 56% | |======================================== | 58% | |========================================= | 59% | |========================================== | 61% | |=========================================== | 62% | |============================================ | 63% | |============================================= | 65% | |============================================== | 66% | |=============================================== | 68% | |================================================ | 69% | |================================================= | 70% | |================================================== | 72% | |=================================================== | 73% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |======================================================= | 79% | |======================================================== | 80% | |========================================================= | 82% | |========================================================== | 83% | |=========================================================== | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 94% | |=================================================================== | 96% | |==================================================================== | 97% | |===================================================================== | 99% | |======================================================================| 100% Now we have a matrix with haze and A0 values for all our images. Let’s compare top three images with low and high haze values. top10_high_haze &lt;- haze_mat %&gt;% dplyr::arrange(desc(haze)) %&gt;% slice(1:3) top10_low_haze &lt;- haze_mat %&gt;% arrange(haze)%&gt;% slice(1:3) par(mar= c(0,0,0,0), mfrow=c(3,2), oma=c(0,0,3,0)) for(i in 1:3){ img &lt;- readJPEG(as.character(top10_low_haze$file[i])) plot.new() rasterImage(img, par()$usr[1], par()$usr[3], par()$usr[2], par()$usr[4]) img &lt;- readJPEG(as.character(top10_high_haze$file[i])) plot.new() rasterImage(img, par()$usr[1], par()$usr[3], par()$usr[2], par()$usr[4]) } mtext(&#39;Top images with low (left) and high (right) haze values at Point Reyes&#39;, font = 2, outer = TRUE) Let’s classify those into hazy and non-hazy as per the PhenoCam standard of 0.4. # classify image as hazy: T/F haze_mat=haze_mat%&gt;% mutate(haze_mat, foggy=ifelse(haze&gt;.4, TRUE, FALSE)) head(haze_mat) ## file haze A0 foggy ## 1 data/pointreyes/pointreyes_2017_01_01_120056.jpg 0.2249810 0.6970257 FALSE ## 2 data/pointreyes/pointreyes_2017_01_06_120210.jpg 0.2339372 0.6826148 FALSE ## 3 data/pointreyes/pointreyes_2017_01_16_120105.jpg 0.2312940 0.7009978 FALSE ## 4 data/pointreyes/pointreyes_2017_01_21_120105.jpg 0.4536108 0.6209055 TRUE ## 5 data/pointreyes/pointreyes_2017_01_26_120106.jpg 0.2297961 0.6813884 FALSE ## 6 data/pointreyes/pointreyes_2017_01_31_120125.jpg 0.4206842 0.6315728 TRUE Now we can save all the foggy images to a new folder that will retain the foggy images but keep them separate from the non-foggy ones that we want to analyze. # identify directory to move the foggy images to foggy_dir &lt;- paste0(pointreyes_dir, &#39;foggy&#39;) clear_dir &lt;- paste0(pointreyes_dir, &#39;clear&#39;) # if a new directory, create new directory at this file path dir.create(foggy_dir, showWarnings = FALSE) dir.create(clear_dir, showWarnings = FALSE) # copy the files to the new directories #file.copy(haze_mat[foggy==TRUE,file], to = foggy_dir) #file.copy(haze_mat[foggy==FALSE,file], to = clear_dir) Now that we have our images separated, we can get the full list of haze values only for those images that are not classified as “foggy”. # this is an alternative approach instead of a for loop # loading all the images as a list of arrays pointreyes_clear_images &lt;- dir(path = clear_dir, pattern = &#39;*.jpg&#39;, ignore.case = TRUE, full.names = TRUE) img_list &lt;- lapply(pointreyes_clear_images, FUN = jpeg::readJPEG) # getting the haze value for the list # patience - this takes a bit of time haze_list &lt;- t(sapply(img_list, FUN = getHazeFactor)) # view first few entries head(haze_list) ## haze A0 ## [1,] 0.224981 0.6970257 ## [2,] 0.2339372 0.6826148 ## [3,] 0.231294 0.7009978 ## [4,] 0.2297961 0.6813884 ## [5,] 0.2152078 0.6949932 ## [6,] 0.345584 0.6789334 We can then use these values for further analyses and data correction. The hazer R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/hazer. 4.4 Extracting Timeseries from Images using the xROI R Package In this section, we’ll learn how to use an interactive open-source toolkit, the xROI R package that facilitates the process of time series extraction and improves the quality of the final data. The xROI package provides a responsive environment for scientists to interactively: delineate regions of interest (ROIs), handle field of view (FOV) shifts, and extract and export time series data characterizing color-based metrics. Using the xROI R package, the user can detect FOV shifts with minimal difficulty. The software gives user the opportunity to re-adjust mask files or redraw new ones every time an FOV shift occurs. 4.4.1 xROI Design The R language and Shiny package were used as the main development tool for xROI, while Markdown, HTML, CSS and JavaScript languages were used to improve the interactivity. While Shiny apps are primarily used for web-based applications to be used online, the package authors used Shiny for its graphical user interface capabilities. In other words, both the User Interface (UI) and server modules are run locally from the same machine and hence no internet connection is required (after installation). The xROI’s UI element presents a side-panel for data entry and three main tab-pages, each responsible for a specific task. The server-side element consists of R and bash scripts. Image processing and geospatial features were performed using the Geospatial Data Abstraction Library (GDAL) and the rgdal and raster R packages. 4.4.2 Install xROI The xROI R package has been published on The Comprehensive R Archive Network (CRAN). The latest tested xROI package can be installed from the CRAN packages repository by running the following command in an R environment. utils::install.packages(&#39;xROI&#39;, repos = &quot;http://cran.us.r-project.org&quot; ) Alternatively, the latest beta release of xROI can be directly downloaded and installed from the development GitHub repository. # install devtools first utils::install.packages(&#39;devtools&#39;, repos = &quot;http://cran.us.r-project.org&quot; ) # use devtools to install from GitHub devtools::install_github(&quot;bnasr/xROI&quot;) xROI depends on many R packages including: raster, rgdal, sp, jpeg, tiff, shiny, shinyjs, shinyBS, shinyAce, shinyTime, shinyFiles, shinydashboard, shinythemes, colourpicker, rjson, stringr, data.table, lubridate, plotly, moments, and RCurl. All the required libraries and packages will be automatically installed with installation of xROI. The package offers a fully interactive high-level interface as well as a set of low-level functions for ROI processing. 4.4.3 Launch xROI A comprehensive user manual for low-level image processing using xROI is available from CRAN xROI.pdf. While the user manual includes a set of examples for each function; here we will learn to use the graphical interactive mode. Calling the Launch() function, as we’ll do below, opens up the interactive mode in your operating system’s default web browser. The landing page offers an example dataset to explore different modules or upload a new dataset of images. You can launch the interactive mode can be launched from an interactive R environment. # load xROI library(xROI) # launch xROI Launch() Or from the command line (e.g. bash in Linux, Terminal in macOS and Command Prompt in Windows machines) where an R engine is already installed. Rscript -e “xROI::Launch(Interactive = TRUE)” 4.4.4 End xROI When you are done with the xROI interface you can close the tab in your browser and end the session in R by using one of the following options In RStudio: Press the key on your keyboard. In R Terminal: Press &lt;Ctrl + C&gt; on your keyboard. 4.4.5 Use xROI To get some hands-on experience with xROI, we can analyze images from the dukehw of the PhenoCam network. You can download the data set from this link (direct download). Follow the steps below: First,save and extract (unzip) the file on your computer. Second, open the data set in xROI by setting the file path to your data # launch data in ROI # first edit the path below to the dowloaded directory you just extracted xROI::Launch(&#39;/path/to/extracted/directory&#39;) # alternatively, you can run without specifying a path and use the interface to browse Now, draw an ROI and the metadata. Then, save the metadata and explore its content. Now we can explore if there is any FOV shift in the dataset using the CLI processer tab. Finally, we can go to the Time series extraction tab. Extract the time-series. Save the output and explore the dataset in R. 4.5 Documentation and Citation More documentation about xROI can be found from: Seyednarollah, et al. 2019. knitr::include_graphics(&#39;docs/images/xROI-ms2019.png&#39;) &gt;xROI published in ISPRS Journal of Photogrammetry and Remote Sensing, 2019 4.6 Challenge: Use xROI Let’s use xROI on a little more challenging site with field of view shifts. Download and extract the data set from this link (direct download, 218 MB) and follow the above steps to extract the time-series. The xROI R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/xROI. 4.7 The PhenoCam Network Mission &amp; Design 4.8 Pulling Data via the phenocamapi Package The phenocamapi R package is developed to simplify interacting with the PhenoCam network dataset and perform data wrangling steps on PhenoCam sites’ data and metadata. This tutorial will show you the basic commands for accessing PhenoCam data through the PhenoCam API. The phenocampapi R package is developed and maintained by Bijan Seyednarollah. The most recent release is available on GitHub (PhenocamAPI). Additional vignettes can be found on how to merge external time-series (e.g. Flux data) with the PhenoCam time-series. We begin with several useful skills and tools for extracting PhenoCam data directly from the server: Exploring the PhenoCam metadata Filtering the dataset by site attributes Downloading PhenoCam time-series data Extracting the list of midday images Downloading midday images for a given time range 4.9 Exploring PhenoCam metadata Each PhenoCam site has specific metadata including but not limited to how a site is set up and where it is located, what vegetation type is visible from the camera, and its climate regime. Each PhenoCam may have zero to several Regions of Interest (ROIs) per vegetation type. The phenocamapi package is an interface to interact with the PhenoCam server to extract those data and process them in an R environment. To explore the PhenoCam data, we’ll use several packages for this tutorial. library(data.table) library(phenocamapi) ## Loading required package: rjson ## Loading required package: RCurl ## Warning: package &#39;RCurl&#39; was built under R version 3.6.2 library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(jpeg) We can obtain an up-to-date data.frame of the metadata of the entire PhenoCam network using the get_phenos() function. The returning value would be a data.table in order to simplify further data exploration. # obtaining the phenocam site metadata from the server as data.table phenos &lt;- get_phenos() # checking out the first few sites head(phenos$site) ## [1] &quot;aafcottawacfiaf14e&quot; &quot;aafcottawacfiaf14w&quot; &quot;acadia&quot; ## [4] &quot;aguatibiaeast&quot; &quot;aguatibianorth&quot; &quot;ahwahnee&quot; # checking out the columns colnames(phenos) ## [1] &quot;site&quot; &quot;lat&quot; ## [3] &quot;lon&quot; &quot;elev&quot; ## [5] &quot;active&quot; &quot;utc_offset&quot; ## [7] &quot;date_first&quot; &quot;date_last&quot; ## [9] &quot;infrared&quot; &quot;contact1&quot; ## [11] &quot;contact2&quot; &quot;site_description&quot; ## [13] &quot;site_type&quot; &quot;group&quot; ## [15] &quot;camera_description&quot; &quot;camera_orientation&quot; ## [17] &quot;flux_data&quot; &quot;flux_networks&quot; ## [19] &quot;flux_sitenames&quot; &quot;dominant_species&quot; ## [21] &quot;primary_veg_type&quot; &quot;secondary_veg_type&quot; ## [23] &quot;site_meteorology&quot; &quot;MAT_site&quot; ## [25] &quot;MAP_site&quot; &quot;MAT_daymet&quot; ## [27] &quot;MAP_daymet&quot; &quot;MAT_worldclim&quot; ## [29] &quot;MAP_worldclim&quot; &quot;koeppen_geiger&quot; ## [31] &quot;ecoregion&quot; &quot;landcover_igbp&quot; ## [33] &quot;dataset_version1&quot; &quot;site_acknowledgements&quot; ## [35] &quot;modified&quot; &quot;flux_networks_name&quot; ## [37] &quot;flux_networks_url&quot; &quot;flux_networks_description&quot; Now we have a better idea of the types of metadata that are available for the Phenocams. 4.9.1 Remove null values We may want to explore some of the patterns in the metadata before we jump into specific locations. Let’s look at Mean Annual Precipitation (MAP) and Mean Annual Temperature (MAT) across the different field site and classify those by the primary vegetation type (primary_veg_type) for each site. We can find out what the abbreviations for the vegetation types mean from the following table: Abbreviation Description AG agriculture | DB deciduous broadleaf | DN deciduous needleleaf | EB evergreen broadleaf | EN evergreen needleleaf | GR grassland | MX mixed vegetation (generally EN/DN, DB/EN, or DB/EB) | SH shrubs | TN tundra (includes sedges, lichens, mosses, etc.) | WT wetland | NV non-vegetated | RF reference panel | XX unspecified | To do this we’d first want to remove the sites where there is not data and then plot the data. # removing the sites with unkown MAT and MAP values phenos &lt;- phenos[!((MAT_worldclim == -9999)|(MAP_worldclim == -9999))] # extracting the PhenoCam climate space based on the WorldClim dataset # and plotting the sites across the climate space different vegetation type as different symbols and colors phenos[primary_veg_type==&#39;DB&#39;, plot(MAT_worldclim, MAP_worldclim, pch = 19, col = &#39;green&#39;, xlim = c(-5, 27), ylim = c(0, 4000))] ## NULL phenos[primary_veg_type==&#39;DN&#39;, points(MAT_worldclim, MAP_worldclim, pch = 1, col = &#39;darkgreen&#39;)] ## NULL phenos[primary_veg_type==&#39;EN&#39;, points(MAT_worldclim, MAP_worldclim, pch = 17, col = &#39;brown&#39;)] ## NULL phenos[primary_veg_type==&#39;EB&#39;, points(MAT_worldclim, MAP_worldclim, pch = 25, col = &#39;orange&#39;)] ## NULL phenos[primary_veg_type==&#39;AG&#39;, points(MAT_worldclim, MAP_worldclim, pch = 12, col = &#39;yellow&#39;)] ## NULL phenos[primary_veg_type==&#39;SH&#39;, points(MAT_worldclim, MAP_worldclim, pch = 23, col = &#39;red&#39;)] ## NULL legend(&#39;topleft&#39;, legend = c(&#39;DB&#39;,&#39;DN&#39;, &#39;EN&#39;,&#39;EB&#39;,&#39;AG&#39;, &#39;SH&#39;), pch = c(19, 1, 17, 25, 12, 23), col = c(&#39;green&#39;, &#39;darkgreen&#39;, &#39;brown&#39;, &#39;orange&#39;, &#39;yellow&#39;, &#39;red&#39; )) 4.9.2 Filtering using attributes Alternatively, we may want to only include Phenocams with certain attributes in our datasets. For example, we may be interested only in sites with a co-located flux tower. For this, we’d want to filter for those with a flux tower using the flux_sitenames attribute in the metadata. # store sites with flux_data available and the FLUX site name is specified phenofluxsites &lt;- phenos[flux_data==TRUE&amp;!is.na(flux_sitenames)&amp;flux_sitenames!=&#39;&#39;, .(PhenoCam=site, Flux=flux_sitenames)] # return as table #and specify which variables to retain phenofluxsites &lt;- phenofluxsites[Flux!=&#39;&#39;] # see the first few rows head(phenofluxsites) ## PhenoCam Flux ## 1: alligatorriver US-NC4 ## 2: arsbrooks10 US-Br1: Brooks Field Site 10- Ames ## 3: arsbrooks11 US-Br3: Brooks Field Site 11- Ames ## 4: arscolesnorth LTAR ## 5: arscolessouth LTAR ## 6: arsgreatbasinltar098 US-Rws We could further identify which of those Phenocams with a flux tower and in deciduous broadleaf forests (primary_veg_type=='DB'). #list deciduous broadleaf sites with flux tower DB.flux &lt;- phenos[flux_data==TRUE&amp;primary_veg_type==&#39;DB&#39;, site] # return just the site names as a list # see the first few rows head(DB.flux) ## [1] &quot;alligatorriver&quot; &quot;bartlett&quot; &quot;bartlettir&quot; &quot;bbc1&quot; ## [5] &quot;bbc2&quot; &quot;bbc3&quot; 4.10 PhenoCam time series PhenoCam time series are extracted time series data obtained from ROI’s for a given site. 4.10.1 Obtain ROIs To download the phenological time series from the PhenoCam, we need to know the site name, vegetation type and ROI ID. This information can be obtained from each specific PhenoCam page on the PhenoCam website or by using the get_rois() function. # obtaining the list of all the available ROI&#39;s on the PhenoCam server rois &lt;- get_rois() # view what information is returned colnames(rois) ## [1] &quot;roi_name&quot; &quot;site&quot; &quot;lat&quot; ## [4] &quot;lon&quot; &quot;roitype&quot; &quot;active&quot; ## [7] &quot;show_link&quot; &quot;show_data_link&quot; &quot;sequence_number&quot; ## [10] &quot;description&quot; &quot;first_date&quot; &quot;last_date&quot; ## [13] &quot;site_years&quot; &quot;missing_data_pct&quot; &quot;roi_page&quot; ## [16] &quot;roi_stats_file&quot; &quot;one_day_summary&quot; &quot;three_day_summary&quot; ## [19] &quot;data_release&quot; # view first few locations head(rois$roi_name) ## [1] &quot;alligatorriver_DB_1000&quot; &quot;arbutuslake_DB_1000&quot; ## [3] &quot;arbutuslakeinlet_DB_1000&quot; &quot;arbutuslakeinlet_EN_1000&quot; ## [5] &quot;arbutuslakeinlet_EN_2000&quot; &quot;archboldavir_AG_1000&quot; 4.10.2 Download time series The get_pheno_ts() function can download a time series and return the result as a data.table. Let’s work with the Duke Forest Hardwood Stand (dukehw) PhenoCam and specifically the ROI DB_1000 we can run the following code. # list ROIs for dukehw rois[site==&#39;dukehw&#39;,] ## roi_name site lat lon roitype active show_link ## 1: dukehw_DB_1000 dukehw 35.97358 -79.10037 DB TRUE TRUE ## show_data_link sequence_number description ## 1: TRUE 1000 canopy level DB forest at awesome Duke forest ## first_date last_date site_years missing_data_pct ## 1: 2013-06-01 2020-09-07 7.1 3.0 ## roi_page ## 1: https://phenocam.sr.unh.edu/webcam/roi/dukehw/DB_1000/ ## roi_stats_file ## 1: https://phenocam.sr.unh.edu/data/archive/dukehw/ROI/dukehw_DB_1000_roistats.csv ## one_day_summary ## 1: https://phenocam.sr.unh.edu/data/archive/dukehw/ROI/dukehw_DB_1000_1day.csv ## three_day_summary ## 1: https://phenocam.sr.unh.edu/data/archive/dukehw/ROI/dukehw_DB_1000_3day.csv ## data_release ## 1: NA # to obtain the DB 1000 from dukehw dukehw_DB_1000 &lt;- get_pheno_ts(site = &#39;dukehw&#39;, vegType = &#39;DB&#39;, roiID = 1000, type = &#39;3day&#39;) # what data are available str(dukehw_DB_1000) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 888 obs. of 35 variables: ## $ date : Factor w/ 888 levels &quot;2013-06-01&quot;,&quot;2013-06-04&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ doy : int 152 155 158 161 164 167 170 173 176 179 ... ## $ image_count : int 57 76 77 77 77 78 21 0 0 0 ... ## $ midday_filename : Factor w/ 859 levels &quot;dukehw_2013_06_01_120111.jpg&quot;,..: 1 2 3 4 5 6 7 859 859 859 ... ## $ midday_r : num 91.3 76.4 60.6 76.5 88.9 ... ## $ midday_g : num 97.9 85 73.2 82.2 95.7 ... ## $ midday_b : num 47.4 33.6 35.6 37.1 51.4 ... ## $ midday_gcc : num 0.414 0.436 0.432 0.42 0.406 ... ## $ midday_rcc : num 0.386 0.392 0.358 0.391 0.377 ... ## $ r_mean : num 87.6 79.9 72.7 80.9 83.8 ... ## $ r_std : num 5.9 6 9.5 8.23 5.89 ... ## $ g_mean : num 92.1 86.9 84 88 89.7 ... ## $ g_std : num 6.34 5.26 7.71 7.77 6.47 ... ## $ b_mean : num 46.1 38 39.6 43.1 46.7 ... ## $ b_std : num 4.48 3.42 5.29 4.73 4.01 ... ## $ gcc_mean : num 0.408 0.425 0.429 0.415 0.407 ... ## $ gcc_std : num 0.00859 0.0089 0.01318 0.01243 0.01072 ... ## $ gcc_50 : num 0.408 0.427 0.431 0.416 0.407 ... ## $ gcc_75 : num 0.414 0.431 0.435 0.424 0.415 ... ## $ gcc_90 : num 0.417 0.434 0.44 0.428 0.421 ... ## $ rcc_mean : num 0.388 0.39 0.37 0.381 0.38 ... ## $ rcc_std : num 0.01176 0.01032 0.01326 0.00881 0.00995 ... ## $ rcc_50 : num 0.387 0.391 0.373 0.383 0.382 ... ## $ rcc_75 : num 0.391 0.396 0.378 0.388 0.385 ... ## $ rcc_90 : num 0.397 0.399 0.382 0.391 0.389 ... ## $ max_solar_elev : num 76 76.3 76.6 76.8 76.9 ... ## $ snow_flag : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_mean: logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_50 : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_75 : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_90 : logi NA NA NA NA NA NA ... ## $ YEAR : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ DOY : int 152 155 158 161 164 167 170 173 176 179 ... ## $ YYYYMMDD : chr &quot;2013-06-01&quot; &quot;2013-06-04&quot; &quot;2013-06-07&quot; &quot;2013-06-10&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; We now have a variety of data related to this ROI from the Hardwood Stand at Duke Forest. Green Chromatic Coordinate (GCC) is a measure of “greenness” of an area and is widely used in Phenocam images as an indicator of the green pigment in vegetation. Let’s use this measure to look at changes in GCC over time at this site. Looking back at the available data, we have several options for GCC. gcc90 is the 90th quantile of GCC in the pixels across the ROI (for more details, PhenoCam v1 description). We’ll use this as it tracks the upper greenness values while not including many outliners. Before we can plot gcc-90 we do need to fix our dates and convert them from Factors to Date to correctly plot. # date variable into date format dukehw_DB_1000[,date:=as.Date(date)] # plot gcc_90 dukehw_DB_1000[,plot(date, gcc_90, col = &#39;green&#39;, type = &#39;b&#39;)] ## NULL mtext(&#39;Duke Forest, Hardwood&#39;, font = 2) 4.11 Download midday images While PhenoCam sites may have many images in a given day, many simple analyses can use just the midday image when the sun is most directly overhead the canopy. Therefore, extracting a list of midday images (only one image a day) can be useful. # obtaining midday_images for dukehw duke_middays &lt;- get_midday_list(&#39;dukehw&#39;) # see the first few rows head(duke_middays) ## [1] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/05/dukehw_2013_05_31_150331.jpg&quot; ## [2] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_01_120111.jpg&quot; ## [3] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_02_120109.jpg&quot; ## [4] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_03_120110.jpg&quot; ## [5] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_04_120119.jpg&quot; ## [6] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_05_120110.jpg&quot; Now we have a list of all the midday images from this Phenocam. Let’s download them and plot # download a file destfile &lt;- tempfile(fileext = &#39;.jpg&#39;) # download only the first available file # modify the `[1]` to download other images download.file(duke_middays[1], destfile = destfile, mode = &#39;wb&#39;) # plot the image img &lt;- try(readJPEG(destfile)) if(class(img)!=&#39;try-error&#39;){ par(mar= c(0,0,0,0)) plot(0:1,0:1, type=&#39;n&#39;, axes= FALSE, xlab= &#39;&#39;, ylab = &#39;&#39;) rasterImage(img, 0, 0, 1, 1) } 4.11.1 Download midday images for a given time range Now we can access all the midday images and download them one at a time. However, we frequently want all the images within a specific time range of interest. We’ll learn how to do that next. # open a temporary directory tmp_dir &lt;- tempdir() # download a subset. Example dukehw 2017 download_midday_images(site = &#39;dukehw&#39;, # which site y = 2017, # which year(s) months = 1:12, # which month(s) days = 15, # which days on month(s) download_dir = tmp_dir) # where on your computer ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% ## [1] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T&quot; # list of downloaded files duke_middays_path &lt;- dir(tmp_dir, pattern = &#39;dukehw*&#39;, full.names = TRUE) head(duke_middays_path) ## [1] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T/dukehw_2017_01_15_120109.jpg&quot; ## [2] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T/dukehw_2017_02_15_120108.jpg&quot; ## [3] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T/dukehw_2017_03_15_120151.jpg&quot; ## [4] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T/dukehw_2017_04_15_120110.jpg&quot; ## [5] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T/dukehw_2017_05_15_120108.jpg&quot; ## [6] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpbnAL9T/dukehw_2017_06_15_120120.jpg&quot; We can demonstrate the seasonality of Duke forest observed from the camera. (Note this code may take a while to run through the loop). n &lt;- length(duke_middays_path) par(mar= c(0,0,0,0), mfrow=c(4,3), oma=c(0,0,3,0)) for(i in 1:n){ img &lt;- readJPEG(duke_middays_path[i]) plot(0:1,0:1, type=&#39;n&#39;, axes= FALSE, xlab= &#39;&#39;, ylab = &#39;&#39;) rasterImage(img, 0, 0, 1, 1) mtext(month.name[i], line = -2) } mtext(&#39;Seasonal variation of forest at Duke Hardwood Forest&#39;, font = 2, outer = TRUE) The goal of this section was to show how to download a limited number of midday images from the PhenoCam server. However, more extensive datasets should be downloaded from the PhenoCam . The phenocamapi R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/phenocamapi. 4.12 Digital Repeat Photography Exercises 4.12.1 Digital Repeat Photography Computational First let’s load some packages: library(jsonlite) ## Warning: package &#39;jsonlite&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;jsonlite&#39; ## The following objects are masked from &#39;package:rjson&#39;: ## ## fromJSON, toJSON library(phenocamapi) library(plotly) ## Warning: package &#39;plotly&#39; was built under R version 3.6.2 ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(phenocamr) library(dplyr) As a refresher, there are two main ways to pull in PhenoCam data. First, directly via the API: c = jsonlite::fromJSON(&#39;https://phenocam.sr.unh.edu/api/cameras/?format=json&amp;limit=2000&#39;) c = c$results c_m=c$sitemetadata c$sitemetadata=NULL cams_=cbind(c, c_m) cams_[is.na(cams_)] = &#39;N&#39; cams_[, 2:4] &lt;- sapply(cams_[, 2:4], as.numeric) #changing lat/lon/elev from string values into numeric ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion head(cams_) ## Sitename Lat Lon Elev active utc_offset date_first ## 1 aafcottawacfiaf14e 45.29210 -75.76640 90 TRUE -5 2020-04-27 ## 2 aafcottawacfiaf14w 45.29210 -75.76640 90 TRUE -5 2020-05-01 ## 3 acadia 44.37694 -68.26083 158 TRUE -5 2007-03-15 ## 4 aguatibiaeast 33.62200 -116.86700 1086 FALSE -8 2007-08-16 ## 5 aguatibianorth 33.60222 -117.34368 1090 FALSE -8 2003-10-01 ## 6 ahwahnee 37.74670 -119.58160 1199 TRUE -8 2008-08-28 ## date_last infrared contact1 ## 1 2020-08-31 N Elizabeth Pattey &lt;elizabeth DOT pattey AT canada DOT ca&gt; ## 2 2020-08-31 N Elizabeth Pattey &lt;elizabeth DOT pattey AT canada DOT ca&gt; ## 3 2020-09-07 N Dee Morse &lt;dee_morse AT nps DOT gov&gt; ## 4 2019-01-25 N Ann E Mebane &lt;amebane AT fs DOT fed DOT us&gt; ## 5 2006-10-25 N ## 6 2020-09-07 N Dee Morse &lt;dee_morse AT nps DOT gov&gt; ## contact2 ## 1 Luc Pelletier &lt;luc DOT pelletier3 AT canada DOT ca&gt; ## 2 Luc Pelletier &lt;luc DOT pelletier3 AT canada DOT ca&gt; ## 3 John Gross &lt;John_Gross AT nps DOT gov&gt; ## 4 Kristi Savig &lt;KSavig AT air-resource DOT com&gt; ## 5 ## 6 John Gross &lt;John_Gross AT nps DOT gov&gt; ## site_description site_type ## 1 AAFC Site - Ottawa (On) - CFIA - Field F14 -East Flux Tower II ## 2 AAFC Site - Ottawa (On) - CFIA - Field F14 -West Flux Tower II ## 3 Acadia National Park, McFarland Hill, near Bar Harbor, Maine III ## 4 Agua Tibia Wilderness, California III ## 5 Agua Tibia Wilderness, California III ## 6 Ahwahnee Meadow, Yosemite National Park, California III ## group camera_description camera_orientation flux_data ## 1 N Campbell Scientific CCFC NE TRUE ## 2 N Campbell Scientific CCFC WNW TRUE ## 3 National Park Service unknown NE FALSE ## 4 USFS unknown SW FALSE ## 5 USFS unknown NE FALSE ## 6 National Park Service unknown E FALSE ## flux_networks flux_sitenames ## 1 NULL N ## 2 OTHER, , Other/Unaffiliated N ## 3 NULL ## 4 NULL ## 5 NULL ## 6 NULL ## dominant_species primary_veg_type ## 1 Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 2 Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 3 DB ## 4 SH ## 5 SH ## 6 EN ## secondary_veg_type site_meteorology MAT_site MAP_site MAT_daymet MAP_daymet ## 1 AG TRUE 6.4 943 6.3 952 ## 2 AG TRUE 6.4 943 6.3 952 ## 3 EN FALSE N N 7.05 1439 ## 4 FALSE N N 15.75 483 ## 5 FALSE N N 16 489 ## 6 GR FALSE N N 12.25 871 ## MAT_worldclim MAP_worldclim koeppen_geiger ecoregion landcover_igbp ## 1 6 863 Dfb 8 12 ## 2 6 863 Dfb 8 12 ## 3 6.5 1303 Dfb 8 5 ## 4 14.9 504 Csa 11 7 ## 5 13.8 729 Csa 11 7 ## 6 11.8 886 Csb 6 8 ## site_acknowledgements ## 1 Camera funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 2 Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 3 Camera images from Acadia National Park are provided courtesy of the National Park Service Air Resources Program. ## 4 Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 5 Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 6 Camera images from Yosemite National Park are provided courtesy of the National Park Service Air Resources Program. ## modified ## 1 2020-05-04T10:46:30.065790-04:00 ## 2 2020-05-04T10:46:32.523976-04:00 ## 3 2016-11-01T15:42:15.016778-04:00 ## 4 2016-11-01T15:42:15.086984-04:00 ## 5 2016-11-01T15:42:15.095277-04:00 ## 6 2016-11-01T15:42:15.111916-04:00 And second, via the phenocamapi package: phenos=get_phenos() head(phenos) ## site lat lon elev active utc_offset date_first ## 1: aafcottawacfiaf14e 45.29210 -75.76640 90 TRUE -5 2020-04-27 ## 2: aafcottawacfiaf14w 45.29210 -75.76640 90 TRUE -5 2020-05-01 ## 3: acadia 44.37694 -68.26083 158 TRUE -5 2007-03-15 ## 4: aguatibiaeast 33.62200 -116.86700 1086 FALSE -8 2007-08-16 ## 5: aguatibianorth 33.60222 -117.34368 1090 FALSE -8 2003-10-01 ## 6: ahwahnee 37.74670 -119.58160 1199 TRUE -8 2008-08-28 ## date_last infrared contact1 ## 1: 2020-08-31 N Elizabeth Pattey &lt;elizabeth.pattey@canada.ca&gt; ## 2: 2020-08-31 N Elizabeth Pattey &lt;elizabeth.pattey@canada.ca&gt; ## 3: 2020-09-07 N Dee Morse &lt;dee_morse@nps.gov&gt; ## 4: 2019-01-25 N Ann E Mebane &lt;amebane@fs.fed.us&gt; ## 5: 2006-10-25 N ## 6: 2020-09-07 N Dee Morse &lt;dee_morse@nps.gov&gt; ## contact2 ## 1: Luc Pelletier &lt;luc.pelletier3@canada.ca&gt; ## 2: Luc Pelletier &lt;luc.pelletier3@canada.ca&gt; ## 3: John Gross &lt;John_Gross@nps.gov&gt; ## 4: Kristi Savig &lt;KSavig@air-resource.com&gt; ## 5: ## 6: John Gross &lt;John_Gross@nps.gov&gt; ## site_description site_type ## 1: AAFC Site - Ottawa (On) - CFIA - Field F14 -East Flux Tower II ## 2: AAFC Site - Ottawa (On) - CFIA - Field F14 -West Flux Tower II ## 3: Acadia National Park, McFarland Hill, near Bar Harbor, Maine III ## 4: Agua Tibia Wilderness, California III ## 5: Agua Tibia Wilderness, California III ## 6: Ahwahnee Meadow, Yosemite National Park, California III ## group camera_description camera_orientation flux_data ## 1: &lt;NA&gt; Campbell Scientific CCFC NE TRUE ## 2: &lt;NA&gt; Campbell Scientific CCFC WNW TRUE ## 3: National Park Service unknown NE FALSE ## 4: USFS unknown SW FALSE ## 5: USFS unknown NE FALSE ## 6: National Park Service unknown E FALSE ## flux_networks flux_sitenames ## 1: &lt;list[0]&gt; &lt;NA&gt; ## 2: &lt;list[1]&gt; &lt;NA&gt; ## 3: &lt;list[0]&gt; ## 4: &lt;list[0]&gt; ## 5: &lt;list[0]&gt; ## 6: &lt;list[0]&gt; ## dominant_species primary_veg_type ## 1: Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 2: Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 3: DB ## 4: SH ## 5: SH ## 6: EN ## secondary_veg_type site_meteorology MAT_site MAP_site MAT_daymet MAP_daymet ## 1: AG TRUE 6.4 943 6.30 952 ## 2: AG TRUE 6.4 943 6.30 952 ## 3: EN FALSE NA NA 7.05 1439 ## 4: FALSE NA NA 15.75 483 ## 5: FALSE NA NA 16.00 489 ## 6: GR FALSE NA NA 12.25 871 ## MAT_worldclim MAP_worldclim koeppen_geiger ecoregion landcover_igbp ## 1: 6.0 863 Dfb 8 12 ## 2: 6.0 863 Dfb 8 12 ## 3: 6.5 1303 Dfb 8 5 ## 4: 14.9 504 Csa 11 7 ## 5: 13.8 729 Csa 11 7 ## 6: 11.8 886 Csb 6 8 ## dataset_version1 ## 1: NA ## 2: NA ## 3: NA ## 4: NA ## 5: NA ## 6: NA ## site_acknowledgements ## 1: Camera funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 2: Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 3: Camera images from Acadia National Park are provided courtesy of the National Park Service Air Resources Program. ## 4: Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 5: Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 6: Camera images from Yosemite National Park are provided courtesy of the National Park Service Air Resources Program. ## modified flux_networks_name flux_networks_url ## 1: 2020-05-04T10:46:30.065790-04:00 &lt;NA&gt; &lt;NA&gt; ## 2: 2020-05-04T10:46:32.523976-04:00 OTHER ## 3: 2016-11-01T15:42:15.016778-04:00 &lt;NA&gt; &lt;NA&gt; ## 4: 2016-11-01T15:42:15.086984-04:00 &lt;NA&gt; &lt;NA&gt; ## 5: 2016-11-01T15:42:15.095277-04:00 &lt;NA&gt; &lt;NA&gt; ## 6: 2016-11-01T15:42:15.111916-04:00 &lt;NA&gt; &lt;NA&gt; ## flux_networks_description ## 1: &lt;NA&gt; ## 2: Other/Unaffiliated ## 3: &lt;NA&gt; ## 4: &lt;NA&gt; ## 5: &lt;NA&gt; ## 6: &lt;NA&gt; To familiarize yourself with the phenocam API, let’s explore the structure: https://phenocam.sr.unh.edu/api/ Explore the options for filtering, file type and so forth. Now, based on either direct API access or via the phenocamapi package, generate a dataframe of phenocam sites. Select two phenocam sites from different plant functional types to explore (e.g. one grassland site and one evergreen needleleaf site) #example GrassSites=cams_%&gt;% filter(cams_$primary_veg_type==&#39;GR&#39;) head(GrassSites) ## Sitename Lat Lon Elev active utc_offset date_first ## 1 archboldbahia 27.16560 -81.21611 8 TRUE -5 2017-03-21 ## 2 arizonagrass 31.59070 -110.50920 1469 FALSE -7 2008-01-01 ## 3 arsgacp2 31.43950 -83.59146 101 FALSE 2 2016-04-27 ## 4 bozeman 45.78306 -110.77778 2332 FALSE -7 2015-08-16 ## 5 butte 45.95304 -112.47964 1682 TRUE -7 2008-04-01 ## 6 coaloilpoint 34.41369 -119.88023 6 FALSE -8 2008-05-11 ## date_last infrared contact1 ## 1 2020-09-07 Y Amartya Saha &lt;asaha AT archbold-station DOT org&gt; ## 2 2010-04-25 N Mark Heuer &lt;Mark DOT Heuer AT noaa DOT gov&gt; ## 3 2018-01-23 Y David Bosch &lt;David DOT Bosch AT ars DOT usda DOT gov&gt; ## 4 2019-12-18 Y Paul Stoy &lt;paul DOT stoy AT gmail DOT com&gt; ## 5 2020-09-07 N James Gallagher &lt;jgallagher AT opendap DOT org&gt; ## 6 2012-12-05 N Roberts &lt;dar AT geog DOT ucsb DOT edu&gt; ## contact2 ## 1 Elizabeth Boughton &lt;eboughton AT archbold-station DOT org&gt; ## 2 Tilden Meyers &lt;tilden DOT meyers AT noaa DOT gov&gt; ## 3 ## 4 ## 5 Martha Apple &lt;MApple AT mtech DOT edu&gt; ## 6 ## site_description site_type ## 1 Archbold Biological Station, Florida, USA I ## 2 Sierra Vista, Arizona III ## 3 Southeast Watershed Research Laboratory EC2 Tifton, Georgia I ## 4 Bangtail Study Area, Montana State University, Montana I ## 5 Continental Divide, Butte, Montana I ## 6 Coal Oil Point Natural Reserve, Santa Barbara, California II ## group camera_description camera_orientation flux_data ## 1 LTAR StarDot NetCam SC N FALSE ## 2 GEWEX Olympus D-360L N FALSE ## 3 LTAR N N FALSE ## 4 AmericaView AMERIFLUX StarDot NetCam SC N TRUE ## 5 PhenoCam StarDot NetCam SC E FALSE ## 6 unknown FALSE ## flux_networks flux_sitenames ## 1 NULL N ## 2 NULL N ## 3 NULL N ## 4 AMERIFLUX, http://ameriflux.lbl.gov, AmeriFlux Network US-MTB (forthcoming) ## 5 NULL ## 6 NULL ## dominant_species ## 1 ## 2 ## 3 ## 4 Festuca idahoensis ## 5 Agropyron cristatum, Poa pratensis, Phalaris arundinaceae, Carex. sp., Geum triflorum, Ericameria nauseousa, Centaurea macula, Achillea millefolium, Senecio sp., Lupinus sp., Penstemon sp., Linaria vulgaris, Cirsium arvense; Alnus incana, Salix sp., Populus tremuloides ## 6 Lolium multiflorum, Bromus horadaceous, Avena fatua, Plantago lanceolata, Sisyrinchium bellum ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 1 GR N FALSE N N ## 2 GR N FALSE N N ## 3 GR N FALSE N N ## 4 GR EN FALSE 5 850 ## 5 GR SH FALSE N N ## 6 GR SH FALSE 14.4 424 ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 1 22.85 1302 22.5 1208 Cfa 8 ## 2 16.25 468 15.3 446 BSk 12 ## 3 19 1251 18.7 1213 Cfa 8 ## 4 2.3 981 0.9 728 Dfb 6 ## 5 5.05 365 4.3 311 BSk 6 ## 6 16.15 623 15.4 405 Csb 11 ## landcover_igbp ## 1 14 ## 2 7 ## 3 14 ## 4 10 ## 5 10 ## 6 7 ## site_acknowledgements ## 1 ## 2 ## 3 ## 4 Research at the Bozeman site is supported by Colorado State University and the AmericaView program (grants G13AC00393, G11AC20461, G15AC00056) with phenocam equipment and deployment sponsored by the Department of Interior North Central Climate Science Center. ## 5 Research at the Continental Divide PhenoCam Site in Butte, Montana is supported by the National Science Foundation-EPSCoR (grant NSF-0701906), OpenDap, Inc., and Montana Tech of the University of Montana ## 6 ## modified ## 1 2019-01-07T18:36:07.631244-05:00 ## 2 2018-04-13T10:46:23.462958-04:00 ## 3 2018-06-18T20:27:17.280524-04:00 ## 4 2016-11-01T15:42:19.771057-04:00 ## 5 2016-11-01T15:42:19.846100-04:00 ## 6 2016-11-01T15:42:19.929455-04:00 FirstSite=GrassSites[5, ] #randomly chose the fifth site in the table FirstSite ## Sitename Lat Lon Elev active utc_offset date_first date_last ## 5 butte 45.95304 -112.4796 1682 TRUE -7 2008-04-01 2020-09-07 ## infrared contact1 ## 5 N James Gallagher &lt;jgallagher AT opendap DOT org&gt; ## contact2 site_description ## 5 Martha Apple &lt;MApple AT mtech DOT edu&gt; Continental Divide, Butte, Montana ## site_type group camera_description camera_orientation flux_data ## 5 I PhenoCam StarDot NetCam SC E FALSE ## flux_networks flux_sitenames ## 5 NULL ## dominant_species ## 5 Agropyron cristatum, Poa pratensis, Phalaris arundinaceae, Carex. sp., Geum triflorum, Ericameria nauseousa, Centaurea macula, Achillea millefolium, Senecio sp., Lupinus sp., Penstemon sp., Linaria vulgaris, Cirsium arvense; Alnus incana, Salix sp., Populus tremuloides ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 5 GR SH FALSE N N ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 5 5.05 365 4.3 311 BSk 6 ## landcover_igbp ## 5 10 ## site_acknowledgements ## 5 Research at the Continental Divide PhenoCam Site in Butte, Montana is supported by the National Science Foundation-EPSCoR (grant NSF-0701906), OpenDap, Inc., and Montana Tech of the University of Montana ## modified ## 5 2016-11-01T15:42:19.846100-04:00 Chose your own sites and build out your code chunk here: print(&#39;build your code here&#39;) ## [1] &quot;build your code here&quot; Koen Huffkens developed the phenocamr package, which streamlines access to quality controlled data. We will now use this package to download and process site based data according to a standardized methodology. A full description of the methodology is provided in Scientific Data: Tracking vegetation phenology across diverse North American biomes using PhenoCam imagery (Richardson et al. 2018). #uncomment if you need to install via devtools #if(!require(devtools)){install.package(devtools)} #devtools::install_github(&quot;khufkens/phenocamr&quot;) library(phenocamr) Use the dataframe of sites that you want to analyze to feed the phenocamr package. Note: you can choose either a daily or 3 day product dir.create(&#39;data/&#39;, showWarnings = F) phenocamr::download_phenocam( frequency = 3, veg_type = &#39;DB&#39;, roi_id = 1000, site = &#39;harvard$&#39;, phenophase = TRUE, out_dir = &quot;data/&quot; ) ## Downloading: harvard_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! #&gt; Downloading: harvard_DB_1000_3day.csv #&gt; -- Flagging outliers! #&gt; -- Smoothing time series! #&gt; -- Estimating transition dates! Now look in your working directory. You have data! Read it in: # load the time series data but replace the csv filename with whatever you downloaded df &lt;- read.table(&quot;data/harvard_DB_1000_3day.csv&quot;, header = TRUE, sep = &quot;,&quot;) # read in the transidation date file td &lt;- read.table(&quot;data/harvard_DB_1000_3day_transition_dates.csv&quot;, header = TRUE, sep = &quot;,&quot;) Let’s take a look at the data: p = plot_ly() %&gt;% add_trace( data = df, x = ~ as.Date(date), y = ~ smooth_gcc_90, name = &#39;Smoothed GCC&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_markers( data=df, x ~ as.Date(date), y = ~gcc_90, name = &#39;GCC&#39;, type = &#39;scatter&#39;, color =&#39;#07A4B5&#39;, opacity=.5 ) p ## Warning: `arrange_()` is deprecated as of dplyr 0.7.0. ## Please use `arrange()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: Ignoring 3215 observations ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels What patterns do you notice? How would we go about determining say the start of spring? (SOS) 4.12.2 Threshold values Let’s subset the transition date (td) for each year when 25% of the greenness amplitude (of the 90^th) percentile is reached (threshold_25). # select the rising (spring dates) for 25% threshold of Gcc 90 spring &lt;- td[td$direction == &quot;rising&quot; &amp; td$gcc_value == &quot;gcc_90&quot;,] Let’s create a simple plot_ly line graph of the smooth Green Chromatic Coordinate (Gcc) and add points for transition dates: p = plot_ly() %&gt;% add_trace( data = df, x = ~ as.Date(date), y = ~ smooth_gcc_90, name = &#39;PhenoCam GCC&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_markers( data= spring, x = ~ as.Date(spring$transition_25, origin = &quot;1970-01-01&quot;), y = ~ spring$threshold_25, type = &#39;scatter&#39;, mode = &#39;marker&#39;, name = &#39;Spring Dates&#39;) p Now we can see the transition date for each year of interest and the annual patterns of Gcc. However, if you want more control over the parameters used during processing, you can run through the three default processing steps as implemented in download_phenocam() and set parameters manually. Of particular interest is the option to specify your own threshold used in determining transition dates. What would be a reasonable threshold for peak greenness? Or autumn onset? Look at the ts dataset and phenocamr package and come up with a threshold. Use the same code to plot it here: # #some hint code # #what does &#39;rising&#39; versus &#39;falling&#39; denote? # #what threshold should you choose? # #td &lt;- phenophases(&quot;butte_GR_1000_3day.csv&quot;, # # internal = TRUE, # # upper_thresh = 0.8) fall &lt;- td[td$direction == &quot;falling&quot; &amp; td$gcc_value == &quot;gcc_90&quot;,] #Now generate a fall dataframe, what metrics should you use? 4.12.3 Comparing phenology across vegetation types Let’s load in a function to make plotting smoother. I’ve dropped it here in the markdown so that you can edit it and re-run it as you see fit: gcc_plot = function(gcc, spring, fall){ unix = &quot;1970-01-01&quot; p = plot_ly( data = gcc, x = ~ date, y = ~ gcc_90, showlegend = FALSE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_trace( y = ~ smooth_gcc_90, mode = &quot;lines&quot;, line = list(width = 2, color = &quot;rgb(120,120,120)&quot;), name = &quot;Gcc loess fit&quot;, showlegend = TRUE ) %&gt;% # SOS spring # 10% add_trace( data = spring, x = ~ as.Date(transition_10), y = ~ threshold_10, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#7FFF00&quot;, symbol = &quot;circle&quot;), name = &quot;SOS (10%)&quot;, showlegend = TRUE ) %&gt;% add_segments(x = ~ as.Date(transition_10_lower_ci), xend = ~ as.Date(transition_10_upper_ci), # y = ~ 0, # yend = ~ 1, y = ~ threshold_10, yend = ~ threshold_10, line = list(color = &quot;#7FFF00&quot;), name = &quot;SOS (10%) - CI&quot; ) %&gt;% # 25 % add_trace( x = ~ as.Date(transition_25), y = ~ threshold_25, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#66CD00&quot;, symbol = &quot;square&quot;), showlegend = TRUE, name = &quot;SOS (25%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_25_lower_ci), xend = ~ as.Date(transition_25_upper_ci), y = ~ threshold_25, yend = ~ threshold_25, line = list(color = &quot;#66CD00&quot;), name = &quot;SOS (25%) - CI&quot; ) %&gt;% # 50 % add_trace( x = ~ as.Date(transition_50), y = ~ threshold_50, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#458B00&quot;, symbol = &quot;diamond&quot;), showlegend = TRUE, name = &quot;SOS (50%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_50_lower_ci), xend = ~ as.Date(transition_50_upper_ci), y = ~ threshold_50, yend = ~ threshold_50, line = list(color = &quot;#458B00&quot;), name = &quot;SOS (50%) - CI&quot; ) %&gt;% # EOS fall # 50% add_trace( data = fall, x = ~ as.Date(transition_50), y = ~ threshold_50, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#FFB90F&quot;, symbol = &quot;diamond&quot;), showlegend = TRUE, name = &quot;EOS (50%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_50_lower_ci), xend = ~ as.Date(transition_50_upper_ci), y = ~ threshold_50, yend = ~ threshold_50, line = list(color = &quot;#FFB90F&quot;), name = &quot;EOS (50%) - CI&quot; ) %&gt;% # 25 % add_trace( x = ~ as.Date(transition_25), y = ~ threshold_25, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#CD950C&quot;, symbol = &quot;square&quot;), showlegend = TRUE, name = &quot;EOS (25%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_25_lower_ci), xend = ~ as.Date(transition_25_upper_ci), y = ~ threshold_25, yend = ~ threshold_25, line = list(color = &quot;#CD950C&quot;), name = &quot;EOS (25%) - CI&quot; ) %&gt;% # 10 % add_trace( x = ~ as.Date(transition_10), y = ~ threshold_10, mode = &quot;markers&quot;, marker = list(color = &quot;#8B6508&quot;, symbol = &quot;circle&quot;), showlegend = TRUE, name = &quot;EOS (10%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_10_lower_ci), xend = ~ as.Date(transition_10_upper_ci), y = ~ threshold_10, yend = ~ threshold_10, line = list(color = &quot;#8B6508&quot;), name = &quot;EOS (10%) - CI&quot; ) return (p) } gcc_p = gcc_plot(df, spring, fall) gcc_p &lt;- gcc_p %&gt;% layout( legend = list(x = 0.9, y = 0.9), xaxis = list( type = &#39;date&#39;, tickformat = &quot; %B&lt;br&gt;%Y&quot;, title=&#39;Year&#39;), yaxis = list( title = &#39;PhenoCam GCC&#39; )) gcc_p ## Warning: Ignoring 3215 observations ## Warning: `group_by_()` is deprecated as of dplyr 0.7.0. ## Please use `group_by()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: Can&#39;t display both discrete &amp; non-discrete data on same axis What is the difference in 25% greenness onset for your first site? #hint, look at the spring dataframe you just generated #some hints to get you started # d=spring$transition_25 # d=as.Date(d) # d #more code hints # dates_split &lt;- data.frame(date = d, # year = as.numeric(format(d, format = &quot;%Y&quot;)), # month = as.numeric(format(d, format = &quot;%m&quot;)), # day = as.numeric(format(d, format = &quot;%d&quot;))) Generate a plot of smoothed gcc and transition dates for your two sites and subplot them. What do you notice? #some hint code for subplotting in plot_ly: #p &lt;- subplot(p1, p2, nrows=2) #p 4.12.4 Comparing phenology of the same plant function type (PFT) across climate space As Dr. Richardson mentioned in his introduction lecture, the same plant functional types (e.g. grasslands) can have very different phenologogical cycles. Let’s pick two phenocam grassland sites: one from a tropical climate (kamuela), and one from an arid climate (konza) here in Arizona: GrassSites=GrassSites[c(27,32),] Now use the code you’ve generated above to pull in data from those sites: print(&#39;code here&#39;) ## [1] &quot;code here&quot; #&gt; [1] &quot;code here&quot; Now let’s create a subplot of your grasslands to compare phenology, some hint code below: #some hint code for subplotting in plot_ly: #p &lt;- subplot(p1, p2, nrows=2) #p Once you have a subplot of grassland phenology across 2 climates answer the following questions your markdown: 1. What seasonal patterns do you see? 2. Do you think you set your thresholds correctly for transition dates/phenophases? How might that very as a function of climate? 3. What are the challenges of forecasting or modeling tropical versus arid grasslands? 4.12.5 Digital Repeat Photography Written Question 1: How might or does the PhenoCam project intersect with your current research or future career goals? (1 paragraph) Question 2: Use the map on the PhenoCam website to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: Are there PhenoCams that are in study regions of interest to you? Which PhenoCam sites does your current research or final project ideas coincide with? Are they connected to other networks (e.g. LTAR, NEON, Fluxnet)? What is the data record length for the sites you’re interested in? Question 3: Consider either your current or future research, or a question you’d like to address durring this course: Which types of PhenoCam data may be more useful to address these questions? What non-PhenoCam data resources could be combined with NEON data to help address your question? What challenges, if any, could you foresee when beginning to work with these data? Question 4: Intro to PhenoCam Culmination Activity Write up a 1-page summary of a project that you might want to explore using PhenoCam data over the duration of this course. Include the types of PhenoCam (and other data) that you will need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. "],
["frequently-asked-questions.html", "Frequently Asked Questions: 4.13 Where can I find due dates for assignments? 4.14 How do I submit assignments? 4.15 Do I still have to submit written exercises as .Rmd and .pdf? 4.16 What’s better for code, conciseness or readability? 4.17 How find I find resources to navigate the NEON Data Portal? 4.18 How can I best prepare for class and succeed?", " Frequently Asked Questions: 4.13 Where can I find due dates for assignments? All assignment deadlines can be found in BBLearn. Within this textbook we have suggestions for the timing of all writen questions, coding labs and final culmination writes ups. All material by infrastructure is due before we begin the next infrastructure. 4.14 How do I submit assignments? All assignments (except the very first git assignment) are submitted via BBLearn as both .Rmds and .pdfs 4.15 Do I still have to submit written exercises as .Rmd and .pdf? Yes. 4.16 What’s better for code, conciseness or readability? Readability &gt; Conciseness I personally almost always use dplyr, my thoughts, pulled largely from Dr. Derek Sonderegger’s Statistical Computing Course are below: The pipe command in dplyr %&gt;% allows for very readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function and was introduced in the magrittr package. The beauty of this comes when you have a suite of functions that take input arguments of the same type as their output. They are human readable! For example if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). Dr. Hadley Wickham (aka R genius) gave the following example of readability: bopping( scooping_up( hopping_through(foo_foo), field_mice), head) is more readably written: foo_foo %&gt;% hopping_through(forest) %&gt;% scooping_up( field_mice) %&gt;% bopping( head ) In dplyr, all the functions take a data set as its first argument and outputs an appropriately modified data set. This allows me to chain together commands in a readable fashion. Then in 3 months I don’t have to wonder what on earth I was doing last time I opened this project, if I filtered the data, etc etc. Your future self will sincerely thank your past self. 4.17 How find I find resources to navigate the NEON Data Portal? A fantastic powerpoint giving you step-by-step directions can be found here. 4.18 How can I best prepare for class and succeed? Read the textbook, click on linked resources including videos, review materials as we go or read ahead. Complete assignments as we go or ahead of time. Do not wait until the last minute. Pick a ‘coding buddy’ and help each other tackle errors that arise. Reach out to your instructors if you need clarification on assignments. "]
]
